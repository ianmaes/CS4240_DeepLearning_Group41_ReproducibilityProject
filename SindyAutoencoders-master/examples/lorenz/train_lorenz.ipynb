{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../src\")\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from example_lorenz import get_lorenz_data\n",
    "from sindy_utils import library_size\n",
    "from training import train_network\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate training, validation, testing data\n",
    "noise_strength = 1e-6\n",
    "training_data = get_lorenz_data(1024, noise_strength=noise_strength)\n",
    "validation_data = get_lorenz_data(20, noise_strength=noise_strength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up model and training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256000\n"
     ]
    }
   ],
   "source": [
    "params = {}\n",
    "\n",
    "params['input_dim'] = 128\n",
    "params['latent_dim'] = 3\n",
    "params['model_order'] = 1\n",
    "params['poly_order'] = 3\n",
    "params['include_sine'] = False\n",
    "params['library_dim'] = library_size(params['latent_dim'], params['poly_order'], params['include_sine'], True)\n",
    "\n",
    "# sequential thresholding parameters\n",
    "params['sequential_thresholding'] = True\n",
    "params['coefficient_threshold'] = 0.1\n",
    "params['threshold_frequency'] = 500\n",
    "params['coefficient_mask'] = np.ones((params['library_dim'], params['latent_dim']))\n",
    "params['coefficient_initialization'] = 'constant'\n",
    "\n",
    "# loss function weighting\n",
    "params['loss_weight_decoder'] = 1.0\n",
    "params['loss_weight_sindy_z'] = 0.0\n",
    "params['loss_weight_sindy_x'] = 1e-4\n",
    "params['loss_weight_sindy_regularization'] = 1e-5\n",
    "\n",
    "params['activation'] = 'sigmoid'\n",
    "params['widths'] = [64,32]\n",
    "\n",
    "# training parameters\n",
    "params['epoch_size'] = training_data['x'].shape[0]\n",
    "print(training_data['x'].shape[0])\n",
    "params['batch_size'] = 1024\n",
    "params['learning_rate'] = 1e-3\n",
    "\n",
    "params['data_path'] = os.getcwd() + '/'\n",
    "params['print_progress'] = True\n",
    "params['print_frequency'] = 1\n",
    "\n",
    "# training time cutoffs\n",
    "params['max_epochs'] = 5001\n",
    "params['refinement_epochs'] = 1001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT 0\n",
      "WARNING:tensorflow:From C:\\Users\\ianma\\AppData\\Local\\Temp\\ipykernel_77504\\4054194948.py:10: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From ../../src\\autoencoder.py:28: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From ../../src\\autoencoder.py:194: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From ../../src\\training.py:11: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\ianma\\anaconda3\\envs\\tfv1\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From ../../src\\training.py:13: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "WARNING:tensorflow:From ../../src\\training.py:13: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From ../../src\\training.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "TRAINING\n",
      "WARNING:tensorflow:From ../../src\\training.py:27: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From ../../src\\training.py:28: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "Epoch 0\n",
      "   training loss 0.045687608420848846, (0.044293147, 934.3374, 13.857964, 0.86668324)\n",
      "   validation loss 0.03662244230508804, (0.03533124, 835.3166, 12.825352, 0.86668324)\n",
      "decoder loss ratio: 0.194529, decoder SINDy loss  ratio: 1.081948\n",
      "Epoch 1\n",
      "   training loss 0.018394380807876587, (0.017124698, 298.72192, 12.612467, 0.8436403)\n",
      "   validation loss 0.014864443801343441, (0.013678304, 258.27496, 11.777029, 0.8436403)\n",
      "decoder loss ratio: 0.075311, decoder SINDy loss  ratio: 0.993512\n",
      "Epoch 2\n",
      "   training loss 0.01193722989410162, (0.010845265, 192.95837, 10.837191, 0.82447785)\n",
      "   validation loss 0.010237421840429306, (0.009200375, 180.44293, 10.288021, 0.82447785)\n",
      "decoder loss ratio: 0.050656, decoder SINDy loss  ratio: 0.867899\n",
      "Epoch 3\n",
      "   training loss 0.01035204716026783, (0.009392358, 180.67188, 9.514358, 0.8253352)\n",
      "   validation loss 0.009184809401631355, (0.008258874, 166.09703, 9.17682, 0.8253352)\n",
      "decoder loss ratio: 0.045472, decoder SINDy loss  ratio: 0.774158\n",
      "Epoch 4\n",
      "   training loss 0.009227397851645947, (0.00842414, 222.38708, 7.947559, 0.85022753)\n",
      "   validation loss 0.008295349776744843, (0.0075034583, 197.68636, 7.8338985, 0.85022753)\n",
      "decoder loss ratio: 0.041313, decoder SINDy loss  ratio: 0.660869\n",
      "Epoch 5\n",
      "   training loss 0.008182156831026077, (0.007586078, 320.1406, 5.873439, 0.87346166)\n",
      "   validation loss 0.007355155423283577, (0.006756012, 281.6503, 5.904091, 0.87346166)\n",
      "decoder loss ratio: 0.037198, decoder SINDy loss  ratio: 0.498070\n",
      "Epoch 6\n",
      "   training loss 0.007406899239867926, (0.006916175, 379.21774, 4.8193493, 0.87892455)\n",
      "   validation loss 0.006599785294383764, (0.0061007366, 335.89032, 4.9025955, 0.87892455)\n",
      "decoder loss ratio: 0.033590, decoder SINDy loss  ratio: 0.413584\n",
      "Epoch 7\n",
      "   training loss 0.006741325370967388, (0.0062651155, 244.68306, 4.6738753, 0.88226134)\n",
      "   validation loss 0.005911264568567276, (0.0054265307, 207.37286, 4.759115, 0.88226134)\n",
      "decoder loss ratio: 0.029878, decoder SINDy loss  ratio: 0.401480\n",
      "Epoch 8\n",
      "   training loss 0.004581277258694172, (0.0040302975, 93.98199, 5.4206076, 0.8918964)\n",
      "   validation loss 0.003869149601086974, (0.003319183, 72.180565, 5.4104767, 0.8918964)\n",
      "decoder loss ratio: 0.018275, decoder SINDy loss  ratio: 0.456428\n",
      "Epoch 9\n",
      "   training loss 0.0027066245675086975, (0.0021545305, 50.194153, 5.430691, 0.9025044)\n",
      "   validation loss 0.002296071732416749, (0.001738055, 37.3107, 5.4899178, 0.9025044)\n",
      "decoder loss ratio: 0.009569, decoder SINDy loss  ratio: 0.463130\n",
      "Epoch 10\n",
      "   training loss 0.0020639922004193068, (0.0015313223, 33.79571, 5.2353373, 0.913633)\n",
      "   validation loss 0.0017491027247160673, (0.0012030059, 24.4356, 5.369606, 0.913633)\n",
      "decoder loss ratio: 0.006624, decoder SINDy loss  ratio: 0.452981\n",
      "Epoch 11\n",
      "   training loss 0.0016876246081665158, (0.0011898481, 26.31286, 4.8857746, 0.91990316)\n",
      "   validation loss 0.0014502055710181594, (0.00093636266, 17.725677, 5.0464396, 0.91990316)\n",
      "decoder loss ratio: 0.005155, decoder SINDy loss  ratio: 0.425718\n",
      "Epoch 12\n",
      "   training loss 0.0014709960669279099, (0.0010097872, 20.048721, 4.5194182, 0.9267119)\n",
      "   validation loss 0.0012862393632531166, (0.0008075457, 13.556834, 4.6942663, 0.9267119)\n",
      "decoder loss ratio: 0.004446, decoder SINDy loss  ratio: 0.396009\n",
      "Epoch 13\n",
      "   training loss 0.0013021165505051613, (0.00088344817, 15.575967, 4.0926666, 0.94016784)\n",
      "   validation loss 0.0011515524238348007, (0.00071005366, 11.068902, 4.3209705, 0.94016784)\n",
      "decoder loss ratio: 0.003909, decoder SINDy loss  ratio: 0.364518\n",
      "Epoch 14\n",
      "   training loss 0.0012010927312076092, (0.0008200633, 12.637435, 3.7143145, 0.95980054)\n",
      "   validation loss 0.0010793181136250496, (0.000669197, 9.594627, 4.005232, 0.95980054)\n",
      "decoder loss ratio: 0.003685, decoder SINDy loss  ratio: 0.337882\n",
      "Epoch 15\n",
      "   training loss 0.0011564863380044699, (0.0008061887, 10.767525, 3.4048004, 0.98176223)\n",
      "   validation loss 0.0010540452785789967, (0.000669435, 8.707273, 3.7479258, 0.98176223)\n",
      "decoder loss ratio: 0.003686, decoder SINDy loss  ratio: 0.316175\n",
      "Epoch 16\n",
      "   training loss 0.0010946578113362193, (0.0007672125, 9.497563, 3.1734262, 1.0102789)\n",
      "   validation loss 0.0009978337911888957, (0.0006333312, 8.06084, 3.5439985, 1.0102789)\n",
      "decoder loss ratio: 0.003487, decoder SINDy loss  ratio: 0.298972\n",
      "Epoch 17\n",
      "   training loss 0.0010600367095321417, (0.00075145916, 8.54768, 2.9817119, 1.040631)\n",
      "   validation loss 0.0009635099559091032, (0.00061753456, 7.555024, 3.355691, 1.040631)\n",
      "decoder loss ratio: 0.003400, decoder SINDy loss  ratio: 0.283086\n",
      "Epoch 18\n",
      "   training loss 0.0010203344281762838, (0.0007274133, 7.7788568, 2.8217912, 1.0742131)\n",
      "   validation loss 0.0009198077023029327, (0.0005909878, 7.123501, 3.1807778, 1.0742131)\n",
      "decoder loss ratio: 0.003254, decoder SINDy loss  ratio: 0.268331\n",
      "Epoch 19\n",
      "   training loss 0.0009692478342913091, (0.00068909145, 7.1148477, 2.6907053, 1.1085846)\n",
      "   validation loss 0.0008576550171710551, (0.0005447253, 6.7110343, 3.0184395, 1.1085846)\n",
      "decoder loss ratio: 0.002999, decoder SINDy loss  ratio: 0.254636\n",
      "Epoch 20\n",
      "   training loss 0.0009392657084390521, (0.00066745735, 6.535328, 2.6036801, 1.1440389)\n",
      "   validation loss 0.0008186266059055924, (0.00051888986, 6.299269, 2.8829641, 1.1440389)\n",
      "decoder loss ratio: 0.002857, decoder SINDy loss  ratio: 0.243207\n",
      "Epoch 21\n",
      "   training loss 0.0009196589235216379, (0.0006586463, 6.1052384, 2.4923637, 1.1776298)\n",
      "   validation loss 0.0007887438987381756, (0.0005048879, 5.985087, 2.7207975, 1.1776298)\n",
      "decoder loss ratio: 0.002780, decoder SINDy loss  ratio: 0.229527\n",
      "Epoch 22\n",
      "   training loss 0.000906983099412173, (0.0006562389, 5.7764363, 2.3863218, 1.2112015)\n",
      "   validation loss 0.0007659562979824841, (0.00049727183, 5.7510395, 2.5657241, 1.2112015)\n",
      "decoder loss ratio: 0.002738, decoder SINDy loss  ratio: 0.216445\n",
      "Epoch 23\n",
      "   training loss 0.000882818247191608, (0.0006417455, 5.522091, 2.286425, 1.2430228)\n",
      "   validation loss 0.0007287542102858424, (0.0004749933, 5.528526, 2.4133067, 1.2430228)\n",
      "decoder loss ratio: 0.002615, decoder SINDy loss  ratio: 0.203587\n",
      "Epoch 24\n",
      "   training loss 0.0008564881281927228, (0.00062492734, 5.283243, 2.1883156, 1.2729214)\n",
      "   validation loss 0.0006896887207403779, (0.0004504793, 5.3376412, 2.2648025, 1.2729214)\n",
      "decoder loss ratio: 0.002480, decoder SINDy loss  ratio: 0.191059\n",
      "Epoch 25\n",
      "   training loss 0.0008353495504707098, (0.0006132739, 5.240846, 2.0907345, 1.3002193)\n",
      "   validation loss 0.0006538470042869449, (0.00042811275, 5.3685865, 2.127321, 1.3002193)\n",
      "decoder loss ratio: 0.002357, decoder SINDy loss  ratio: 0.179461\n",
      "Epoch 26\n",
      "   training loss 0.0008080552215687931, (0.00059376645, 5.024753, 2.0102677, 1.326199)\n",
      "   validation loss 0.0006189249688759446, (0.00040456504, 5.2089095, 2.0109797, 1.326199)\n",
      "decoder loss ratio: 0.002227, decoder SINDy loss  ratio: 0.169646\n",
      "Epoch 27\n",
      "   training loss 0.0007908564293757081, (0.0005794951, 4.7481976, 1.9781713, 1.3544241)\n",
      "   validation loss 0.0005974043742753565, (0.00038924871, 4.9902043, 1.9461143, 1.3544241)\n",
      "decoder loss ratio: 0.002143, decoder SINDy loss  ratio: 0.164174\n",
      "Epoch 28\n",
      "   training loss 0.0007577927317470312, (0.0005527318, 4.597385, 1.9125425, 1.3806654)\n",
      "   validation loss 0.0005613600369542837, (0.00036218093, 4.885856, 1.8537248, 1.3806654)\n",
      "decoder loss ratio: 0.001994, decoder SINDy loss  ratio: 0.156380\n",
      "Epoch 29\n",
      "   training loss 0.0007067231927067041, (0.00050886825, 4.4740505, 1.83801, 1.4053929)\n",
      "   validation loss 0.0005104144220240414, (0.00032104822, 4.754885, 1.7531224, 1.4053929)\n",
      "decoder loss ratio: 0.001768, decoder SINDy loss  ratio: 0.147894\n",
      "Epoch 30\n",
      "   training loss 0.0006609471747651696, (0.000468423, 4.366588, 1.7823846, 1.4285649)\n",
      "   validation loss 0.0004698910051956773, (0.00028792172, 4.6602654, 1.6768364, 1.4285649)\n",
      "decoder loss ratio: 0.001585, decoder SINDy loss  ratio: 0.141458\n",
      "Epoch 31\n",
      "   training loss 0.000640739337541163, (0.0004529129, 4.2237477, 1.7329797, 1.4528443)\n",
      "   validation loss 0.00045932826469652355, (0.00028411628, 4.4549193, 1.6068356, 1.4528443)\n",
      "decoder loss ratio: 0.001564, decoder SINDy loss  ratio: 0.135553\n",
      "Epoch 32\n",
      "   training loss 0.0005862682592123747, (0.00040460937, 4.161761, 1.6689173, 1.476715)\n",
      "   validation loss 0.00041469678399153054, (0.00024742231, 4.374943, 1.5250732, 1.476715)\n",
      "decoder loss ratio: 0.001362, decoder SINDy loss  ratio: 0.128655\n",
      "Epoch 33\n",
      "   training loss 0.0006200856878422201, (0.00043704436, 3.9910088, 1.6803985, 1.5001453)\n",
      "   validation loss 0.00046671860036440194, (0.00029915024, 4.1433144, 1.5256691, 1.5001453)\n",
      "decoder loss ratio: 0.001647, decoder SINDy loss  ratio: 0.128706\n",
      "Epoch 34\n",
      "   training loss 0.0005313858273439109, (0.00036023953, 4.001945, 1.5593008, 1.5216228)\n",
      "   validation loss 0.0003841487632598728, (0.00022981169, 4.1277623, 1.3912084, 1.5216228)\n",
      "decoder loss ratio: 0.001265, decoder SINDy loss  ratio: 0.117363\n",
      "Epoch 35\n",
      "   training loss 0.0005221437313593924, (0.00035520078, 3.90463, 1.5151641, 1.5426544)\n",
      "   validation loss 0.0003863348683807999, (0.00023737251, 3.9873254, 1.3353581, 1.5426544)\n",
      "decoder loss ratio: 0.001307, decoder SINDy loss  ratio: 0.112651\n",
      "Epoch 36\n",
      "   training loss 0.00048730752314440906, (0.00032418297, 3.8092601, 1.4749782, 1.5626761)\n",
      "   validation loss 0.0003620754578150809, (0.00021785528, 3.8418171, 1.2859343, 1.5626761)\n",
      "decoder loss ratio: 0.001199, decoder SINDy loss  ratio: 0.108482\n",
      "Epoch 37\n",
      "   training loss 0.000490729114972055, (0.00032504485, 3.673743, 1.4986742, 1.5816853)\n",
      "   validation loss 0.0003829581546597183, (0.00023688028, 3.6568449, 1.3026102, 1.5816853)\n",
      "decoder loss ratio: 0.001304, decoder SINDy loss  ratio: 0.109888\n",
      "Epoch 38\n",
      "   training loss 0.00042100707651115954, (0.00026748813, 3.646661, 1.3753152, 1.5987445)\n",
      "   validation loss 0.0003203260712325573, (0.00018669904, 3.628624, 1.176396, 1.5987445)\n",
      "decoder loss ratio: 0.001028, decoder SINDy loss  ratio: 0.099241\n",
      "Epoch 39\n",
      "   training loss 0.00039976040716283023, (0.00024590315, 3.5014687, 1.3770564, 1.6151595)\n",
      "   validation loss 0.0003114435530733317, (0.00017792326, 3.4265482, 1.1736869, 1.6151595)\n",
      "decoder loss ratio: 0.000980, decoder SINDy loss  ratio: 0.099012\n",
      "Epoch 40\n",
      "   training loss 0.00037180373328737915, (0.00022138412, 3.4022725, 1.341193, 1.6300315)\n",
      "   validation loss 0.0002971212088596076, (0.00016684723, 3.3330102, 1.1397367, 1.6300315)\n",
      "decoder loss ratio: 0.000919, decoder SINDy loss  ratio: 0.096148\n",
      "Epoch 41\n",
      "   training loss 0.0003281704557593912, (0.0001852366, 3.2784023, 1.2648406, 1.6449809)\n",
      "   validation loss 0.00026283893384970725, (0.00013944219, 3.2138088, 1.0694693, 1.6449809)\n",
      "decoder loss ratio: 0.000768, decoder SINDy loss  ratio: 0.090221\n",
      "Epoch 42\n",
      "   training loss 0.0002957302494905889, (0.00015992223, 3.1493065, 1.1921568, 1.6592323)\n",
      "   validation loss 0.0002418199146632105, (0.00012440792, 3.1048458, 1.0081968, 1.6592323)\n",
      "decoder loss ratio: 0.000685, decoder SINDy loss  ratio: 0.085052\n",
      "Epoch 43\n",
      "   training loss 0.0002685133076738566, (0.00013750108, 2.9994717, 1.1429057, 1.6721655)\n",
      "   validation loss 0.00022586443810723722, (0.000111964575, 2.9935625, 0.97178215, 1.6721655)\n",
      "decoder loss ratio: 0.000616, decoder SINDy loss  ratio: 0.081980\n",
      "Epoch 44\n",
      "   training loss 0.00024259676865767688, (0.000116879455, 2.8462274, 1.0888088, 1.6836438)\n",
      "   validation loss 0.0002105491585098207, (0.00010031531, 2.8908677, 0.9339742, 1.6836438)\n",
      "decoder loss ratio: 0.000552, decoder SINDy loss  ratio: 0.078790\n",
      "Epoch 45\n",
      "   training loss 0.00022416144201997668, (0.00010492442, 2.663741, 1.022887, 1.6948342)\n",
      "   validation loss 0.00020122497517149895, (9.564221e-05, 2.7323303, 0.8863443, 1.6948342)\n",
      "decoder loss ratio: 0.000527, decoder SINDy loss  ratio: 0.074772\n",
      "Epoch 46\n",
      "   training loss 0.0002343510277569294, (0.00011836011, 2.499777, 0.9893992, 1.7051007)\n",
      "   validation loss 0.00022234718198888004, (0.00011808509, 2.6202178, 0.87211096, 1.7051007)\n",
      "decoder loss ratio: 0.000650, decoder SINDy loss  ratio: 0.073571\n",
      "Epoch 47\n",
      "   training loss 0.0001866605889517814, (7.92432e-05, 2.3169186, 0.90261555, 1.7155831)\n",
      "   validation loss 0.00017801196372602135, (8.0087135e-05, 2.4828844, 0.80768996, 1.7155831)\n",
      "decoder loss ratio: 0.000441, decoder SINDy loss  ratio: 0.068137\n",
      "Epoch 48\n",
      "   training loss 0.00016849834355525672, (6.851507e-05, 2.1511478, 0.8271383, 1.7269446)\n",
      "   validation loss 0.00016494888404849917, (7.2231036e-05, 2.3757815, 0.7544842, 1.7269446)\n",
      "decoder loss ratio: 0.000398, decoder SINDy loss  ratio: 0.063648\n",
      "Epoch 49\n",
      "   training loss 0.0001628169120522216, (6.720883e-05, 1.9816957, 0.7822358, 1.7384502)\n",
      "   validation loss 0.00016379624139517546, (7.369014e-05, 2.2243042, 0.727216, 1.7384502)\n",
      "decoder loss ratio: 0.000406, decoder SINDy loss  ratio: 0.061348\n",
      "Epoch 50\n",
      "   training loss 0.00014064057904761285, (5.185364e-05, 1.8428539, 0.7129148, 1.749546)\n",
      "   validation loss 0.00014383876987267286, (5.8581554e-05, 2.1201713, 0.6776175, 1.749546)\n",
      "decoder loss ratio: 0.000323, decoder SINDy loss  ratio: 0.057164\n",
      "Epoch 51\n",
      "   training loss 0.00013552008022088557, (5.1988136e-05, 1.7063106, 0.65919495, 1.7612454)\n",
      "   validation loss 0.00014116406964603812, (5.9540253e-05, 1.9875193, 0.64011353, 1.7612454)\n",
      "decoder loss ratio: 0.000328, decoder SINDy loss  ratio: 0.054000\n",
      "Epoch 52\n",
      "   training loss 0.0001317373535130173, (5.4384655e-05, 1.5869309, 0.5959276, 1.775995)\n",
      "   validation loss 0.00013934355229139328, (6.228947e-05, 1.8785449, 0.5929412, 1.775995)\n",
      "decoder loss ratio: 0.000343, decoder SINDy loss  ratio: 0.050021\n",
      "Epoch 53\n",
      "   training loss 0.00011732265556929633, (4.4621236e-05, 1.4938093, 0.5478759, 1.7913837)\n",
      "   validation loss 0.0001260775316040963, (5.2383024e-05, 1.7965622, 0.5578067, 1.7913837)\n",
      "decoder loss ratio: 0.000288, decoder SINDy loss  ratio: 0.047057\n",
      "Epoch 54\n",
      "   training loss 0.0001175310171674937, (4.8869813e-05, 1.4128805, 0.50564754, 1.8096453)\n",
      "   validation loss 0.0001270258944714442, (5.6455992e-05, 1.702144, 0.5247345, 1.8096453)\n",
      "decoder loss ratio: 0.000311, decoder SINDy loss  ratio: 0.044267\n",
      "Epoch 55\n",
      "   training loss 0.00010483233199920505, (3.9552793e-05, 1.3514866, 0.47006565, 1.8272976)\n",
      "   validation loss 0.00011492733028717339, (4.704909e-05, 1.6423801, 0.49605277, 1.8272976)\n",
      "decoder loss ratio: 0.000259, decoder SINDy loss  ratio: 0.041847\n",
      "Epoch 56\n",
      "   training loss 0.0001054554886650294, (4.4181368e-05, 1.279842, 0.42824724, 1.8449402)\n",
      "   validation loss 0.00011577742407098413, (5.1141942e-05, 1.5442662, 0.4618608, 1.8449402)\n",
      "decoder loss ratio: 0.000282, decoder SINDy loss  ratio: 0.038963\n",
      "Epoch 57\n",
      "   training loss 0.00010413781274110079, (4.6043617e-05, 1.2118111, 0.39463753, 1.8630451)\n",
      "   validation loss 0.00011447511496953666, (5.2467924e-05, 1.4566953, 0.43376744, 1.8630451)\n",
      "decoder loss ratio: 0.000289, decoder SINDy loss  ratio: 0.036593\n",
      "Epoch 58\n",
      "   training loss 0.00011577281111385673, (5.826265e-05, 1.159707, 0.38706362, 1.8803796)\n",
      "   validation loss 0.0001266751205548644, (6.547219e-05, 1.3931048, 0.42399132, 1.8803796)\n",
      "decoder loss ratio: 0.000360, decoder SINDy loss  ratio: 0.035768\n",
      "Epoch 59\n",
      "   training loss 9.294049232266843e-05, (3.984609e-05, 1.1189783, 0.34123814, 1.8970592)\n",
      "   validation loss 0.00010209379979642108, (4.4598426e-05, 1.3245758, 0.38524786, 1.8970592)\n",
      "decoder loss ratio: 0.000246, decoder SINDy loss  ratio: 0.032500\n",
      "Epoch 60\n",
      "   training loss 8.289569814223796e-05, (3.177907e-05, 1.083869, 0.31985182, 1.9131444)\n",
      "   validation loss 9.170263365376741e-05, (3.6216687e-05, 1.2739706, 0.36354506, 1.9131444)\n",
      "decoder loss ratio: 0.000199, decoder SINDy loss  ratio: 0.030669\n",
      "Epoch 61\n",
      "   training loss 8.736995368963107e-05, (3.8537342e-05, 1.0335292, 0.29538578, 1.9294034)\n",
      "   validation loss 9.541853069094941e-05, (4.1957654e-05, 1.1970302, 0.34166843, 1.9294034)\n",
      "decoder loss ratio: 0.000231, decoder SINDy loss  ratio: 0.028823\n",
      "Epoch 62\n",
      "   training loss 8.956151577876881e-05, (4.1660693e-05, 1.0120535, 0.28456858, 1.9443966)\n",
      "   validation loss 9.775471698958427e-05, (4.5485343e-05, 1.1600204, 0.32825413, 1.9443966)\n",
      "decoder loss ratio: 0.000250, decoder SINDy loss  ratio: 0.027692\n",
      "Epoch 63\n",
      "   training loss 7.864645158406347e-05, (3.300823e-05, 0.9622169, 0.26046738, 1.9591484)\n",
      "   validation loss 8.572477236157283e-05, (3.550489e-05, 1.0891885, 0.30628398, 1.9591484)\n",
      "decoder loss ratio: 0.000195, decoder SINDy loss  ratio: 0.025838\n",
      "Epoch 64\n",
      "   training loss 7.950744475238025e-05, (3.5526627e-05, 0.91500705, 0.24238144, 1.9742674)\n",
      "   validation loss 8.590395736973733e-05, (3.732811e-05, 1.0281471, 0.28833178, 1.9742674)\n",
      "decoder loss ratio: 0.000206, decoder SINDy loss  ratio: 0.024324\n",
      "Epoch 65\n",
      "   training loss 7.757855200907215e-05, (3.4433004e-05, 0.87646675, 0.23259789, 1.9885758)\n",
      "   validation loss 8.360031642951071e-05, (3.6151363e-05, 0.98552287, 0.27563193, 1.9885758)\n",
      "decoder loss ratio: 0.000199, decoder SINDy loss  ratio: 0.023252\n",
      "Epoch 66\n",
      "   training loss 8.066420559771359e-05, (3.9231807e-05, 0.84047526, 0.21407643, 2.002476)\n",
      "   validation loss 8.561522554373369e-05, (3.9726867e-05, 0.9260962, 0.258636, 2.002476)\n",
      "decoder loss ratio: 0.000219, decoder SINDy loss  ratio: 0.021819\n",
      "Epoch 67\n",
      "   training loss 9.851362119661644e-05, (5.6729485e-05, 0.792819, 0.21624702, 2.0159438)\n",
      "   validation loss 0.00010522265074541792, (5.9516395e-05, 0.8728294, 0.2554682, 2.0159438)\n",
      "decoder loss ratio: 0.000328, decoder SINDy loss  ratio: 0.021551\n",
      "Epoch 68\n",
      "   training loss 8.015158527996391e-05, (4.0913677e-05, 0.77441555, 0.1894803, 2.0289876)\n",
      "   validation loss 8.38787600514479e-05, (4.0428356e-05, 0.8347428, 0.23160532, 2.0289876)\n",
      "decoder loss ratio: 0.000223, decoder SINDy loss  ratio: 0.019538\n",
      "Epoch 69\n",
      "   training loss 7.726709736743942e-05, (3.8711005e-05, 0.7483053, 0.18140182, 2.0415914)\n",
      "   validation loss 7.999123772606254e-05, (3.7457856e-05, 0.793782, 0.2211747, 2.0415914)\n",
      "decoder loss ratio: 0.000206, decoder SINDy loss  ratio: 0.018658\n",
      "Epoch 70\n",
      "   training loss 7.907438703114167e-05, (4.1536565e-05, 0.71607596, 0.17002615, 2.0535207)\n",
      "   validation loss 8.16181127447635e-05, (4.020012e-05, 0.7526209, 0.2088279, 2.0535207)\n",
      "decoder loss ratio: 0.000221, decoder SINDy loss  ratio: 0.017617\n",
      "Epoch 71\n",
      "   training loss 0.0001012179272947833, (6.475978e-05, 0.6961827, 0.15811935, 2.0646212)\n",
      "   validation loss 0.00010393678530817851, (6.363415e-05, 0.7220869, 0.19656424, 2.0646212)\n",
      "decoder loss ratio: 0.000350, decoder SINDy loss  ratio: 0.016582\n",
      "Epoch 72\n",
      "   training loss 6.986009248066694e-05, (3.345865e-05, 0.6391283, 0.15661237, 2.0740206)\n",
      "   validation loss 7.258308323798701e-05, (3.2675256e-05, 0.6702493, 0.19167621, 2.0740206)\n",
      "decoder loss ratio: 0.000180, decoder SINDy loss  ratio: 0.016170\n",
      "Epoch 73\n",
      "   training loss 8.751763380132616e-05, (5.2381132e-05, 0.62924016, 0.14296949, 2.0839548)\n",
      "   validation loss 8.910853648558259e-05, (5.048384e-05, 0.6458332, 0.17785144, 2.0839548)\n",
      "decoder loss ratio: 0.000278, decoder SINDy loss  ratio: 0.015004\n",
      "Epoch 74\n",
      "   training loss 6.570082041434944e-05, (3.0570554e-05, 0.588738, 0.14205655, 2.0924613)\n",
      "   validation loss 6.663663225481287e-05, (2.841329e-05, 0.6065121, 0.17298728, 2.0924613)\n",
      "decoder loss ratio: 0.000156, decoder SINDy loss  ratio: 0.014593\n",
      "Epoch 75\n",
      "   training loss 9.937844151863828e-05, (6.542486e-05, 0.589318, 0.12940386, 2.1013198)\n",
      "   validation loss 0.00010090199793921784, (6.375336e-05, 0.59022355, 0.16135438, 2.1013198)\n",
      "decoder loss ratio: 0.000351, decoder SINDy loss  ratio: 0.013612\n",
      "Epoch 76\n",
      "   training loss 5.92917094763834e-05, (2.522748e-05, 0.53697366, 0.12981729, 2.10825)\n",
      "   validation loss 5.969695484964177e-05, (2.2909628e-05, 0.54633516, 0.1570483, 2.10825)\n",
      "decoder loss ratio: 0.000126, decoder SINDy loss  ratio: 0.013249\n",
      "Epoch 77\n",
      "   training loss 6.62489328533411e-05, (3.2833454e-05, 0.5285504, 0.12248461, 2.1167023)\n",
      "   validation loss 6.616530299652368e-05, (3.0086223e-05, 0.5194212, 0.14912052, 2.1167023)\n",
      "decoder loss ratio: 0.000166, decoder SINDy loss  ratio: 0.012580\n",
      "Epoch 78\n",
      "   training loss 7.975244079716504e-05, (4.6938494e-05, 0.52327675, 0.115439475, 2.1269996)\n",
      "   validation loss 7.983828254509717e-05, (4.4483484e-05, 0.50269425, 0.140848, 2.1269996)\n",
      "decoder loss ratio: 0.000245, decoder SINDy loss  ratio: 0.011882\n",
      "Epoch 79\n",
      "   training loss 6.187087274156511e-05, (2.8959254e-05, 0.4895804, 0.115526706, 2.1358953)\n",
      "   validation loss 6.13982992945239e-05, (2.635316e-05, 0.47228682, 0.13686192, 2.1358953)\n",
      "decoder loss ratio: 0.000145, decoder SINDy loss  ratio: 0.011546\n",
      "Epoch 80\n",
      "   training loss 6.541483890032396e-05, (3.2940734e-05, 0.48546806, 0.1101519, 2.145892)\n",
      "   validation loss 6.465375918196514e-05, (3.0138322e-05, 0.4511136, 0.13056518, 2.145892)\n",
      "decoder loss ratio: 0.000166, decoder SINDy loss  ratio: 0.011014\n",
      "Epoch 81\n",
      "   training loss 7.610308239236474e-05, (4.4194217e-05, 0.4777837, 0.103612036, 2.1547658)\n",
      "   validation loss 7.578081567771733e-05, (4.1922325e-05, 0.43655947, 0.12310839, 2.1547658)\n",
      "decoder loss ratio: 0.000231, decoder SINDy loss  ratio: 0.010385\n",
      "Epoch 82\n",
      "   training loss 5.8667057601269335e-05, (2.6627913e-05, 0.44849238, 0.10411203, 2.1627944)\n",
      "   validation loss 5.774191959062591e-05, (2.4079127e-05, 0.41209573, 0.120348535, 2.1627944)\n",
      "decoder loss ratio: 0.000133, decoder SINDy loss  ratio: 0.010153\n",
      "Epoch 83\n",
      "   training loss 6.296409264905378e-05, (3.1142503e-05, 0.44420162, 0.101010635, 2.1720526)\n",
      "   validation loss 6.185090751387179e-05, (2.8492826e-05, 0.39456356, 0.116375595, 2.1720526)\n",
      "decoder loss ratio: 0.000157, decoder SINDy loss  ratio: 0.009817\n",
      "Epoch 84\n",
      "   training loss 7.99171975813806e-05, (4.8750593e-05, 0.43896806, 0.09363863, 2.1802745)\n",
      "   validation loss 7.96888634795323e-05, (4.7038262e-05, 0.38843438, 0.10847861, 2.1802745)\n",
      "decoder loss ratio: 0.000259, decoder SINDy loss  ratio: 0.009151\n",
      "Epoch 85\n",
      "   training loss 6.850229692645371e-05, (3.6847458e-05, 0.40529212, 0.09785498, 2.1869342)\n",
      "   validation loss 6.801554991398007e-05, (3.531339e-05, 0.36101073, 0.10832821, 2.1869342)\n",
      "decoder loss ratio: 0.000194, decoder SINDy loss  ratio: 0.009139\n",
      "Epoch 86\n",
      "   training loss 5.8667970733949915e-05, (2.7215181e-05, 0.40433732, 0.09506902, 2.194589)\n",
      "   validation loss 5.7684886996867135e-05, (2.5333824e-05, 0.35192758, 0.10405173, 2.194589)\n",
      "decoder loss ratio: 0.000139, decoder SINDy loss  ratio: 0.008778\n",
      "Epoch 87\n",
      "   training loss 6.61493104416877e-05, (3.4919653e-05, 0.37879804, 0.09227683, 2.2001977)\n",
      "   validation loss 6.567371019627899e-05, (3.362953e-05, 0.3357572, 0.100422084, 2.2001977)\n",
      "decoder loss ratio: 0.000185, decoder SINDy loss  ratio: 0.008472\n",
      "Epoch 88\n",
      "   training loss 5.834562762174755e-05, (2.7453825e-05, 0.38131896, 0.088226646, 2.206914)\n",
      "   validation loss 5.738272739108652e-05, (2.5796258e-05, 0.32645008, 0.09517334, 2.206914)\n",
      "decoder loss ratio: 0.000142, decoder SINDy loss  ratio: 0.008029\n",
      "Epoch 89\n",
      "   training loss 8.020740642677993e-05, (4.881053e-05, 0.35877216, 0.092759356, 2.2120945)\n",
      "   validation loss 8.022724068723619e-05, (4.8518086e-05, 0.31154212, 0.095882095, 2.2120945)\n",
      "decoder loss ratio: 0.000267, decoder SINDy loss  ratio: 0.008089\n",
      "Epoch 90\n",
      "   training loss 5.225945278652944e-05, (2.179528e-05, 0.35927257, 0.08288245, 2.217593)\n",
      "   validation loss 5.090177364763804e-05, (1.9923105e-05, 0.30015984, 0.08802739, 2.217593)\n",
      "decoder loss ratio: 0.000110, decoder SINDy loss  ratio: 0.007426\n",
      "Epoch 91\n",
      "   training loss 5.220045568421483e-05, (2.1996017e-05, 0.34980926, 0.07976138, 2.2228303)\n",
      "   validation loss 5.0526570703368634e-05, (1.992431e-05, 0.29042065, 0.08373961, 2.2228303)\n",
      "decoder loss ratio: 0.000110, decoder SINDy loss  ratio: 0.007064\n",
      "Epoch 92\n",
      "   training loss 5.965640229987912e-05, (2.9282204e-05, 0.34597793, 0.080948874, 2.227931)\n",
      "   validation loss 5.8321333199273795e-05, (2.7757538e-05, 0.2843632, 0.082844846, 2.227931)\n",
      "decoder loss ratio: 0.000153, decoder SINDy loss  ratio: 0.006989\n",
      "Epoch 93\n",
      "   training loss 5.179944128030911e-05, (2.1457714e-05, 0.32908258, 0.0802283, 2.23189)\n",
      "   validation loss 5.045584839535877e-05, (2.0044377e-05, 0.26964304, 0.080925725, 2.23189)\n",
      "decoder loss ratio: 0.000110, decoder SINDy loss  ratio: 0.006827\n",
      "Epoch 94\n",
      "   training loss 8.936785161495209e-05, (5.8822843e-05, 0.31754553, 0.08193144, 2.2351868)\n",
      "   validation loss 9.024643077282235e-05, (5.9760157e-05, 0.2589631, 0.081344046, 2.2351868)\n",
      "decoder loss ratio: 0.000329, decoder SINDy loss  ratio: 0.006862\n",
      "Epoch 95\n",
      "   training loss 4.931067087454721e-05, (1.9382367e-05, 0.32354358, 0.07535462, 2.2392843)\n",
      "   validation loss 4.811652979697101e-05, (1.8240104e-05, 0.2567361, 0.074835844, 2.2392843)\n",
      "decoder loss ratio: 0.000100, decoder SINDy loss  ratio: 0.006313\n",
      "Epoch 96\n",
      "   training loss 4.3543677747948095e-05, (1.3987839e-05, 0.31268513, 0.07145013, 2.2410827)\n",
      "   validation loss 4.180661926511675e-05, (1.2280022e-05, 0.25006053, 0.07115773, 2.2410827)\n",
      "decoder loss ratio: 0.000068, decoder SINDy loss  ratio: 0.006003\n",
      "Epoch 97\n",
      "   training loss 5.372234227252193e-05, (2.405002e-05, 0.31055084, 0.07224152, 2.244817)\n",
      "   validation loss 5.2766568842343986e-05, (2.3310658e-05, 0.24236062, 0.07007739, 2.244817)\n",
      "decoder loss ratio: 0.000128, decoder SINDy loss  ratio: 0.005912\n",
      "Epoch 98\n",
      "   training loss 6.35242322459817e-05, (3.350914e-05, 0.29207447, 0.075469986, 2.2468097)\n",
      "   validation loss 6.241985829547048e-05, (3.28783e-05, 0.2320161, 0.07073461, 2.2468097)\n",
      "decoder loss ratio: 0.000181, decoder SINDy loss  ratio: 0.005967\n",
      "Epoch 99\n",
      "   training loss 5.503238207893446e-05, (2.5713918e-05, 0.3017116, 0.06825826, 2.2492638)\n",
      "   validation loss 5.4427768191089854e-05, (2.5415218e-05, 0.2307139, 0.06519913, 2.2492638)\n",
      "decoder loss ratio: 0.000140, decoder SINDy loss  ratio: 0.005500\n",
      "Epoch 100\n",
      "   training loss 4.419196193339303e-05, (1.50763735e-05, 0.291294, 0.06607547, 2.2508044)\n",
      "   validation loss 4.2817831854335964e-05, (1.4014974e-05, 0.2194573, 0.06294815, 2.2508044)\n",
      "decoder loss ratio: 0.000077, decoder SINDy loss  ratio: 0.005310\n",
      "Epoch 101\n",
      "   training loss 7.685880700591952e-05, (4.6443918e-05, 0.27793753, 0.07895845, 2.2519047)\n",
      "   validation loss 7.653477223357186e-05, (4.7068508e-05, 0.21726741, 0.06947218, 2.2519047)\n",
      "decoder loss ratio: 0.000259, decoder SINDy loss  ratio: 0.005861\n",
      "Epoch 102\n",
      "   training loss 4.5153887185733765e-05, (1.6109087e-05, 0.2795595, 0.06516957, 2.2527843)\n",
      "   validation loss 4.359139347798191e-05, (1.51087315e-05, 0.2107397, 0.059548207, 2.2527843)\n",
      "decoder loss ratio: 0.000083, decoder SINDy loss  ratio: 0.005023\n",
      "Epoch 103\n",
      "   training loss 5.176913691684604e-05, (2.2987395e-05, 0.28021476, 0.062312707, 2.255047)\n",
      "   validation loss 5.11236212332733e-05, (2.2848224e-05, 0.20543835, 0.057249296, 2.255047)\n",
      "decoder loss ratio: 0.000126, decoder SINDy loss  ratio: 0.004830\n",
      "Epoch 104\n",
      "   training loss 5.7934405049309134e-05, (2.9197487e-05, 0.2790233, 0.0617738, 2.255954)\n",
      "   validation loss 5.7840748922899365e-05, (2.964274e-05, 0.20126407, 0.056384698, 2.255954)\n",
      "decoder loss ratio: 0.000163, decoder SINDy loss  ratio: 0.004757\n",
      "Epoch 105\n",
      "   training loss 4.4331543904263526e-05, (1.543609e-05, 0.26140967, 0.06343753, 2.2551703)\n",
      "   validation loss 4.3161526264157146e-05, (1.5031749e-05, 0.19171806, 0.055780742, 2.2551703)\n",
      "decoder loss ratio: 0.000083, decoder SINDy loss  ratio: 0.004706\n",
      "Epoch 106\n",
      "   training loss 5.494458309840411e-05, (2.6555153e-05, 0.2657307, 0.058226857, 2.2566745)\n",
      "   validation loss 5.462715489557013e-05, (2.685912e-05, 0.1893668, 0.05201291, 2.2566745)\n",
      "decoder loss ratio: 0.000148, decoder SINDy loss  ratio: 0.004388\n",
      "Epoch 107\n",
      "   training loss 5.687381781172007e-05, (2.8089838e-05, 0.25330743, 0.06230071, 2.255391)\n",
      "   validation loss 5.628177314065397e-05, (2.838656e-05, 0.18103999, 0.053413063, 2.255391)\n",
      "decoder loss ratio: 0.000156, decoder SINDy loss  ratio: 0.004506\n",
      "Epoch 108\n",
      "   training loss 4.540789086604491e-05, (1.6848691e-05, 0.25982276, 0.05997944, 2.2561255)\n",
      "   validation loss 4.462276410777122e-05, (1.6876877e-05, 0.17739469, 0.051846337, 2.2561255)\n",
      "decoder loss ratio: 0.000093, decoder SINDy loss  ratio: 0.004374\n",
      "Epoch 109\n",
      "   training loss 3.998367901658639e-05, (1.153306e-05, 0.25217018, 0.059086297, 2.2541993)\n",
      "   validation loss 3.8473801396321505e-05, (1.0876338e-05, 0.1763251, 0.05055474, 2.2541993)\n",
      "decoder loss ratio: 0.000060, decoder SINDy loss  ratio: 0.004265\n",
      "Epoch 110\n",
      "   training loss 4.82502109662164e-05, (1.9681976e-05, 0.24703097, 0.0601962, 2.2548616)\n",
      "   validation loss 4.761283707921393e-05, (2.004167e-05, 0.16961479, 0.050225522, 2.2548616)\n",
      "decoder loss ratio: 0.000110, decoder SINDy loss  ratio: 0.004237\n",
      "Epoch 111\n",
      "   training loss 4.0120161429513246e-05, (1.1959409e-05, 0.24081914, 0.056214727, 2.253928)\n",
      "   validation loss 3.8778060115873814e-05, (1.15426965e-05, 0.1652251, 0.046960846, 2.253928)\n",
      "decoder loss ratio: 0.000064, decoder SINDy loss  ratio: 0.003962\n",
      "Epoch 112\n",
      "   training loss 4.489738057600334e-05, (1.707561e-05, 0.23836881, 0.05290209, 2.2531564)\n",
      "   validation loss 4.3953798012807965e-05, (1.69823e-05, 0.16242589, 0.044399347, 2.2531564)\n",
      "decoder loss ratio: 0.000094, decoder SINDy loss  ratio: 0.003746\n",
      "Epoch 113\n",
      "   training loss 4.958143108524382e-05, (2.1805576e-05, 0.2371362, 0.052546807, 2.2521174)\n",
      "   validation loss 4.897888356936164e-05, (2.2026055e-05, 0.15863144, 0.044316556, 2.2521174)\n",
      "decoder loss ratio: 0.000121, decoder SINDy loss  ratio: 0.003739\n",
      "Epoch 114\n",
      "   training loss 4.8420413804706186e-05, (2.0504709e-05, 0.23275892, 0.05411019, 2.2504685)\n",
      "   validation loss 4.820518734049983e-05, (2.1184696e-05, 0.15563154, 0.04515807, 2.2504685)\n",
      "decoder loss ratio: 0.000117, decoder SINDy loss  ratio: 0.003810\n",
      "Epoch 115\n",
      "   training loss 3.839123382931575e-05, (1.0473349e-05, 0.22096233, 0.054334022, 2.2484484)\n",
      "   validation loss 3.711491808644496e-05, (1.0253304e-05, 0.15082936, 0.043771315, 2.2484484)\n",
      "decoder loss ratio: 0.000056, decoder SINDy loss  ratio: 0.003693\n",
      "Epoch 116\n",
      "   training loss 3.5932294849772006e-05, (8.360339e-06, 0.21852855, 0.051127795, 2.2459176)\n",
      "   validation loss 3.456435661064461e-05, (7.955494e-06, 0.14981945, 0.041496854, 2.2459176)\n",
      "decoder loss ratio: 0.000044, decoder SINDy loss  ratio: 0.003501\n",
      "Epoch 117\n",
      "   training loss 4.311835436965339e-05, (1.5450712e-05, 0.21786444, 0.052149124, 2.245273)\n",
      "   validation loss 4.2259591282345355e-05, (1.565532e-05, 0.1446918, 0.04151541, 2.245273)\n",
      "decoder loss ratio: 0.000086, decoder SINDy loss  ratio: 0.003502\n",
      "Epoch 118\n",
      "   training loss 4.505789911490865e-05, (1.7914143e-05, 0.21461587, 0.047096815, 2.2434075)\n",
      "   validation loss 4.417030868353322e-05, (1.7925184e-05, 0.14242952, 0.03811048, 2.2434075)\n",
      "decoder loss ratio: 0.000099, decoder SINDy loss  ratio: 0.003215\n",
      "Epoch 119\n",
      "   training loss 5.210589006310329e-05, (2.4989215e-05, 0.21370055, 0.04700079, 2.2416596)\n",
      "   validation loss 5.082881762064062e-05, (2.4604655e-05, 0.13664415, 0.03807567, 2.2416596)\n",
      "decoder loss ratio: 0.000135, decoder SINDy loss  ratio: 0.003212\n",
      "Epoch 120\n",
      "   training loss 5.082684219814837e-05, (2.3815033e-05, 0.2125444, 0.04625348, 2.2386463)\n",
      "   validation loss 5.0853916036430746e-05, (2.4773528e-05, 0.13668089, 0.036939252, 2.2386463)\n",
      "decoder loss ratio: 0.000136, decoder SINDy loss  ratio: 0.003116\n",
      "Epoch 121\n",
      "   training loss 4.441849887371063e-05, (1.7384267e-05, 0.20429707, 0.04677218, 2.2357013)\n",
      "   validation loss 4.3867737986147404e-05, (1.784014e-05, 0.1337781, 0.036705863, 2.2357013)\n",
      "decoder loss ratio: 0.000098, decoder SINDy loss  ratio: 0.003097\n",
      "Epoch 122\n",
      "   training loss 5.102539580548182e-05, (2.4083054e-05, 0.2040868, 0.04601662, 2.2340682)\n",
      "   validation loss 4.99548768857494e-05, (2.3984985e-05, 0.1286806, 0.0362921, 2.2340682)\n",
      "decoder loss ratio: 0.000132, decoder SINDy loss  ratio: 0.003062\n",
      "Epoch 123\n",
      "   training loss 4.4904594687977806e-05, (1.8042518e-05, 0.20127924, 0.045563173, 2.230576)\n",
      "   validation loss 4.469702980713919e-05, (1.8858917e-05, 0.12879953, 0.03532353, 2.230576)\n",
      "decoder loss ratio: 0.000104, decoder SINDy loss  ratio: 0.002980\n",
      "Epoch 124\n",
      "   training loss 5.076789966551587e-05, (2.4174511e-05, 0.20229149, 0.04308782, 2.2284608)\n",
      "   validation loss 5.055513247498311e-05, (2.4876452e-05, 0.12596609, 0.03394075, 2.2284608)\n",
      "decoder loss ratio: 0.000137, decoder SINDy loss  ratio: 0.002863\n",
      "Epoch 125\n",
      "   training loss 3.738343366421759e-05, (1.0250349e-05, 0.19785799, 0.04885408, 2.224768)\n",
      "   validation loss 3.660732909338549e-05, (1.07032365e-05, 0.12203345, 0.03656413, 2.224768)\n",
      "decoder loss ratio: 0.000059, decoder SINDy loss  ratio: 0.003085\n",
      "Epoch 126\n",
      "   training loss 5.158103158464655e-05, (2.5042737e-05, 0.19391131, 0.043032024, 2.2235096)\n",
      "   validation loss 5.1226787036284804e-05, (2.5700147e-05, 0.11886183, 0.032915447, 2.2235096)\n",
      "decoder loss ratio: 0.000142, decoder SINDy loss  ratio: 0.002777\n",
      "Epoch 127\n",
      "   training loss 4.0237409848487005e-05, (1.3412668e-05, 0.20107377, 0.04624, 2.2200742)\n",
      "   validation loss 3.9446626033168286e-05, (1.3815068e-05, 0.119986735, 0.034308158, 2.2200742)\n",
      "decoder loss ratio: 0.000076, decoder SINDy loss  ratio: 0.002894\n",
      "Epoch 128\n",
      "   training loss 3.677358108689077e-05, (1.0098394e-05, 0.18583934, 0.044885922, 2.2186596)\n",
      "   validation loss 3.6022724088979885e-05, (1.0506569e-05, 0.11466834, 0.033295598, 2.2186596)\n",
      "decoder loss ratio: 0.000058, decoder SINDy loss  ratio: 0.002809\n",
      "Epoch 129\n",
      "   training loss 3.917096182703972e-05, (1.28139945e-05, 0.18872964, 0.042259846, 2.2130983)\n",
      "   validation loss 3.8598966057179496e-05, (1.32617715e-05, 0.11670008, 0.032062117, 2.2130983)\n",
      "decoder loss ratio: 0.000073, decoder SINDy loss  ratio: 0.002705\n",
      "Epoch 130\n",
      "   training loss 4.217040986986831e-05, (1.51928e-05, 0.19386195, 0.048637748, 2.2113836)\n",
      "   validation loss 4.1450759454164654e-05, (1.5846377e-05, 0.11369733, 0.03490546, 2.2113836)\n",
      "decoder loss ratio: 0.000087, decoder SINDy loss  ratio: 0.002945\n",
      "Epoch 131\n",
      "   training loss 5.269709799904376e-05, (2.656929e-05, 0.20031695, 0.04030079, 2.2097726)\n",
      "   validation loss 5.3194002248346806e-05, (2.807382e-05, 0.11358213, 0.030224567, 2.2097726)\n",
      "decoder loss ratio: 0.000155, decoder SINDy loss  ratio: 0.002550\n",
      "Epoch 132\n",
      "   training loss 3.635011307778768e-05, (1.0142796e-05, 0.18574263, 0.041443642, 2.2062955)\n",
      "   validation loss 3.544285209500231e-05, (1.03811035e-05, 0.10828913, 0.029987942, 2.2062955)\n",
      "decoder loss ratio: 0.000057, decoder SINDy loss  ratio: 0.002530\n",
      "Epoch 133\n",
      "   training loss 4.145320417592302e-05, (1.5542993e-05, 0.18759525, 0.038703714, 2.203984)\n",
      "   validation loss 4.0871440432965755e-05, (1.5996598e-05, 0.107946075, 0.028350016, 2.203984)\n",
      "decoder loss ratio: 0.000088, decoder SINDy loss  ratio: 0.002392\n",
      "Epoch 134\n",
      "   training loss 4.669814370572567e-05, (2.07251e-05, 0.19088654, 0.03965587, 2.2007458)\n",
      "   validation loss 4.673166768043302e-05, (2.1846989e-05, 0.10804028, 0.028772216, 2.2007458)\n",
      "decoder loss ratio: 0.000120, decoder SINDy loss  ratio: 0.002427\n",
      "Epoch 135\n",
      "   training loss 6.0960832342971116e-05, (3.5031866e-05, 0.1897459, 0.039472617, 2.1981707)\n",
      "   validation loss 6.145681982161477e-05, (3.6620346e-05, 0.10787614, 0.028547699, 2.1981707)\n",
      "decoder loss ratio: 0.000202, decoder SINDy loss  ratio: 0.002408\n",
      "Epoch 136\n",
      "   training loss 3.2609066693112254e-05, (6.6327866e-06, 0.18723221, 0.040335596, 2.1942723)\n",
      "   validation loss 3.161597123835236e-05, (6.828318e-06, 0.102927625, 0.028449295, 2.1942723)\n",
      "decoder loss ratio: 0.000038, decoder SINDy loss  ratio: 0.002400\n",
      "Epoch 137\n",
      "   training loss 6.841382855782285e-05, (4.2419393e-05, 0.18899928, 0.040646058, 2.192983)\n",
      "   validation loss 6.82552854414098e-05, (4.340333e-05, 0.104541376, 0.029221218, 2.192983)\n",
      "decoder loss ratio: 0.000239, decoder SINDy loss  ratio: 0.002465\n",
      "Epoch 138\n",
      "   training loss 3.490100061753765e-05, (8.841427e-06, 0.18609004, 0.041712355, 2.188834)\n",
      "   validation loss 3.4097338357241824e-05, (9.313252e-06, 0.10022843, 0.028957464, 2.188834)\n",
      "decoder loss ratio: 0.000051, decoder SINDy loss  ratio: 0.002443\n",
      "Epoch 139\n",
      "   training loss 3.646203549578786e-05, (1.07366e-05, 0.17908211, 0.03850601, 2.1874833)\n",
      "   validation loss 3.596467649913393e-05, (1.137028e-05, 0.09977383, 0.027195657, 2.1874833)\n",
      "decoder loss ratio: 0.000063, decoder SINDy loss  ratio: 0.002294\n",
      "Epoch 140\n",
      "   training loss 3.774344804696739e-05, (1.2093664e-05, 0.18506372, 0.038185142, 2.1831272)\n",
      "   validation loss 3.7361514841904864e-05, (1.2765038e-05, 0.09932501, 0.027652051, 2.1831272)\n",
      "decoder loss ratio: 0.000070, decoder SINDy loss  ratio: 0.002333\n",
      "Epoch 141\n",
      "   training loss 3.43284809787292e-05, (8.649467e-06, 0.18124653, 0.03870124, 2.1808891)\n",
      "   validation loss 3.343689604662359e-05, (8.90903e-06, 0.098245196, 0.027189765, 2.1808891)\n",
      "decoder loss ratio: 0.000049, decoder SINDy loss  ratio: 0.002294\n",
      "Epoch 142\n",
      "   training loss 3.169397314195521e-05, (6.2207973e-06, 0.18704636, 0.03713528, 2.1759648)\n",
      "   validation loss 3.053536784136668e-05, (6.0007515e-06, 0.10285322, 0.027749676, 2.1759648)\n",
      "decoder loss ratio: 0.000033, decoder SINDy loss  ratio: 0.002341\n",
      "Epoch 143\n",
      "   training loss 3.3637617889326066e-05, (8.0219515e-06, 0.18642205, 0.038646486, 2.175102)\n",
      "   validation loss 3.257263233535923e-05, (8.128065e-06, 0.099151224, 0.02693548, 2.175102)\n",
      "decoder loss ratio: 0.000045, decoder SINDy loss  ratio: 0.002272\n",
      "Epoch 144\n",
      "   training loss 3.961415131925605e-05, (1.4031394e-05, 0.19020353, 0.03845377, 2.173738)\n",
      "   validation loss 3.852236477541737e-05, (1.406976e-05, 0.09901616, 0.027152251, 2.173738)\n",
      "decoder loss ratio: 0.000077, decoder SINDy loss  ratio: 0.002291\n",
      "Epoch 145\n",
      "   training loss 4.403094499139115e-05, (1.8674673e-05, 0.17951258, 0.03642011, 2.171426)\n",
      "   validation loss 4.3875254050362855e-05, (1.959229e-05, 0.09698267, 0.025687015, 2.171426)\n",
      "decoder loss ratio: 0.000108, decoder SINDy loss  ratio: 0.002167\n",
      "Epoch 146\n",
      "   training loss 3.458821083768271e-05, (9.102323e-06, 0.18028945, 0.03810687, 2.1675203)\n",
      "   validation loss 3.364444637554698e-05, (9.3083245e-06, 0.09593801, 0.02660921, 2.1675203)\n",
      "decoder loss ratio: 0.000051, decoder SINDy loss  ratio: 0.002245\n",
      "Epoch 147\n",
      "   training loss 3.740844113053754e-05, (1.196393e-05, 0.1773016, 0.037982877, 2.1646225)\n",
      "   validation loss 3.6576137063093483e-05, (1.2265207e-05, 0.0949388, 0.026647061, 2.1646225)\n",
      "decoder loss ratio: 0.000068, decoder SINDy loss  ratio: 0.002248\n",
      "Epoch 148\n",
      "   training loss 8.047141454881057e-05, (5.4330118e-05, 0.20118786, 0.045692515, 2.1572046)\n",
      "   validation loss 7.898101466707885e-05, (5.411466e-05, 0.11252495, 0.032943074, 2.1572046)\n",
      "decoder loss ratio: 0.000298, decoder SINDy loss  ratio: 0.002779\n",
      "Epoch 149\n",
      "   training loss 3.13222044496797e-05, (6.0419907e-06, 0.18686457, 0.037191924, 2.1561024)\n",
      "   validation loss 3.04291897919029e-05, (6.282594e-06, 0.09646722, 0.025855722, 2.1561024)\n",
      "decoder loss ratio: 0.000035, decoder SINDy loss  ratio: 0.002181\n",
      "Epoch 150\n",
      "   training loss 3.566026862245053e-05, (1.0438535e-05, 0.18691163, 0.03665411, 2.1556323)\n",
      "   validation loss 3.5033011954510584e-05, (1.0911155e-05, 0.09726878, 0.025655353, 2.1556323)\n",
      "decoder loss ratio: 0.000060, decoder SINDy loss  ratio: 0.002164\n",
      "Epoch 151\n",
      "   training loss 3.724512498592958e-05, (1.2040294e-05, 0.18272161, 0.036691513, 2.1535683)\n",
      "   validation loss 3.632684820331633e-05, (1.2230689e-05, 0.09503175, 0.025604801, 2.1535683)\n",
      "decoder loss ratio: 0.000067, decoder SINDy loss  ratio: 0.002160\n",
      "Epoch 152\n",
      "   training loss 4.1553255869075656e-05, (1.6529999e-05, 0.17941839, 0.035142712, 2.1508987)\n",
      "   validation loss 4.0982569771585986e-05, (1.6999586e-05, 0.09501166, 0.024739992, 2.1508987)\n",
      "decoder loss ratio: 0.000094, decoder SINDy loss  ratio: 0.002087\n",
      "Epoch 153\n",
      "   training loss 3.76319803763181e-05, (1.2728951e-05, 0.17396104, 0.034245517, 2.147848)\n",
      "   validation loss 3.707676660269499e-05, (1.3181324e-05, 0.09259451, 0.024169656, 2.147848)\n",
      "decoder loss ratio: 0.000073, decoder SINDy loss  ratio: 0.002039\n",
      "Epoch 154\n",
      "   training loss 3.3026830351445824e-05, (8.265145e-06, 0.17786855, 0.03333268, 2.1428418)\n",
      "   validation loss 3.216708864783868e-05, (8.355778e-06, 0.09246172, 0.023828948, 2.1428418)\n",
      "decoder loss ratio: 0.000046, decoder SINDy loss  ratio: 0.002010\n",
      "Epoch 155\n",
      "   training loss 3.254726107115857e-05, (7.7096665e-06, 0.17346418, 0.034263752, 2.1411219)\n",
      "   validation loss 3.1689240131527185e-05, (7.8673065e-06, 0.09165264, 0.024107158, 2.1411219)\n",
      "decoder loss ratio: 0.000043, decoder SINDy loss  ratio: 0.002034\n",
      "Epoch 156\n",
      "   training loss 3.613741137087345e-05, (1.0894923e-05, 0.17146099, 0.03869386, 2.1373103)\n",
      "   validation loss 3.562120400601998e-05, (1.1557606e-05, 0.09219327, 0.026904987, 2.1373103)\n",
      "decoder loss ratio: 0.000064, decoder SINDy loss  ratio: 0.002270\n",
      "Epoch 157\n",
      "   training loss 3.510087844915688e-05, (1.0190693e-05, 0.16686642, 0.03565929, 2.1344256)\n",
      "   validation loss 3.440667569520883e-05, (1.0567205e-05, 0.09101187, 0.02495213, 2.1344256)\n",
      "decoder loss ratio: 0.000058, decoder SINDy loss  ratio: 0.002105\n",
      "Epoch 158\n",
      "   training loss 3.281288081780076e-05, (8.159823e-06, 0.16899182, 0.033321217, 2.1320937)\n",
      "   validation loss 3.198884951416403e-05, (8.307043e-06, 0.090666816, 0.023608694, 2.1320937)\n",
      "decoder loss ratio: 0.000046, decoder SINDy loss  ratio: 0.001992\n",
      "Epoch 159\n",
      "   training loss 4.042666841996834e-05, (1.599167e-05, 0.16657797, 0.031321984, 2.1302798)\n",
      "   validation loss 3.917553840437904e-05, (1.5609927e-05, 0.09146063, 0.022628136, 2.1302798)\n",
      "decoder loss ratio: 0.000086, decoder SINDy loss  ratio: 0.001909\n",
      "Epoch 160\n",
      "   training loss 3.552822090568952e-05, (1.0660695e-05, 0.17167342, 0.03626474, 2.1241052)\n",
      "   validation loss 3.4685599530348554e-05, (1.0922845e-05, 0.09360512, 0.025217028, 2.1241052)\n",
      "decoder loss ratio: 0.000060, decoder SINDy loss  ratio: 0.002127\n",
      "Epoch 161\n",
      "   training loss 3.4325737942708656e-05, (9.678097e-06, 0.16814764, 0.034271788, 2.1220462)\n",
      "   validation loss 3.347267920617014e-05, (9.816671e-06, 0.09172048, 0.024355488, 2.1220462)\n",
      "decoder loss ratio: 0.000054, decoder SINDy loss  ratio: 0.002055\n",
      "Epoch 162\n",
      "   training loss 3.6589270166587085e-05, (1.196791e-05, 0.17039627, 0.034505054, 2.1170855)\n",
      "   validation loss 3.622503936639987e-05, (1.2582722e-05, 0.092651315, 0.024714645, 2.1170855)\n",
      "decoder loss ratio: 0.000069, decoder SINDy loss  ratio: 0.002085\n",
      "Epoch 163\n",
      "   training loss 3.524783096509054e-05, (1.0544884e-05, 0.17274381, 0.035481703, 2.1154776)\n",
      "   validation loss 3.436191036598757e-05, (1.074095e-05, 0.09244509, 0.02466183, 2.1154776)\n",
      "decoder loss ratio: 0.000059, decoder SINDy loss  ratio: 0.002080\n",
      "Epoch 164\n",
      "   training loss 3.408038173802197e-05, (9.742174e-06, 0.16968249, 0.032020964, 2.113611)\n",
      "   validation loss 3.320497489767149e-05, (9.789216e-06, 0.090860106, 0.022796474, 2.113611)\n",
      "decoder loss ratio: 0.000054, decoder SINDy loss  ratio: 0.001923\n",
      "Epoch 165\n",
      "   training loss 3.611766442190856e-05, (1.1402779e-05, 0.1682486, 0.03617861, 2.1097026)\n",
      "   validation loss 3.6043777072336525e-05, (1.2404015e-05, 0.08961736, 0.025427356, 2.1097026)\n",
      "decoder loss ratio: 0.000068, decoder SINDy loss  ratio: 0.002145\n",
      "Epoch 166\n",
      "   training loss 3.3490559872007e-05, (9.043297e-06, 0.16320963, 0.033788335, 2.106843)\n",
      "   validation loss 3.2727912184782326e-05, (9.280707e-06, 0.08892201, 0.02378775, 2.106843)\n",
      "decoder loss ratio: 0.000051, decoder SINDy loss  ratio: 0.002007\n",
      "Epoch 167\n",
      "   training loss 3.3122811146313325e-05, (8.789464e-06, 0.16306888, 0.03296714, 2.1036634)\n",
      "   validation loss 3.2183772418648005e-05, (8.810308e-06, 0.09020968, 0.023368308, 2.1036634)\n",
      "decoder loss ratio: 0.000049, decoder SINDy loss  ratio: 0.001971\n",
      "Epoch 168\n",
      "   training loss 3.227766865165904e-05, (8.091124e-06, 0.16270365, 0.031787872, 2.100776)\n",
      "   validation loss 3.1287250749301165e-05, (8.0098325e-06, 0.0893143, 0.022696594, 2.100776)\n",
      "decoder loss ratio: 0.000044, decoder SINDy loss  ratio: 0.001915\n",
      "Epoch 169\n",
      "   training loss 3.301900869701058e-05, (9.0193325e-06, 0.16036306, 0.030155325, 2.0984142)\n",
      "   validation loss 3.2417741749668494e-05, (9.251935e-06, 0.089692995, 0.021816658, 2.0984142)\n",
      "decoder loss ratio: 0.000051, decoder SINDy loss  ratio: 0.001840\n",
      "Epoch 170\n",
      "   training loss 3.843296872219071e-05, (1.4376246e-05, 0.17330785, 0.031370357, 2.091969)\n",
      "   validation loss 3.7863625038880855e-05, (1.461947e-05, 0.09231968, 0.023244638, 2.091969)\n",
      "decoder loss ratio: 0.000080, decoder SINDy loss  ratio: 0.001961\n",
      "Epoch 171\n",
      "   training loss 3.0242870707297698e-05, (6.2862546e-06, 0.16655244, 0.030504331, 2.0906184)\n",
      "   validation loss 2.9581351554952562e-05, (6.5108e-06, 0.08759997, 0.021643676, 2.0906184)\n",
      "decoder loss ratio: 0.000036, decoder SINDy loss  ratio: 0.001826\n",
      "Epoch 172\n",
      "   training loss 3.276696224929765e-05, (8.633668e-06, 0.16275875, 0.03245332, 2.0887964)\n",
      "   validation loss 3.198107879143208e-05, (8.781017e-06, 0.088163264, 0.023120977, 2.0887964)\n",
      "decoder loss ratio: 0.000048, decoder SINDy loss  ratio: 0.001950\n",
      "Epoch 173\n",
      "   training loss 2.9581173293991014e-05, (5.692961e-06, 0.15986605, 0.030312488, 2.0856965)\n",
      "   validation loss 2.8712925995932892e-05, (5.6639064e-06, 0.086572275, 0.021920566, 2.0856965)\n",
      "decoder loss ratio: 0.000031, decoder SINDy loss  ratio: 0.001849\n",
      "Epoch 174\n",
      "   training loss 3.124286013189703e-05, (7.320531e-06, 0.15866572, 0.030974807, 2.082485)\n",
      "   validation loss 3.0292581868707202e-05, (7.2537973e-06, 0.08682144, 0.022139352, 2.082485)\n",
      "decoder loss ratio: 0.000040, decoder SINDy loss  ratio: 0.001868\n",
      "Epoch 175\n",
      "   training loss 3.2007093977881595e-05, (8.084429e-06, 0.15719832, 0.03134382, 2.0788283)\n",
      "   validation loss 3.1069244869286194e-05, (8.028209e-06, 0.08644588, 0.022527548, 2.0788283)\n",
      "decoder loss ratio: 0.000044, decoder SINDy loss  ratio: 0.001900\n",
      "Epoch 176\n",
      "   training loss 3.88197586289607e-05, (1.4801755e-05, 0.16501285, 0.032658428, 2.075216)\n",
      "   validation loss 3.8980968383839354e-05, (1.5849037e-05, 0.08869064, 0.02379771, 2.075216)\n",
      "decoder loss ratio: 0.000087, decoder SINDy loss  ratio: 0.002008\n",
      "Epoch 177\n",
      "   training loss 3.158883555443026e-05, (7.816266e-06, 0.16146505, 0.030450841, 2.0727487)\n",
      "   validation loss 3.060165181523189e-05, (7.691574e-06, 0.08720484, 0.02182594, 2.0727487)\n",
      "decoder loss ratio: 0.000042, decoder SINDy loss  ratio: 0.001841\n",
      "Epoch 178\n",
      "   training loss 3.0184095521690324e-05, (6.5046465e-06, 0.15955491, 0.029800162, 2.0699432)\n",
      "   validation loss 2.9179000193835236e-05, (6.317086e-06, 0.08599368, 0.02162482, 2.0699432)\n",
      "decoder loss ratio: 0.000035, decoder SINDy loss  ratio: 0.001824\n",
      "Epoch 179\n",
      "   training loss 5.3693125664722174e-05, (3.0258849e-05, 0.16982608, 0.027582543, 2.0676024)\n",
      "   validation loss 5.412496830103919e-05, (3.1401585e-05, 0.093737334, 0.020473614, 2.0676024)\n",
      "decoder loss ratio: 0.000173, decoder SINDy loss  ratio: 0.001727\n",
      "Epoch 180\n",
      "   training loss 3.1964413210516796e-05, (8.292876e-06, 0.16672838, 0.030456519, 2.0625885)\n",
      "   validation loss 3.123216447420418e-05, (8.440537e-06, 0.08670347, 0.021657417, 2.0625885)\n",
      "decoder loss ratio: 0.000046, decoder SINDy loss  ratio: 0.001827\n",
      "Epoch 181\n",
      "   training loss 2.9002201699768193e-05, (5.4719567e-06, 0.16236746, 0.029279642, 2.060228)\n",
      "   validation loss 2.8237231163075194e-05, (5.543093e-06, 0.08502577, 0.020918572, 2.060228)\n",
      "decoder loss ratio: 0.000031, decoder SINDy loss  ratio: 0.001765\n",
      "Epoch 182\n",
      "   training loss 3.132483107037842e-05, (7.636526e-06, 0.16035543, 0.031240735, 2.0564234)\n",
      "   validation loss 3.0561492167180404e-05, (7.733157e-06, 0.08607938, 0.022641005, 2.0564234)\n",
      "decoder loss ratio: 0.000043, decoder SINDy loss  ratio: 0.001910\n",
      "Epoch 183\n",
      "   training loss 3.1325591407949105e-05, (7.7968525e-06, 0.16023372, 0.02994482, 2.0534258)\n",
      "   validation loss 3.0408946258830838e-05, (7.726048e-06, 0.08639038, 0.021486413, 2.0534258)\n",
      "decoder loss ratio: 0.000043, decoder SINDy loss  ratio: 0.001813\n",
      "Epoch 184\n",
      "   training loss 3.252155875088647e-05, (9.035501e-06, 0.15969436, 0.029866647, 2.0499396)\n",
      "   validation loss 3.154633304802701e-05, (8.858771e-06, 0.086545184, 0.021881646, 2.0499396)\n",
      "decoder loss ratio: 0.000049, decoder SINDy loss  ratio: 0.001846\n",
      "Epoch 185\n",
      "   training loss 3.366504824953154e-05, (1.0135802e-05, 0.15906632, 0.030590106, 2.0470235)\n",
      "   validation loss 3.2617797842249274e-05, (9.923985e-06, 0.086791955, 0.022235766, 2.0470235)\n",
      "decoder loss ratio: 0.000055, decoder SINDy loss  ratio: 0.001876\n",
      "Epoch 186\n",
      "   training loss 3.354643195052631e-05, (1.0164149e-05, 0.15822522, 0.029427715, 2.0439513)\n",
      "   validation loss 3.2674652175046504e-05, (1.00801435e-05, 0.08664557, 0.021549955, 2.0439513)\n",
      "decoder loss ratio: 0.000055, decoder SINDy loss  ratio: 0.001818\n",
      "Epoch 187\n",
      "   training loss 4.3605861719697714e-05, (2.0522599e-05, 0.15667419, 0.026650291, 2.0418234)\n",
      "   validation loss 4.2690313421189785e-05, (2.0276715e-05, 0.084409945, 0.019953655, 2.0418234)\n",
      "decoder loss ratio: 0.000112, decoder SINDy loss  ratio: 0.001683\n",
      "Epoch 188\n",
      "   training loss 2.9830105631845072e-05, (6.7647934e-06, 0.16320507, 0.027124625, 2.035285)\n",
      "   validation loss 2.9223578167147934e-05, (6.879848e-06, 0.08571371, 0.0199088, 2.035285)\n",
      "decoder loss ratio: 0.000038, decoder SINDy loss  ratio: 0.001680\n",
      "Epoch 189\n",
      "   training loss 3.18360143864993e-05, (8.460879e-06, 0.16262521, 0.030403903, 2.0334744)\n",
      "   validation loss 3.099971218034625e-05, (8.467428e-06, 0.08618002, 0.02197538, 2.0334744)\n",
      "decoder loss ratio: 0.000047, decoder SINDy loss  ratio: 0.001854\n",
      "Epoch 190\n",
      "   training loss 3.0006529414094985e-05, (6.956593e-06, 0.16042034, 0.027414717, 2.0308466)\n",
      "   validation loss 2.907846646849066e-05, (6.739531e-06, 0.08496844, 0.02030469, 2.0308466)\n",
      "decoder loss ratio: 0.000037, decoder SINDy loss  ratio: 0.001713\n",
      "Epoch 191\n",
      "   training loss 3.31002775055822e-05, (1.0089763e-05, 0.1635761, 0.027340228, 2.0276492)\n",
      "   validation loss 3.185603054589592e-05, (9.5270025e-06, 0.08569899, 0.02052538, 2.0276492)\n",
      "decoder loss ratio: 0.000052, decoder SINDy loss  ratio: 0.001732\n",
      "Epoch 192\n",
      "   training loss 2.824402690748684e-05, (5.3307576e-06, 0.1516032, 0.026727498, 2.024052)\n",
      "   validation loss 2.7462720026960596e-05, (5.214493e-06, 0.082776815, 0.020077085, 2.024052)\n",
      "decoder loss ratio: 0.000029, decoder SINDy loss  ratio: 0.001694\n",
      "Epoch 193\n",
      "   training loss 3.436849874560721e-05, (1.1479275e-05, 0.15334436, 0.026766347, 2.021259)\n",
      "   validation loss 3.348641621414572e-05, (1.1278788e-05, 0.08361576, 0.01995038, 2.021259)\n",
      "decoder loss ratio: 0.000062, decoder SINDy loss  ratio: 0.001683\n",
      "Epoch 194\n",
      "   training loss 3.1249648600351065e-05, (8.23286e-06, 0.1627522, 0.028450955, 2.0171695)\n",
      "   validation loss 3.031326377822552e-05, (8.048211e-06, 0.08724738, 0.02093359, 2.0171695)\n",
      "decoder loss ratio: 0.000044, decoder SINDy loss  ratio: 0.001766\n",
      "Epoch 195\n",
      "   training loss 3.67214743164368e-05, (1.3735339e-05, 0.16194472, 0.028432297, 2.0142906)\n",
      "   validation loss 3.5960430977866054e-05, (1.3682847e-05, 0.08729483, 0.021346804, 2.0142906)\n",
      "decoder loss ratio: 0.000075, decoder SINDy loss  ratio: 0.001801\n",
      "Epoch 196\n",
      "   training loss 3.609368286561221e-05, (1.3334226e-05, 0.15270413, 0.026504682, 2.0108988)\n",
      "   validation loss 3.5159253457095474e-05, (1.305304e-05, 0.083401754, 0.019972237, 2.0108988)\n",
      "decoder loss ratio: 0.000072, decoder SINDy loss  ratio: 0.001685\n",
      "Epoch 197\n",
      "   training loss 3.395215389900841e-05, (1.0676238e-05, 0.16307722, 0.032198586, 2.0056057)\n",
      "   validation loss 3.338403257657774e-05, (1.10329465e-05, 0.085173205, 0.022950297, 2.0056057)\n",
      "decoder loss ratio: 0.000061, decoder SINDy loss  ratio: 0.001936\n",
      "Epoch 198\n",
      "   training loss 2.956123171315994e-05, (6.7795486e-06, 0.15962507, 0.027513668, 2.0030317)\n",
      "   validation loss 2.863204099412542e-05, (6.581198e-06, 0.08381186, 0.020205265, 2.0030317)\n",
      "decoder loss ratio: 0.000036, decoder SINDy loss  ratio: 0.001705\n",
      "Epoch 199\n",
      "   training loss 3.183247827109881e-05, (9.129435e-06, 0.15766852, 0.02700275, 2.0002768)\n",
      "   validation loss 3.093936902587302e-05, (8.913364e-06, 0.08423784, 0.020232365, 2.0002768)\n",
      "decoder loss ratio: 0.000049, decoder SINDy loss  ratio: 0.001707\n",
      "Epoch 200\n",
      "   training loss 3.055591878364794e-05, (7.884833e-06, 0.15500548, 0.02711243, 1.9959843)\n",
      "   validation loss 2.9410670322249644e-05, (7.4210348e-06, 0.08261106, 0.02029792, 1.9959843)\n",
      "decoder loss ratio: 0.000041, decoder SINDy loss  ratio: 0.001712\n",
      "Epoch 201\n",
      "   training loss 3.08620510622859e-05, (8.172945e-06, 0.15438555, 0.027609145, 1.9928193)\n",
      "   validation loss 2.9886228730902076e-05, (7.9138645e-06, 0.08265138, 0.02044171, 1.9928193)\n",
      "decoder loss ratio: 0.000044, decoder SINDy loss  ratio: 0.001724\n",
      "Epoch 202\n",
      "   training loss 2.9166949389036745e-05, (6.5923464e-06, 0.15370128, 0.02676628, 1.9897975)\n",
      "   validation loss 2.8107100661145523e-05, (6.2207164e-06, 0.08247516, 0.019884095, 1.9897975)\n",
      "decoder loss ratio: 0.000034, decoder SINDy loss  ratio: 0.001677\n",
      "Epoch 203\n",
      "   training loss 2.9949576855869964e-05, (7.684064e-06, 0.15623073, 0.02412172, 1.9853342)\n",
      "   validation loss 2.9689050279557705e-05, (7.971651e-06, 0.08227643, 0.018640572, 1.9853342)\n",
      "decoder loss ratio: 0.000044, decoder SINDy loss  ratio: 0.001573\n",
      "Epoch 204\n",
      "   training loss 2.92434651782969e-05, (6.717599e-06, 0.15884574, 0.026990037, 1.9826864)\n",
      "   validation loss 2.8175949410069734e-05, (6.3747084e-06, 0.083047315, 0.019743796, 1.9826864)\n",
      "decoder loss ratio: 0.000035, decoder SINDy loss  ratio: 0.001666\n",
      "Epoch 205\n",
      "   training loss 3.324385033920407e-05, (1.0838699e-05, 0.1608947, 0.026124362, 1.9792713)\n",
      "   validation loss 3.2287134672515094e-05, (1.0544279e-05, 0.08277226, 0.019501409, 1.9792713)\n",
      "decoder loss ratio: 0.000058, decoder SINDy loss  ratio: 0.001645\n",
      "Epoch 206\n",
      "   training loss 2.9224645913927816e-05, (6.8226323e-06, 0.15765797, 0.026456676, 1.9756347)\n",
      "   validation loss 2.8288623070693575e-05, (6.5706486e-06, 0.081617795, 0.019616283, 1.9756347)\n",
      "decoder loss ratio: 0.000036, decoder SINDy loss  ratio: 0.001655\n",
      "Epoch 207\n",
      "   training loss 3.056946661672555e-05, (8.552135e-06, 0.15781623, 0.023091877, 1.9708145)\n",
      "   validation loss 3.0816088838037103e-05, (9.266225e-06, 0.08182007, 0.018417217, 1.9708145)\n",
      "decoder loss ratio: 0.000051, decoder SINDy loss  ratio: 0.001554\n",
      "Epoch 208\n",
      "   training loss 2.997575211338699e-05, (7.6634315e-06, 0.1603119, 0.026316354, 1.9680686)\n",
      "   validation loss 2.91136275336612e-05, (7.4912055e-06, 0.083088055, 0.019417377, 1.9680686)\n",
      "decoder loss ratio: 0.000041, decoder SINDy loss  ratio: 0.001638\n",
      "Epoch 209\n",
      "   training loss 3.1851956009631976e-05, (9.52927e-06, 0.15919353, 0.026746044, 1.9648081)\n",
      "   validation loss 3.093847772106528e-05, (9.2985765e-06, 0.08315543, 0.019918187, 1.9648081)\n",
      "decoder loss ratio: 0.000051, decoder SINDy loss  ratio: 0.001680\n",
      "Epoch 210\n",
      "   training loss 3.0002800485817716e-05, (7.77678e-06, 0.15381743, 0.026149966, 1.9611024)\n",
      "   validation loss 2.8960081181139685e-05, (7.3911706e-06, 0.08126358, 0.01957888, 1.9611024)\n",
      "decoder loss ratio: 0.000041, decoder SINDy loss  ratio: 0.001652\n",
      "Epoch 211\n",
      "   training loss 3.480928353383206e-05, (1.2721775e-05, 0.15855777, 0.025103502, 1.9577157)\n",
      "   validation loss 3.416206163819879e-05, (1.2696294e-05, 0.08265809, 0.018886112, 1.9577157)\n",
      "decoder loss ratio: 0.000070, decoder SINDy loss  ratio: 0.001593\n",
      "Epoch 212\n",
      "   training loss 2.8744629162247293e-05, (6.7400615e-06, 0.15668724, 0.024570696, 1.95475)\n",
      "   validation loss 2.7820236937259324e-05, (6.436389e-06, 0.08115563, 0.018363493, 1.95475)\n",
      "decoder loss ratio: 0.000035, decoder SINDy loss  ratio: 0.001549\n",
      "Epoch 213\n",
      "   training loss 3.3141484891530126e-05, (1.0948731e-05, 0.16123295, 0.026867768, 1.9505978)\n",
      "   validation loss 3.219258724129759e-05, (1.0683275e-05, 0.082571775, 0.020033369, 1.9505978)\n",
      "decoder loss ratio: 0.000059, decoder SINDy loss  ratio: 0.001690\n",
      "Epoch 214\n",
      "   training loss 2.8431331884348765e-05, (6.4380115e-06, 0.15515225, 0.02523717, 1.9469604)\n",
      "   validation loss 2.735730777203571e-05, (6.0039633e-06, 0.080413, 0.01883741, 1.9469604)\n",
      "decoder loss ratio: 0.000033, decoder SINDy loss  ratio: 0.001589\n",
      "Epoch 215\n",
      "   training loss 3.3725707908160985e-05, (1.2090436e-05, 0.1599272, 0.02216233, 1.941904)\n",
      "   validation loss 3.331575499032624e-05, (1.2179659e-05, 0.08271391, 0.017170565, 1.941904)\n",
      "decoder loss ratio: 0.000067, decoder SINDy loss  ratio: 0.001449\n",
      "Epoch 216\n",
      "   training loss 3.252633541706018e-05, (1.03374705e-05, 0.16452602, 0.027975786, 1.9391288)\n",
      "   validation loss 3.1687151931691915e-05, (1.024057e-05, 0.08417409, 0.02055296, 1.9391288)\n",
      "decoder loss ratio: 0.000056, decoder SINDy loss  ratio: 0.001734\n",
      "Epoch 217\n",
      "   training loss 2.9076069040456787e-05, (7.2755865e-06, 0.15878834, 0.024384841, 1.9361998)\n",
      "   validation loss 2.7846870580106042e-05, (6.6557564e-06, 0.08112924, 0.018291164, 1.9361998)\n",
      "decoder loss ratio: 0.000037, decoder SINDy loss  ratio: 0.001543\n",
      "Epoch 218\n",
      "   training loss 3.4255703212693334e-05, (1.27353305e-05, 0.16734712, 0.022079898, 1.9312385)\n",
      "   validation loss 3.383097282494418e-05, (1.2792006e-05, 0.08472409, 0.017265817, 1.9312385)\n",
      "decoder loss ratio: 0.000070, decoder SINDy loss  ratio: 0.001457\n",
      "Epoch 219\n",
      "   training loss 3.434324753470719e-05, (1.2304865e-05, 0.16864686, 0.027619911, 1.9276394)\n",
      "   validation loss 3.347189704072662e-05, (1.2157726e-05, 0.08508195, 0.020377789, 1.9276394)\n",
      "decoder loss ratio: 0.000067, decoder SINDy loss  ratio: 0.001719\n",
      "Epoch 220\n",
      "   training loss 2.976621726702433e-05, (8.062792e-06, 0.16264753, 0.024531469, 1.9250277)\n",
      "   validation loss 2.8994907552259974e-05, (7.888372e-06, 0.08229966, 0.018562583, 1.9250277)\n",
      "decoder loss ratio: 0.000043, decoder SINDy loss  ratio: 0.001566\n",
      "Epoch 221\n",
      "   training loss 3.2353833375964314e-05, (1.0639193e-05, 0.16117246, 0.025030412, 1.92116)\n",
      "   validation loss 3.145180380670354e-05, (1.0347675e-05, 0.081913084, 0.018925326, 1.92116)\n",
      "decoder loss ratio: 0.000057, decoder SINDy loss  ratio: 0.001597\n",
      "Epoch 222\n",
      "   training loss 3.09452079818584e-05, (9.271949e-06, 0.15649807, 0.02502321, 1.917094)\n",
      "   validation loss 2.9465285479091108e-05, (8.419474e-06, 0.08004275, 0.018748721, 1.917094)\n",
      "decoder loss ratio: 0.000046, decoder SINDy loss  ratio: 0.001582\n",
      "Epoch 223\n",
      "   training loss 3.800122067332268e-05, (1.6355747e-05, 0.15759431, 0.025112957, 1.9134182)\n",
      "   validation loss 3.675603511510417e-05, (1.5709315e-05, 0.08233748, 0.019125376, 1.9134182)\n",
      "decoder loss ratio: 0.000086, decoder SINDy loss  ratio: 0.001613\n",
      "Epoch 224\n",
      "   training loss 2.9727569199167192e-05, (8.177802e-06, 0.15757816, 0.024434615, 1.9106307)\n",
      "   validation loss 2.837852662196383e-05, (7.442006e-06, 0.08010778, 0.018302154, 1.9106307)\n",
      "decoder loss ratio: 0.000041, decoder SINDy loss  ratio: 0.001544\n",
      "Epoch 225\n",
      "   training loss 3.06110450765118e-05, (9.462172e-06, 0.16414003, 0.02087203, 1.9061671)\n",
      "   validation loss 3.0247378163039684e-05, (9.537305e-06, 0.08226584, 0.016484024, 1.9061671)\n",
      "decoder loss ratio: 0.000053, decoder SINDy loss  ratio: 0.001391\n",
      "Epoch 226\n",
      "   training loss 3.040543379029259e-05, (8.861582e-06, 0.1658551, 0.025144972, 1.9029356)\n",
      "   validation loss 2.93881275865715e-05, (8.493936e-06, 0.082395956, 0.018648356, 1.9029356)\n",
      "decoder loss ratio: 0.000047, decoder SINDy loss  ratio: 0.001573\n",
      "Epoch 227\n",
      "   training loss 3.0503448215313256e-05, (9.0537915e-06, 0.16147323, 0.024540968, 1.899556)\n",
      "   validation loss 2.908959868364036e-05, (8.262598e-06, 0.08112686, 0.018314404, 1.899556)\n",
      "decoder loss ratio: 0.000045, decoder SINDy loss  ratio: 0.001545\n",
      "Epoch 228\n",
      "   training loss 2.843326365109533e-05, (7.42241e-06, 0.16588326, 0.020622727, 1.8948581)\n",
      "   validation loss 2.7806334401248023e-05, (7.24063e-06, 0.082901314, 0.016171243, 1.8948581)\n",
      "decoder loss ratio: 0.000040, decoder SINDy loss  ratio: 0.001364\n",
      "Epoch 229\n",
      "   training loss 3.057719368371181e-05, (9.086145e-06, 0.16548546, 0.02579234, 1.8911816)\n",
      "   validation loss 2.992581357830204e-05, (9.108453e-06, 0.08225749, 0.019055473, 1.8911816)\n",
      "decoder loss ratio: 0.000050, decoder SINDy loss  ratio: 0.001608\n",
      "Epoch 230\n",
      "   training loss 3.0413237254833803e-05, (9.197286e-06, 0.16577102, 0.02335912, 1.8880041)\n",
      "   validation loss 2.9232443921500817e-05, (8.601668e-06, 0.081588075, 0.017507346, 1.8880041)\n",
      "decoder loss ratio: 0.000047, decoder SINDy loss  ratio: 0.001477\n",
      "Epoch 231\n",
      "   training loss 3.165360976709053e-05, (1.03368275e-05, 0.163063, 0.024669096, 1.8849874)\n",
      "   validation loss 2.9987524612806737e-05, (9.298093e-06, 0.080575876, 0.018395586, 1.8849874)\n",
      "decoder loss ratio: 0.000051, decoder SINDy loss  ratio: 0.001552\n",
      "Epoch 232\n",
      "   training loss 3.7906942452536896e-05, (1.6768316e-05, 0.16266991, 0.023157127, 1.8822914)\n",
      "   validation loss 3.6956873373128474e-05, (1.637833e-05, 0.08220343, 0.017556312, 1.8822914)\n",
      "decoder loss ratio: 0.000090, decoder SINDy loss  ratio: 0.001481\n",
      "Epoch 233\n",
      "   training loss 2.9837063266313635e-05, (8.719349e-06, 0.16356681, 0.023145437, 1.880317)\n",
      "   validation loss 2.8675218345597386e-05, (8.138239e-06, 0.080373615, 0.017338103, 1.880317)\n",
      "decoder loss ratio: 0.000045, decoder SINDy loss  ratio: 0.001463\n",
      "Epoch 234\n",
      "   training loss 3.187423862982541e-05, (1.1076225e-05, 0.16634367, 0.020313315, 1.8766686)\n",
      "   validation loss 3.183639273629524e-05, (1.1473021e-05, 0.0817646, 0.015966874, 1.8766686)\n",
      "decoder loss ratio: 0.000063, decoder SINDy loss  ratio: 0.001347\n",
      "Epoch 235\n",
      "   training loss 3.1023286283016205e-05, (9.701168e-06, 0.16843317, 0.025790386, 1.8743082)\n",
      "   validation loss 3.0283204978331923e-05, (9.630875e-06, 0.082604565, 0.01909249, 1.8743082)\n",
      "decoder loss ratio: 0.000053, decoder SINDy loss  ratio: 0.001611\n",
      "Epoch 236\n",
      "   training loss 2.954441515612416e-05, (8.548868e-06, 0.1655591, 0.022731297, 1.8722419)\n",
      "   validation loss 2.8337512048892677e-05, (7.911616e-06, 0.08106562, 0.017034784, 1.8722419)\n",
      "decoder loss ratio: 0.000044, decoder SINDy loss  ratio: 0.001437\n",
      "Epoch 237\n",
      "   training loss 3.456966805970296e-05, (1.3474127e-05, 0.16775918, 0.023972195, 1.8698324)\n",
      "   validation loss 3.362066127010621e-05, (1.3107532e-05, 0.08238869, 0.01814806, 1.8698324)\n",
      "decoder loss ratio: 0.000072, decoder SINDy loss  ratio: 0.001531\n",
      "Epoch 238\n",
      "   training loss 2.7696740289684385e-05, (6.5895033e-06, 0.16295907, 0.02451439, 1.8655797)\n",
      "   validation loss 2.677414522622712e-05, (6.3007565e-06, 0.083776996, 0.018175915, 1.8655797)\n",
      "decoder loss ratio: 0.000035, decoder SINDy loss  ratio: 0.001533\n",
      "Epoch 239\n",
      "   training loss 2.96696562145371e-05, (8.7787375e-06, 0.16636904, 0.022494243, 1.8641496)\n",
      "   validation loss 2.830051744240336e-05, (7.981186e-06, 0.08145218, 0.016778376, 1.8641496)\n",
      "decoder loss ratio: 0.000044, decoder SINDy loss  ratio: 0.001415\n",
      "Epoch 240\n",
      "   training loss 3.2012147130444646e-05, (1.1059805e-05, 0.16380958, 0.023395026, 1.8612843)\n",
      "   validation loss 2.9755396099062636e-05, (9.396835e-06, 0.07989759, 0.017457206, 1.8612843)\n",
      "decoder loss ratio: 0.000052, decoder SINDy loss  ratio: 0.001473\n",
      "Epoch 241\n",
      "   training loss 3.122487396467477e-05, (1.0336011e-05, 0.16346015, 0.023018818, 1.8586982)\n",
      "   validation loss 2.898169623222202e-05, (8.679756e-06, 0.07865069, 0.017149583, 1.8586982)\n",
      "decoder loss ratio: 0.000048, decoder SINDy loss  ratio: 0.001447\n",
      "Epoch 242\n",
      "   training loss 3.0119403163553216e-05, (9.603938e-06, 0.16686879, 0.01965462, 1.8550004)\n",
      "   validation loss 2.97427068289835e-05, (9.643764e-06, 0.08063344, 0.015489399, 1.8550004)\n",
      "decoder loss ratio: 0.000053, decoder SINDy loss  ratio: 0.001307\n",
      "Epoch 243\n",
      "   training loss 3.095878491876647e-05, (1.0149141e-05, 0.1695763, 0.022800203, 1.8529623)\n",
      "   validation loss 3.0029794288566336e-05, (9.7936045e-06, 0.08279127, 0.017065678, 1.8529623)\n",
      "decoder loss ratio: 0.000054, decoder SINDy loss  ratio: 0.001440\n",
      "Epoch 244\n",
      "   training loss 2.9677821657969616e-05, (8.982499e-06, 0.1623516, 0.021928843, 1.8502439)\n",
      "   validation loss 2.803769166348502e-05, (7.908906e-06, 0.08000771, 0.01626347, 1.8502439)\n",
      "decoder loss ratio: 0.000044, decoder SINDy loss  ratio: 0.001372\n",
      "Epoch 245\n",
      "   training loss 3.479325823718682e-05, (1.4134837e-05, 0.16457312, 0.021758314, 1.8482587)\n",
      "   validation loss 3.235519761801697e-05, (1.2234952e-05, 0.07932932, 0.016376587, 1.8482587)\n",
      "decoder loss ratio: 0.000067, decoder SINDy loss  ratio: 0.001382\n",
      "Epoch 246\n",
      "   training loss 3.1248047889675945e-05, (1.0568687e-05, 0.1683495, 0.022349333, 1.844443)\n",
      "   validation loss 2.9714632546529174e-05, (9.616214e-06, 0.08130155, 0.016539894, 1.844443)\n",
      "decoder loss ratio: 0.000053, decoder SINDy loss  ratio: 0.001395\n",
      "Epoch 247\n",
      "   training loss 3.764811117434874e-05, (1.7263672e-05, 0.16813779, 0.01959815, 1.8424627)\n",
      "   validation loss 3.674615436466411e-05, (1.6834336e-05, 0.08211613, 0.014871913, 1.8424627)\n",
      "decoder loss ratio: 0.000093, decoder SINDy loss  ratio: 0.001255\n",
      "Epoch 248\n",
      "   training loss 2.6372035790700465e-05, (5.8795495e-06, 0.17096795, 0.021172725, 1.8375214)\n",
      "   validation loss 2.5938570615835488e-05, (5.945078e-06, 0.082000636, 0.016182775, 1.8375214)\n",
      "decoder loss ratio: 0.000033, decoder SINDy loss  ratio: 0.001365\n",
      "Epoch 249\n",
      "   training loss 2.8732425562338904e-05, (8.246718e-06, 0.17049687, 0.021304473, 1.8355261)\n",
      "   validation loss 2.7714069801731966e-05, (7.748923e-06, 0.08309828, 0.016098866, 1.8355261)\n",
      "decoder loss ratio: 0.000043, decoder SINDy loss  ratio: 0.001358\n",
      "Epoch 250\n",
      "   training loss 2.9858794732717797e-05, (9.520438e-06, 0.16853236, 0.02005248, 1.8333108)\n",
      "   validation loss 2.8793363526347093e-05, (8.938136e-06, 0.081336476, 0.015221194, 1.8333108)\n",
      "decoder loss ratio: 0.000049, decoder SINDy loss  ratio: 0.001284\n",
      "Epoch 251\n",
      "   training loss 3.1460014724871144e-05, (1.1200914e-05, 0.16942401, 0.019528069, 1.8306293)\n",
      "   validation loss 2.9880084184696898e-05, (1.0083252e-05, 0.081099086, 0.014905397, 1.8306293)\n",
      "decoder loss ratio: 0.000056, decoder SINDy loss  ratio: 0.001257\n",
      "Epoch 252\n",
      "   training loss 3.3866519515868276e-05, (1.3290001e-05, 0.17063974, 0.023141436, 1.8262373)\n",
      "   validation loss 3.082774128415622e-05, (1.0847113e-05, 0.082007825, 0.017182555, 1.8262373)\n",
      "decoder loss ratio: 0.000060, decoder SINDy loss  ratio: 0.001450\n",
      "Epoch 253\n",
      "   training loss 3.339710383443162e-05, (1.2830151e-05, 0.1687407, 0.023288334, 1.823812)\n",
      "   validation loss 3.0933035304769874e-05, (1.0976566e-05, 0.08056878, 0.017183496, 1.823812)\n",
      "decoder loss ratio: 0.000060, decoder SINDy loss  ratio: 0.001450\n",
      "Epoch 254\n",
      "   training loss 3.2566258596489206e-05, (1.20670775e-05, 0.17144684, 0.022877622, 1.8211418)\n",
      "   validation loss 3.0860668630339205e-05, (1.0956358e-05, 0.08166501, 0.016928917, 1.8211418)\n",
      "decoder loss ratio: 0.000060, decoder SINDy loss  ratio: 0.001428\n",
      "Epoch 255\n",
      "   training loss 3.276806091889739e-05, (1.23339605e-05, 0.17185092, 0.022519562, 1.8182143)\n",
      "   validation loss 3.099085370195098e-05, (1.1136049e-05, 0.0821149, 0.016726626, 1.8182143)\n",
      "decoder loss ratio: 0.000061, decoder SINDy loss  ratio: 0.001411\n",
      "Epoch 256\n",
      "   training loss 3.3685726521071047e-05, (1.3293274e-05, 0.17285532, 0.022406936, 1.8151759)\n",
      "   validation loss 3.167781324009411e-05, (1.1859261e-05, 0.08251136, 0.016667932, 1.8151759)\n",
      "decoder loss ratio: 0.000065, decoder SINDy loss  ratio: 0.001406\n",
      "Epoch 257\n",
      "   training loss 3.042844036826864e-05, (1.0213669e-05, 0.17064212, 0.020935852, 1.8121188)\n",
      "   validation loss 2.884774949052371e-05, (9.1666725e-06, 0.08202789, 0.015598911, 1.8121188)\n",
      "decoder loss ratio: 0.000050, decoder SINDy loss  ratio: 0.001316\n",
      "Epoch 258\n",
      "   training loss 5.271781265037134e-05, (3.265718e-05, 0.17354262, 0.019639991, 1.8096637)\n",
      "   validation loss 5.145254544913769e-05, (3.1869586e-05, 0.084087014, 0.014863227, 1.8096637)\n",
      "decoder loss ratio: 0.000175, decoder SINDy loss  ratio: 0.001254\n",
      "Epoch 259\n",
      "   training loss 2.7123689505970106e-05, (7.008738e-06, 0.17339946, 0.020644102, 1.8050542)\n",
      "   validation loss 2.6669589715311304e-05, (7.047344e-06, 0.08320378, 0.015717046, 1.8050542)\n",
      "decoder loss ratio: 0.000039, decoder SINDy loss  ratio: 0.001326\n",
      "Epoch 260\n",
      "   training loss 3.0402385164052248e-05, (1.0330927e-05, 0.17279589, 0.02047155, 1.8024304)\n",
      "   validation loss 2.896960540965665e-05, (9.413425e-06, 0.08273835, 0.015318765, 1.8024304)\n",
      "decoder loss ratio: 0.000052, decoder SINDy loss  ratio: 0.001292\n",
      "Epoch 261\n",
      "   training loss 6.276805652305484e-05, (4.293215e-05, 0.17292239, 0.01851451, 1.798445)\n",
      "   validation loss 6.372748612193391e-05, (4.4262953e-05, 0.08587682, 0.014800807, 1.798445)\n",
      "decoder loss ratio: 0.000244, decoder SINDy loss  ratio: 0.001249\n",
      "Epoch 262\n",
      "   training loss 2.5646786525612697e-05, (5.609019e-06, 0.17135485, 0.020853216, 1.7952445)\n",
      "   validation loss 2.4859991754055955e-05, (5.3516146e-06, 0.08251992, 0.015559326, 1.7952445)\n",
      "decoder loss ratio: 0.000029, decoder SINDy loss  ratio: 0.001313\n",
      "Epoch 263\n",
      "   training loss 2.9369984986260533e-05, (9.3759045e-06, 0.16966693, 0.020689212, 1.792516)\n",
      "   validation loss 2.7647518436424434e-05, (8.1929375e-06, 0.08267668, 0.015294221, 1.792516)\n",
      "decoder loss ratio: 0.000045, decoder SINDy loss  ratio: 0.001290\n",
      "Epoch 264\n",
      "   training loss 3.223678868380375e-05, (1.25046045e-05, 0.17981113, 0.018270064, 1.7905178)\n",
      "   validation loss 3.0902447178959846e-05, (1.1576615e-05, 0.08382375, 0.014206573, 1.7905178)\n",
      "decoder loss ratio: 0.000064, decoder SINDy loss  ratio: 0.001198\n",
      "Epoch 265\n",
      "   training loss 3.2384843507315964e-05, (1.2550172e-05, 0.1776967, 0.019619582, 1.7872711)\n",
      "   validation loss 3.108707824139856e-05, (1.1729666e-05, 0.084037304, 0.014847012, 1.7872711)\n",
      "decoder loss ratio: 0.000065, decoder SINDy loss  ratio: 0.001252\n",
      "Epoch 266\n",
      "   training loss 4.117875505471602e-05, (2.1421078e-05, 0.17333353, 0.019153519, 1.7842327)\n",
      "   validation loss 3.981635381933302e-05, (2.0527734e-05, 0.084155045, 0.014462958, 1.7842327)\n",
      "decoder loss ratio: 0.000113, decoder SINDy loss  ratio: 0.001220\n",
      "Epoch 267\n",
      "   training loss 2.3970909751369618e-05, (4.2666534e-06, 0.17492767, 0.019089922, 1.7795265)\n",
      "   validation loss 2.32942147704307e-05, (4.043025e-06, 0.083623484, 0.014559251, 1.7795265)\n",
      "decoder loss ratio: 0.000022, decoder SINDy loss  ratio: 0.001228\n",
      "Epoch 268\n",
      "   training loss 3.1937695894157514e-05, (1.221614e-05, 0.17858523, 0.019454408, 1.7776116)\n",
      "   validation loss 3.0993014661362395e-05, (1.1730553e-05, 0.0840097, 0.014863455, 1.7776116)\n",
      "decoder loss ratio: 0.000065, decoder SINDy loss  ratio: 0.001254\n",
      "Epoch 269\n",
      "   training loss 3.4213622711831704e-05, (1.4510981e-05, 0.1784417, 0.019581795, 1.7744465)\n",
      "   validation loss 3.2996493246173486e-05, (1.3748513e-05, 0.08506492, 0.015035156, 1.7744465)\n",
      "decoder loss ratio: 0.000076, decoder SINDy loss  ratio: 0.001268\n",
      "Epoch 270\n",
      "   training loss 2.368640161876101e-05, (4.1480844e-06, 0.17823724, 0.018314397, 1.7706878)\n",
      "   validation loss 2.2763259039493278e-05, (3.6510803e-06, 0.08329415, 0.014053017, 1.7706878)\n",
      "decoder loss ratio: 0.000020, decoder SINDy loss  ratio: 0.001186\n",
      "Epoch 271\n",
      "   training loss 4.454078589333221e-05, (2.5082885e-05, 0.17806517, 0.017759502, 1.7681953)\n",
      "   validation loss 4.378993617137894e-05, (2.4763898e-05, 0.08557845, 0.013440868, 1.7681953)\n",
      "decoder loss ratio: 0.000136, decoder SINDy loss  ratio: 0.001134\n",
      "Epoch 272\n",
      "   training loss 2.7186926672584377e-05, (7.5359803e-06, 0.17303462, 0.0201271, 1.7638237)\n",
      "   validation loss 2.6043158868560567e-05, (6.9067805e-06, 0.085750185, 0.014981416, 1.7638237)\n",
      "decoder loss ratio: 0.000038, decoder SINDy loss  ratio: 0.001264\n",
      "Epoch 273\n",
      "   training loss 3.7393649108707905e-05, (1.791407e-05, 0.1746281, 0.018627452, 1.7616837)\n",
      "   validation loss 3.5792996641248465e-05, (1.6773618e-05, 0.08465912, 0.014025452, 1.7616837)\n",
      "decoder loss ratio: 0.000092, decoder SINDy loss  ratio: 0.001183\n",
      "Epoch 274\n",
      "   training loss 2.5960274797398597e-05, (6.3967977e-06, 0.17552905, 0.019876003, 1.7575876)\n",
      "   validation loss 2.496254637662787e-05, (5.893789e-06, 0.08625634, 0.01492881, 1.7575876)\n",
      "decoder loss ratio: 0.000032, decoder SINDy loss  ratio: 0.001259\n",
      "Epoch 275\n",
      "   training loss 3.706111237988807e-05, (1.769296e-05, 0.17502604, 0.01815024, 1.7553128)\n",
      "   validation loss 3.560814366210252e-05, (1.6685828e-05, 0.08469809, 0.0136918565, 1.7553128)\n",
      "decoder loss ratio: 0.000092, decoder SINDy loss  ratio: 0.001155\n",
      "Epoch 276\n",
      "   training loss 2.581496664788574e-05, (6.332975e-06, 0.17565967, 0.019718152, 1.7510178)\n",
      "   validation loss 2.483257958374452e-05, (5.8392825e-06, 0.08670408, 0.014831198, 1.7510178)\n",
      "decoder loss ratio: 0.000032, decoder SINDy loss  ratio: 0.001251\n",
      "Epoch 277\n",
      "   training loss 3.586515595088713e-05, (1.6587906e-05, 0.17512465, 0.017893864, 1.7487863)\n",
      "   validation loss 3.4466589568182826e-05, (1.5627309e-05, 0.08476243, 0.01351418, 1.7487863)\n",
      "decoder loss ratio: 0.000086, decoder SINDy loss  ratio: 0.001140\n",
      "Epoch 278\n",
      "   training loss 2.7252299332758412e-05, (7.796182e-06, 0.17565005, 0.020114895, 1.7444627)\n",
      "   validation loss 2.618108737806324e-05, (7.2364114e-06, 0.08726958, 0.015000497, 1.7444627)\n",
      "decoder loss ratio: 0.000040, decoder SINDy loss  ratio: 0.001265\n",
      "Epoch 279\n",
      "   training loss 3.67505635949783e-05, (1.7540926e-05, 0.17516811, 0.017870415, 1.7422597)\n",
      "   validation loss 3.53808791260235e-05, (1.661421e-05, 0.08570367, 0.013440709, 1.7422597)\n",
      "decoder loss ratio: 0.000091, decoder SINDy loss  ratio: 0.001134\n",
      "Epoch 280\n",
      "   training loss 2.424382728349883e-05, (4.9774135e-06, 0.17603195, 0.018863153, 1.7380099)\n",
      "   validation loss 2.33079626923427e-05, (4.501421e-06, 0.08675343, 0.014264427, 1.7380099)\n",
      "decoder loss ratio: 0.000025, decoder SINDy loss  ratio: 0.001203\n",
      "Epoch 281\n",
      "   training loss 3.4892684197984636e-05, (1.5784273e-05, 0.17587209, 0.017510012, 1.7357411)\n",
      "   validation loss 3.356344677740708e-05, (1.4889741e-05, 0.08497189, 0.013162959, 1.7357411)\n",
      "decoder loss ratio: 0.000082, decoder SINDy loss  ratio: 0.001110\n",
      "Epoch 282\n",
      "   training loss 2.8495818696683273e-05, (9.15358e-06, 0.1755055, 0.02028979, 1.731326)\n",
      "   validation loss 2.7011530619347468e-05, (8.202947e-06, 0.088255994, 0.014953238, 1.731326)\n",
      "decoder loss ratio: 0.000045, decoder SINDy loss  ratio: 0.001261\n",
      "Epoch 283\n",
      "   training loss 3.270271554356441e-05, (1.355697e-05, 0.17390472, 0.018558595, 1.7289884)\n",
      "   validation loss 3.083412229898386e-05, (1.2166883e-05, 0.08663265, 0.013773564, 1.7289884)\n",
      "decoder loss ratio: 0.000067, decoder SINDy loss  ratio: 0.001162\n",
      "Epoch 284\n",
      "   training loss 2.99540588457603e-05, (1.06917605e-05, 0.17889185, 0.020111345, 1.7251165)\n",
      "   validation loss 2.83123881672509e-05, (9.581968e-06, 0.08838517, 0.01479256, 1.7251165)\n",
      "decoder loss ratio: 0.000053, decoder SINDy loss  ratio: 0.001248\n",
      "Epoch 285\n",
      "   training loss 5.1949595217593014e-05, (3.2918473e-05, 0.1784442, 0.018027253, 1.7228401)\n",
      "   validation loss 5.2029427024535835e-05, (3.346007e-05, 0.09024232, 0.013409569, 1.7228401)\n",
      "decoder loss ratio: 0.000184, decoder SINDy loss  ratio: 0.001131\n",
      "Epoch 286\n",
      "   training loss 2.3082713596522808e-05, (4.126092e-06, 0.18602698, 0.017949093, 1.7161713)\n",
      "   validation loss 2.2376443666871637e-05, (3.8638223e-06, 0.08931276, 0.013509086, 1.7161713)\n",
      "decoder loss ratio: 0.000021, decoder SINDy loss  ratio: 0.001140\n",
      "Epoch 287\n",
      "   training loss 2.244115967187099e-05, (3.596042e-06, 0.18556644, 0.017019814, 1.7143137)\n",
      "   validation loss 2.2111124053481035e-05, (3.6866163e-06, 0.087216325, 0.012813707, 1.7143137)\n",
      "decoder loss ratio: 0.000020, decoder SINDy loss  ratio: 0.001081\n",
      "Epoch 288\n",
      "   training loss 2.355927244934719e-05, (4.6712366e-06, 0.1838959, 0.017674513, 1.7120585)\n",
      "   validation loss 2.28023600357119e-05, (4.3812756e-06, 0.087104574, 0.013004989, 1.7120585)\n",
      "decoder loss ratio: 0.000024, decoder SINDy loss  ratio: 0.001097\n",
      "Epoch 289\n",
      "   training loss 2.8484606445999816e-05, (9.498663e-06, 0.18385005, 0.018954609, 1.7090484)\n",
      "   validation loss 2.6958749003824778e-05, (8.471355e-06, 0.08640866, 0.013969099, 1.7090484)\n",
      "decoder loss ratio: 0.000047, decoder SINDy loss  ratio: 0.001178\n",
      "Epoch 290\n",
      "   training loss 2.976429823320359e-05, (1.07423075e-05, 0.18265158, 0.019657064, 1.7056286)\n",
      "   validation loss 2.7613932616077363e-05, (9.126678e-06, 0.08707449, 0.01430968, 1.7056286)\n",
      "decoder loss ratio: 0.000050, decoder SINDy loss  ratio: 0.001207\n",
      "Epoch 291\n",
      "   training loss 2.5986071705119684e-05, (6.9542793e-06, 0.17633809, 0.020114351, 1.7020357)\n",
      "   validation loss 2.4624448997201398e-05, (6.1511496e-06, 0.08746842, 0.014529436, 1.7020357)\n",
      "decoder loss ratio: 0.000034, decoder SINDy loss  ratio: 0.001226\n",
      "Epoch 292\n",
      "   training loss 3.3671094570308924e-05, (1.4440149e-05, 0.17768176, 0.022418875, 1.698906)\n",
      "   validation loss 2.965961175505072e-05, (1.1063321e-05, 0.08829081, 0.016072327, 1.698906)\n",
      "decoder loss ratio: 0.000061, decoder SINDy loss  ratio: 0.001356\n",
      "Epoch 293\n",
      "   training loss 3.342168201925233e-05, (1.4304972e-05, 0.18091345, 0.021563571, 1.6960353)\n",
      "   validation loss 2.9852884836145677e-05, (1.1324034e-05, 0.08857092, 0.015684988, 1.6960353)\n",
      "decoder loss ratio: 0.000062, decoder SINDy loss  ratio: 0.001323\n",
      "Epoch 294\n",
      "   training loss 3.0146227800287306e-05, (1.1107162e-05, 0.18428876, 0.021106444, 1.6928422)\n",
      "   validation loss 2.6806861569639295e-05, (8.312081e-06, 0.08844904, 0.015663583, 1.6928422)\n",
      "decoder loss ratio: 0.000046, decoder SINDy loss  ratio: 0.001321\n",
      "Epoch 295\n",
      "   training loss 2.9098377126501873e-05, (1.0207459e-05, 0.18490136, 0.019961655, 1.6894754)\n",
      "   validation loss 2.6576086384011433e-05, (8.197802e-06, 0.08872687, 0.0148353, 1.6894754)\n",
      "decoder loss ratio: 0.000045, decoder SINDy loss  ratio: 0.001252\n",
      "Epoch 296\n",
      "   training loss 2.9298340450623073e-05, (1.04368355e-05, 0.18468046, 0.019989543, 1.6862551)\n",
      "   validation loss 2.667913395271171e-05, (8.343482e-06, 0.088221446, 0.014731027, 1.6862551)\n",
      "decoder loss ratio: 0.000046, decoder SINDy loss  ratio: 0.001243\n",
      "Epoch 297\n",
      "   training loss 3.872799061355181e-05, (1.9830157e-05, 0.18250199, 0.020678818, 1.6829952)\n",
      "   validation loss 3.47169661836233e-05, (1.6399319e-05, 0.089047804, 0.0148769505, 1.6829952)\n",
      "decoder loss ratio: 0.000090, decoder SINDy loss  ratio: 0.001255\n",
      "Epoch 298\n",
      "   training loss 2.776509427349083e-05, (9.067407e-06, 0.18245304, 0.019027645, 1.6794922)\n",
      "   validation loss 2.6543271815171465e-05, (8.380587e-06, 0.08856484, 0.013677632, 1.6794922)\n",
      "decoder loss ratio: 0.000046, decoder SINDy loss  ratio: 0.001154\n",
      "Epoch 299\n",
      "   training loss 3.8870814023539424e-05, (2.0132664e-05, 0.1828001, 0.0197349, 1.6764657)\n",
      "   validation loss 3.5752367693930864e-05, (1.7569042e-05, 0.09025956, 0.014186685, 1.6764657)\n",
      "decoder loss ratio: 0.000097, decoder SINDy loss  ratio: 0.001197\n",
      "Epoch 300\n",
      "   training loss 2.4805507564451545e-05, (6.2119134e-06, 0.18206535, 0.018706102, 1.6722986)\n",
      "   validation loss 2.367451997997705e-05, (5.5867413e-06, 0.08911091, 0.013647947, 1.6722986)\n",
      "decoder loss ratio: 0.000031, decoder SINDy loss  ratio: 0.001151\n",
      "Epoch 301\n",
      "   training loss 4.531882586888969e-05, (2.6854588e-05, 0.18464792, 0.017676651, 1.6696573)\n",
      "   validation loss 4.426439045346342e-05, (2.6299205e-05, 0.09183381, 0.012686124, 1.6696573)\n",
      "decoder loss ratio: 0.000145, decoder SINDy loss  ratio: 0.001070\n",
      "Epoch 302\n",
      "   training loss 2.718291216297075e-05, (8.79367e-06, 0.18267018, 0.017397739, 1.6649469)\n",
      "   validation loss 2.7032951038563624e-05, (9.0567e-06, 0.08808891, 0.013267819, 1.6649469)\n",
      "decoder loss ratio: 0.000050, decoder SINDy loss  ratio: 0.001119\n",
      "Epoch 303\n",
      "   training loss 3.737814404303208e-05, (1.9022777e-05, 0.18709093, 0.01732574, 1.6622794)\n",
      "   validation loss 3.6408564483281225e-05, (1.850072e-05, 0.08885521, 0.012850516, 1.6622794)\n",
      "decoder loss ratio: 0.000102, decoder SINDy loss  ratio: 0.001084\n",
      "Epoch 304\n",
      "   training loss 2.574754398665391e-05, (7.2461426e-06, 0.18179865, 0.01925281, 1.6576122)\n",
      "   validation loss 2.383417267992627e-05, (5.873917e-06, 0.090758644, 0.013841347, 1.6576122)\n",
      "decoder loss ratio: 0.000032, decoder SINDy loss  ratio: 0.001168\n",
      "Epoch 305\n",
      "   training loss 3.181757347192615e-05, (1.3515389e-05, 0.18177973, 0.017499182, 1.6552266)\n",
      "   validation loss 3.0082655939622782e-05, (1.2268142e-05, 0.08947319, 0.012622486, 1.6552266)\n",
      "decoder loss ratio: 0.000068, decoder SINDy loss  ratio: 0.001065\n",
      "Epoch 306\n",
      "   training loss 2.8025067877024412e-05, (9.711733e-06, 0.18759048, 0.017999263, 1.651341)\n",
      "   validation loss 2.6844380045076832e-05, (9.001737e-06, 0.08908719, 0.013292325, 1.651341)\n",
      "decoder loss ratio: 0.000050, decoder SINDy loss  ratio: 0.001121\n",
      "Epoch 307\n",
      "   training loss 5.8212142903357744e-05, (3.987839e-05, 0.19125746, 0.01849079, 1.6484677)\n",
      "   validation loss 5.927446181885898e-05, (4.1457068e-05, 0.09789977, 0.013327166, 1.6484677)\n",
      "decoder loss ratio: 0.000228, decoder SINDy loss  ratio: 0.001124\n",
      "Epoch 308\n",
      "   training loss 2.1242309230729006e-05, (3.0936876e-06, 0.18473072, 0.017302012, 1.6418421)\n",
      "   validation loss 2.0595573005266488e-05, (2.9251419e-06, 0.0905317, 0.012520105, 1.6418421)\n",
      "decoder loss ratio: 0.000016, decoder SINDy loss  ratio: 0.001056\n",
      "Epoch 309\n",
      "   training loss 2.261782174173277e-05, (4.523212e-06, 0.18236816, 0.017028708, 1.639174)\n",
      "   validation loss 2.201846473326441e-05, (4.378063e-06, 0.08848408, 0.012486632, 1.639174)\n",
      "decoder loss ratio: 0.000024, decoder SINDy loss  ratio: 0.001053\n",
      "Epoch 310\n",
      "   training loss 2.4689416022738442e-05, (6.611482e-06, 0.18388528, 0.01705762, 1.6372173)\n",
      "   validation loss 2.3555587176815607e-05, (5.919114e-06, 0.09113395, 0.01264301, 1.6372173)\n",
      "decoder loss ratio: 0.000033, decoder SINDy loss  ratio: 0.001067\n",
      "Epoch 311\n",
      "   training loss 2.7922691515414044e-05, (9.829233e-06, 0.18089318, 0.017463371, 1.6347121)\n",
      "   validation loss 2.5799174181884155e-05, (8.218937e-06, 0.089177065, 0.012331148, 1.6347121)\n",
      "decoder loss ratio: 0.000045, decoder SINDy loss  ratio: 0.001040\n",
      "Epoch 312\n",
      "   training loss 2.7596619474934414e-05, (9.527943e-06, 0.18003537, 0.017563533, 1.6312324)\n",
      "   validation loss 2.605886220408138e-05, (8.486465e-06, 0.087675065, 0.012600728, 1.6312324)\n",
      "decoder loss ratio: 0.000047, decoder SINDy loss  ratio: 0.001063\n",
      "Epoch 313\n",
      "   training loss 5.9797108406201005e-05, (4.1727853e-05, 0.19334598, 0.017845763, 1.6284678)\n",
      "   validation loss 6.0523005231516436e-05, (4.2958265e-05, 0.096473895, 0.01280062, 1.6284678)\n",
      "decoder loss ratio: 0.000237, decoder SINDy loss  ratio: 0.001080\n",
      "Epoch 314\n",
      "   training loss 2.139121716027148e-05, (3.4431573e-06, 0.19806741, 0.01733731, 1.6214329)\n",
      "   validation loss 2.0648147255997173e-05, (3.1875304e-06, 0.09253174, 0.012462889, 1.6214329)\n",
      "decoder loss ratio: 0.000018, decoder SINDy loss  ratio: 0.001051\n",
      "Epoch 315\n",
      "   training loss 2.312844299012795e-05, (5.3347676e-06, 0.19224517, 0.016066618, 1.6187015)\n",
      "   validation loss 2.3111510017770343e-05, (5.7260067e-06, 0.09018349, 0.011984889, 1.6187015)\n",
      "decoder loss ratio: 0.000032, decoder SINDy loss  ratio: 0.001011\n",
      "Epoch 316\n",
      "   training loss 2.595660771476105e-05, (8.035032e-06, 0.19031948, 0.017530384, 1.6168537)\n",
      "   validation loss 2.503488758520689e-05, (7.6011806e-06, 0.091174476, 0.012651705, 1.6168537)\n",
      "decoder loss ratio: 0.000042, decoder SINDy loss  ratio: 0.001067\n",
      "Epoch 317\n",
      "   training loss 3.253495378885418e-05, (1.4687443e-05, 0.18867113, 0.017060993, 1.6141411)\n",
      "   validation loss 3.1825089536141604e-05, (1.4437162e-05, 0.0888143, 0.012465172, 1.6141411)\n",
      "decoder loss ratio: 0.000079, decoder SINDy loss  ratio: 0.001052\n",
      "Epoch 318\n",
      "   training loss 2.4892655346775427e-05, (7.0462047e-06, 0.18686624, 0.017435309, 1.6102921)\n",
      "   validation loss 2.3831102225813083e-05, (6.4632163e-06, 0.08999271, 0.012649653, 1.6102921)\n",
      "decoder loss ratio: 0.000036, decoder SINDy loss  ratio: 0.001067\n",
      "Epoch 319\n",
      "   training loss 3.6446002923185006e-05, (1.8583134e-05, 0.1836486, 0.017861761, 1.6076692)\n",
      "   validation loss 3.460609877947718e-05, (1.7262915e-05, 0.090804115, 0.012664914, 1.6076692)\n",
      "decoder loss ratio: 0.000095, decoder SINDy loss  ratio: 0.001068\n",
      "Epoch 320\n",
      "   training loss 3.845101309707388e-05, (2.0795906e-05, 0.18140008, 0.016186075, 1.60365)\n",
      "   validation loss 3.8934107578825206e-05, (2.166396e-05, 0.08712336, 0.012336467, 1.60365)\n",
      "decoder loss ratio: 0.000119, decoder SINDy loss  ratio: 0.001041\n",
      "Epoch 321\n",
      "   training loss 2.296859929629136e-05, (5.3820213e-06, 0.1810015, 0.01580579, 1.6005999)\n",
      "   validation loss 2.215191489085555e-05, (4.9740574e-06, 0.086339, 0.011718588, 1.6005999)\n",
      "decoder loss ratio: 0.000027, decoder SINDy loss  ratio: 0.000989\n",
      "Epoch 322\n",
      "   training loss 4.986026760889217e-05, (3.2134514e-05, 0.18888545, 0.017485112, 1.5977243)\n",
      "   validation loss 4.9215512262890115e-05, (3.1980137e-05, 0.095028274, 0.01258133, 1.5977243)\n",
      "decoder loss ratio: 0.000176, decoder SINDy loss  ratio: 0.001061\n",
      "Epoch 323\n",
      "   training loss 2.207653596997261e-05, (4.5005872e-06, 0.18342352, 0.016559875, 1.5919961)\n",
      "   validation loss 2.1607193048112094e-05, (4.4792737e-06, 0.08951682, 0.01207959, 1.5919961)\n",
      "decoder loss ratio: 0.000025, decoder SINDy loss  ratio: 0.001019\n",
      "Epoch 324\n",
      "   training loss 2.3911976313684136e-05, (6.2895206e-06, 0.1818522, 0.017207552, 1.5901701)\n",
      "   validation loss 2.2559099306818098e-05, (5.3890503e-06, 0.089639306, 0.0126834735, 1.5901701)\n",
      "decoder loss ratio: 0.000030, decoder SINDy loss  ratio: 0.001070\n",
      "Epoch 325\n",
      "   training loss 3.400928108021617e-05, (1.6432397e-05, 0.18026876, 0.01700027, 1.5876857)\n",
      "   validation loss 3.2274710974888876e-05, (1.5199495e-05, 0.089311376, 0.011983596, 1.5876857)\n",
      "decoder loss ratio: 0.000084, decoder SINDy loss  ratio: 0.001011\n",
      "Epoch 326\n",
      "   training loss 2.1431796994875185e-05, (3.906509e-06, 0.17908294, 0.016873198, 1.5837969)\n",
      "   validation loss 2.0686164134531282e-05, (3.5994165e-06, 0.08669048, 0.0124877915, 1.5837969)\n",
      "decoder loss ratio: 0.000020, decoder SINDy loss  ratio: 0.001053\n",
      "Epoch 327\n",
      "   training loss 6.0895450587850064e-05, (4.32984e-05, 0.18588978, 0.01783267, 1.5813779)\n",
      "   validation loss 6.11201103311032e-05, (4.4044216e-05, 0.09420833, 0.012621173, 1.5813779)\n",
      "decoder loss ratio: 0.000243, decoder SINDy loss  ratio: 0.001065\n",
      "Epoch 328\n",
      "   training loss 2.0732240955112502e-05, (3.325978e-06, 0.19329129, 0.016573537, 1.5748909)\n",
      "   validation loss 2.0068724552402273e-05, (3.1204777e-06, 0.09187874, 0.011993374, 1.5748909)\n",
      "decoder loss ratio: 0.000017, decoder SINDy loss  ratio: 0.001012\n",
      "Epoch 329\n",
      "   training loss 2.4685134121682495e-05, (7.4286463e-06, 0.18598008, 0.015331656, 1.5723323)\n",
      "   validation loss 2.474815300956834e-05, (7.86459e-06, 0.086976044, 0.011602411, 1.5723323)\n",
      "decoder loss ratio: 0.000043, decoder SINDy loss  ratio: 0.000979\n",
      "Epoch 330\n",
      "   training loss 2.7359510568203405e-05, (9.960792e-06, 0.18396066, 0.016933363, 1.5705382)\n",
      "   validation loss 2.5805393306654878e-05, (8.8743645e-06, 0.09030549, 0.012256473, 1.5705382)\n",
      "decoder loss ratio: 0.000049, decoder SINDy loss  ratio: 0.001034\n",
      "Epoch 331\n",
      "   training loss 3.1032832339406013e-05, (1.36969065e-05, 0.18074535, 0.016580835, 1.5677844)\n",
      "   validation loss 3.007829946000129e-05, (1.3195753e-05, 0.087031096, 0.012047017, 1.5677844)\n",
      "decoder loss ratio: 0.000073, decoder SINDy loss  ratio: 0.001016\n",
      "Epoch 332\n",
      "   training loss 2.218113513663411e-05, (4.875422e-06, 0.18036349, 0.016674971, 1.5638216)\n",
      "   validation loss 2.1187792299315333e-05, (4.345886e-06, 0.087647475, 0.012036919, 1.5638216)\n",
      "decoder loss ratio: 0.000024, decoder SINDy loss  ratio: 0.001015\n",
      "Epoch 333\n",
      "   training loss 4.33473069278989e-05, (2.5997902e-05, 0.18203236, 0.017365668, 1.5612837)\n",
      "   validation loss 4.195232759229839e-05, (2.5094467e-05, 0.09061697, 0.012450258, 1.5612837)\n",
      "decoder loss ratio: 0.000138, decoder SINDy loss  ratio: 0.001050\n",
      "Epoch 334\n",
      "   training loss 2.195989145548083e-05, (4.7282856e-06, 0.17610121, 0.016620183, 1.5569588)\n",
      "   validation loss 2.1502713934751227e-05, (4.720601e-06, 0.0852882, 0.012125252, 1.5569588)\n",
      "decoder loss ratio: 0.000026, decoder SINDy loss  ratio: 0.001023\n",
      "Epoch 335\n",
      "   training loss 5.8412297221366316e-05, (4.1124054e-05, 0.18374066, 0.017451424, 1.55431)\n",
      "   validation loss 5.8207937399856746e-05, (4.1421e-05, 0.093992256, 0.012438376, 1.55431)\n",
      "decoder loss ratio: 0.000228, decoder SINDy loss  ratio: 0.001049\n",
      "Epoch 336\n",
      "   training loss 1.9976927433162928e-05, (2.859059e-06, 0.18451472, 0.016364248, 1.5481445)\n",
      "   validation loss 1.926972072396893e-05, (2.5996223e-06, 0.09075446, 0.0118865445, 1.5481445)\n",
      "decoder loss ratio: 0.000014, decoder SINDy loss  ratio: 0.001003\n",
      "Epoch 337\n",
      "   training loss 3.2144293072633445e-05, (1.5187172e-05, 0.17762184, 0.014981277, 1.5458995)\n",
      "   validation loss 3.271330933785066e-05, (1.6100257e-05, 0.087280385, 0.011540589, 1.5458995)\n",
      "decoder loss ratio: 0.000089, decoder SINDy loss  ratio: 0.000974\n",
      "Epoch 338\n",
      "   training loss 2.4349597879336216e-05, (7.2170537e-06, 0.17725998, 0.016974736, 1.5435072)\n",
      "   validation loss 2.3203074306366034e-05, (6.513704e-06, 0.0868518, 0.012542993, 1.5435072)\n",
      "decoder loss ratio: 0.000036, decoder SINDy loss  ratio: 0.001058\n",
      "Epoch 339\n",
      "   training loss 2.822370697685983e-05, (1.112622e-05, 0.17338562, 0.01692055, 1.5405432)\n",
      "   validation loss 2.6305962819606066e-05, (9.704759e-06, 0.086243734, 0.011957718, 1.5405432)\n",
      "decoder loss ratio: 0.000053, decoder SINDy loss  ratio: 0.001009\n",
      "Epoch 340\n",
      "   training loss 2.956579191959463e-05, (1.2619828e-05, 0.17392723, 0.015749443, 1.537102)\n",
      "   validation loss 2.9524755518650636e-05, (1.2971615e-05, 0.08179207, 0.011821211, 1.537102)\n",
      "decoder loss ratio: 0.000071, decoder SINDy loss  ratio: 0.000997\n",
      "Epoch 341\n",
      "   training loss 2.020742977038026e-05, (3.3145157e-06, 0.16940401, 0.015585879, 1.5334326)\n",
      "   validation loss 1.945986514328979e-05, (2.9859727e-06, 0.084994875, 0.011395672, 1.5334326)\n",
      "decoder loss ratio: 0.000016, decoder SINDy loss  ratio: 0.000961\n",
      "Epoch 342\n",
      "   training loss 7.171087781898677e-05, (5.444367e-05, 0.19337906, 0.019619739, 1.5305233)\n",
      "   validation loss 7.453345460817218e-05, (5.7806163e-05, 0.10426853, 0.014220598, 1.5305233)\n",
      "decoder loss ratio: 0.000318, decoder SINDy loss  ratio: 0.001200\n",
      "Epoch 343\n",
      "   training loss 2.1987529180478305e-05, (5.1187612e-06, 0.17880379, 0.016171869, 1.525158)\n",
      "   validation loss 2.1450186977745034e-05, (5.023819e-06, 0.09276518, 0.011747871, 1.525158)\n",
      "decoder loss ratio: 0.000028, decoder SINDy loss  ratio: 0.000991\n",
      "Epoch 344\n",
      "   training loss 2.543859591241926e-05, (8.673145e-06, 0.17214298, 0.015394797, 1.5225972)\n",
      "   validation loss 2.5227876903954893e-05, (8.838188e-06, 0.08659977, 0.011637181, 1.5225972)\n",
      "decoder loss ratio: 0.000049, decoder SINDy loss  ratio: 0.000982\n",
      "Epoch 345\n",
      "   training loss 2.497892091923859e-05, (8.113235e-06, 0.1717605, 0.016593527, 1.5206333)\n",
      "   validation loss 2.365278487559408e-05, (7.2345747e-06, 0.08791977, 0.01211878, 1.5206333)\n",
      "decoder loss ratio: 0.000040, decoder SINDy loss  ratio: 0.001022\n",
      "Epoch 346\n",
      "   training loss 3.248026769142598e-05, (1.5628573e-05, 0.17237197, 0.016738065, 1.5177886)\n",
      "   validation loss 2.92803997581359e-05, (1.2905893e-05, 0.09446079, 0.011966207, 1.5177886)\n",
      "decoder loss ratio: 0.000071, decoder SINDy loss  ratio: 0.001009\n",
      "Epoch 347\n",
      "   training loss 2.392068927292712e-05, (7.1294294e-06, 0.17055611, 0.016500078, 1.5141253)\n",
      "   validation loss 2.3041016902425326e-05, (6.6751977e-06, 0.08954008, 0.012245656, 1.5141253)\n",
      "decoder loss ratio: 0.000037, decoder SINDy loss  ratio: 0.001033\n",
      "Epoch 348\n",
      "   training loss 3.1283409043680876e-05, (1.4574613e-05, 0.17186572, 0.015983278, 1.5110468)\n",
      "   validation loss 3.0404011340579018e-05, (1.4145007e-05, 0.08780164, 0.011485372, 1.5110468)\n",
      "decoder loss ratio: 0.000078, decoder SINDy loss  ratio: 0.000969\n",
      "Epoch 349\n",
      "   training loss 3.0380197131307796e-05, (1.3753855e-05, 0.1695322, 0.015542235, 1.507212)\n",
      "   validation loss 3.0551273084711283e-05, (1.4278629e-05, 0.086705714, 0.012005251, 1.507212)\n",
      "decoder loss ratio: 0.000079, decoder SINDy loss  ratio: 0.001013\n",
      "Epoch 350\n",
      "   training loss 2.3845164832891896e-05, (7.212612e-06, 0.16827829, 0.015912747, 1.5041277)\n",
      "   validation loss 2.2790460207033902e-05, (6.5863273e-06, 0.08551545, 0.011628547, 1.5041277)\n",
      "decoder loss ratio: 0.000036, decoder SINDy loss  ratio: 0.000981\n",
      "Epoch 351\n",
      "   training loss 3.888571882271208e-05, (2.218194e-05, 0.17403431, 0.016933035, 1.5010477)\n",
      "   validation loss 3.7202662497293204e-05, (2.0978248e-05, 0.09204767, 0.01213935, 1.5010477)\n",
      "decoder loss ratio: 0.000116, decoder SINDy loss  ratio: 0.001024\n",
      "Epoch 352\n",
      "   training loss 2.3794058506609872e-05, (7.12374e-06, 0.16718346, 0.016975537, 1.4972765)\n",
      "   validation loss 2.282383866258897e-05, (6.619764e-06, 0.08753816, 0.012313094, 1.4972765)\n",
      "decoder loss ratio: 0.000036, decoder SINDy loss  ratio: 0.001039\n",
      "Epoch 353\n",
      "   training loss 4.0372018702328205e-05, (2.3786812e-05, 0.16722322, 0.016401652, 1.4945042)\n",
      "   validation loss 3.975964500568807e-05, (2.3681438e-05, 0.090077594, 0.011331675, 1.4945042)\n",
      "decoder loss ratio: 0.000130, decoder SINDy loss  ratio: 0.000956\n",
      "Epoch 354\n",
      "   training loss 2.398768992861733e-05, (7.3839196e-06, 0.16622679, 0.017005716, 1.49032)\n",
      "   validation loss 2.30315636144951e-05, (6.9131797e-06, 0.08789815, 0.012151856, 1.49032)\n",
      "decoder loss ratio: 0.000038, decoder SINDy loss  ratio: 0.001025\n",
      "Epoch 355\n",
      "   training loss 3.3833333873189986e-05, (1.7268363e-05, 0.16371128, 0.01689887, 1.4875083)\n",
      "   validation loss 3.257633215980604e-05, (1.6528631e-05, 0.088859744, 0.011726182, 1.4875083)\n",
      "decoder loss ratio: 0.000091, decoder SINDy loss  ratio: 0.000989\n",
      "Epoch 356\n",
      "   training loss 1.969018194358796e-05, (3.2514072e-06, 0.16220306, 0.01602703, 1.4836072)\n",
      "   validation loss 1.9114482711302117e-05, (3.1208415e-06, 0.08489455, 0.011575689, 1.4836072)\n",
      "decoder loss ratio: 0.000017, decoder SINDy loss  ratio: 0.000977\n",
      "Epoch 357\n",
      "   training loss 2.7536683774087578e-05, (1.1151906e-05, 0.15913098, 0.015752042, 1.4809575)\n",
      "   validation loss 2.61515706370119e-05, (1.0260725e-05, 0.08650551, 0.010812713, 1.4809575)\n",
      "decoder loss ratio: 0.000056, decoder SINDy loss  ratio: 0.000912\n",
      "Epoch 358\n",
      "   training loss 2.207332363468595e-05, (5.734166e-06, 0.16231132, 0.015665475, 1.4772611)\n",
      "   validation loss 2.1768380975117907e-05, (5.8501037e-06, 0.08301242, 0.011456661, 1.4772611)\n",
      "decoder loss ratio: 0.000032, decoder SINDy loss  ratio: 0.000966\n",
      "Epoch 359\n",
      "   training loss 4.147722938796505e-05, (2.5099314e-05, 0.16259521, 0.016336927, 1.4744222)\n",
      "   validation loss 4.090210859430954e-05, (2.5013327e-05, 0.089719824, 0.011445594, 1.4744222)\n",
      "decoder loss ratio: 0.000138, decoder SINDy loss  ratio: 0.000966\n",
      "Epoch 360\n",
      "   training loss 2.61640361713944e-05, (9.799845e-06, 0.16013038, 0.016587615, 1.4705429)\n",
      "   validation loss 2.4846254746080376e-05, (8.955525e-06, 0.08808232, 0.01185301, 1.4705429)\n",
      "decoder loss ratio: 0.000049, decoder SINDy loss  ratio: 0.001000\n",
      "Epoch 361\n",
      "   training loss 2.7118159778183326e-05, (1.0646359e-05, 0.15687943, 0.017993819, 1.467242)\n",
      "   validation loss 2.5123023078776896e-05, (9.1767515e-06, 0.09145716, 0.012738507, 1.467242)\n",
      "decoder loss ratio: 0.000051, decoder SINDy loss  ratio: 0.001075\n",
      "Epoch 362\n",
      "   training loss 1.9019931642105803e-05, (2.8180798e-06, 0.1577347, 0.015688878, 1.4632965)\n",
      "   validation loss 1.844145663199015e-05, (2.6692574e-06, 0.08435105, 0.011392353, 1.4632965)\n",
      "decoder loss ratio: 0.000015, decoder SINDy loss  ratio: 0.000961\n",
      "Epoch 363\n",
      "   training loss 4.462817378225736e-05, (2.8338767e-05, 0.15908492, 0.016839098, 1.4605497)\n",
      "   validation loss 4.432085552252829e-05, (2.8559029e-05, 0.088809595, 0.011563311, 1.4605497)\n",
      "decoder loss ratio: 0.000157, decoder SINDy loss  ratio: 0.000975\n",
      "Epoch 364\n",
      "   training loss 2.4523184038116597e-05, (8.491334e-06, 0.15393393, 0.014731965, 1.4558654)\n",
      "   validation loss 2.4302618840010837e-05, (8.665468e-06, 0.08677346, 0.010784983, 1.4558654)\n",
      "decoder loss ratio: 0.000048, decoder SINDy loss  ratio: 0.000910\n",
      "Epoch 365\n",
      "   training loss 2.4977580324048176e-05, (8.803134e-06, 0.15442939, 0.016446069, 1.4529841)\n",
      "   validation loss 2.397216303506866e-05, (8.284319e-06, 0.08602132, 0.011580039, 1.4529841)\n",
      "decoder loss ratio: 0.000046, decoder SINDy loss  ratio: 0.000977\n",
      "Epoch 366\n",
      "   training loss 2.79890500678448e-05, (1.17579775e-05, 0.1464682, 0.017321536, 1.4498919)\n",
      "   validation loss 2.7668520488077775e-05, (1.1953331e-05, 0.08416662, 0.012162717, 1.4498919)\n",
      "decoder loss ratio: 0.000066, decoder SINDy loss  ratio: 0.001026\n",
      "Epoch 367\n",
      "   training loss 1.9572145902202465e-05, (3.602949e-06, 0.15035133, 0.015028929, 1.4466304)\n",
      "   validation loss 1.895279456221033e-05, (3.399434e-06, 0.08293617, 0.010870565, 1.4466304)\n",
      "decoder loss ratio: 0.000019, decoder SINDy loss  ratio: 0.000917\n",
      "Epoch 368\n",
      "   training loss 4.62606803921517e-05, (3.0139046e-05, 0.1585161, 0.016849201, 1.4436716)\n",
      "   validation loss 4.629169779946096e-05, (3.0691983e-05, 0.09294077, 0.011630008, 1.4436716)\n",
      "decoder loss ratio: 0.000169, decoder SINDy loss  ratio: 0.000981\n",
      "Epoch 369\n",
      "   training loss 1.825884282879997e-05, (2.3931825e-06, 0.15570998, 0.014853682, 1.4380292)\n",
      "   validation loss 1.7598335034563206e-05, (2.1424307e-06, 0.09041364, 0.010756128, 1.4380292)\n",
      "decoder loss ratio: 0.000012, decoder SINDy loss  ratio: 0.000907\n",
      "Epoch 370\n",
      "   training loss 2.310921627213247e-05, (7.2561497e-06, 0.14947559, 0.014950064, 1.4358062)\n",
      "   validation loss 2.2521116989082657e-05, (7.060139e-06, 0.084135175, 0.011029166, 1.4358062)\n",
      "decoder loss ratio: 0.000039, decoder SINDy loss  ratio: 0.000930\n",
      "Epoch 371\n",
      "   training loss 2.0539184333756566e-05, (4.6821356e-06, 0.14417543, 0.015198671, 1.4337181)\n",
      "   validation loss 1.9438990420894697e-05, (4.000868e-06, 0.08481555, 0.011009427, 1.4337181)\n",
      "decoder loss ratio: 0.000022, decoder SINDy loss  ratio: 0.000929\n",
      "Epoch 372\n",
      "   training loss 2.64345871983096e-05, (1.0532655e-05, 0.14058048, 0.015935458, 1.4308387)\n",
      "   validation loss 2.4406856027781032e-05, (8.986705e-06, 0.08722827, 0.011117639, 1.4308387)\n",
      "decoder loss ratio: 0.000049, decoder SINDy loss  ratio: 0.000938\n",
      "Epoch 373\n",
      "   training loss 2.2069240003474988e-05, (6.38388e-06, 0.14783853, 0.014114956, 1.4273865)\n",
      "   validation loss 2.1366140572354198e-05, (6.0954267e-06, 0.085963994, 0.009968492, 1.4273865)\n",
      "decoder loss ratio: 0.000034, decoder SINDy loss  ratio: 0.000841\n",
      "Epoch 374\n",
      "   training loss 2.1433725123642944e-05, (5.6620397e-06, 0.14296319, 0.015292661, 1.424242)\n",
      "   validation loss 2.0796238459297456e-05, (5.4256616e-06, 0.085807145, 0.011281572, 1.424242)\n",
      "decoder loss ratio: 0.000030, decoder SINDy loss  ratio: 0.000952\n",
      "Epoch 375\n",
      "   training loss 3.570645276340656e-05, (1.986359e-05, 0.14247154, 0.016310219, 1.4211842)\n",
      "   validation loss 3.462360837147571e-05, (1.9288593e-05, 0.08892062, 0.011231752, 1.4211842)\n",
      "decoder loss ratio: 0.000106, decoder SINDy loss  ratio: 0.000948\n",
      "Epoch 376\n",
      "   training loss 2.0200030121486634e-05, (4.43917e-06, 0.14399457, 0.015905272, 1.4170334)\n",
      "   validation loss 1.9269629774498753e-05, (3.983923e-06, 0.08677594, 0.011153728, 1.4170334)\n",
      "decoder loss ratio: 0.000022, decoder SINDy loss  ratio: 0.000941\n",
      "Epoch 377\n",
      "   training loss 2.6112509658560157e-05, (1.0377321e-05, 0.1378427, 0.01593527, 1.4141663)\n",
      "   validation loss 2.4398974346695468e-05, (9.155142e-06, 0.08762132, 0.011021689, 1.4141663)\n",
      "decoder loss ratio: 0.000050, decoder SINDy loss  ratio: 0.000930\n",
      "Epoch 378\n",
      "   training loss 2.1125873900018632e-05, (5.544995e-06, 0.14010686, 0.0147752855, 1.4103352)\n",
      "   validation loss 2.0562365534715354e-05, (5.395933e-06, 0.0818143, 0.010630808, 1.4103352)\n",
      "decoder loss ratio: 0.000030, decoder SINDy loss  ratio: 0.000897\n",
      "Epoch 379\n",
      "   training loss 2.463499731675256e-05, (9.046455e-06, 0.13478498, 0.015164083, 1.4072136)\n",
      "   validation loss 2.3413287635776214e-05, (8.262052e-06, 0.08599899, 0.010791007, 1.4072136)\n",
      "decoder loss ratio: 0.000045, decoder SINDy loss  ratio: 0.000910\n",
      "Epoch 380\n",
      "   training loss 2.5795106921577826e-05, (9.974306e-06, 0.1345294, 0.017861934, 1.403461)\n",
      "   validation loss 2.3483629775000736e-05, (8.173748e-06, 0.09141224, 0.012752729, 1.403461)\n",
      "decoder loss ratio: 0.000045, decoder SINDy loss  ratio: 0.001076\n",
      "Epoch 381\n",
      "   training loss 2.4981371097965166e-05, (9.350295e-06, 0.13632898, 0.016331702, 1.3997905)\n",
      "   validation loss 2.425537968520075e-05, (9.076952e-06, 0.08970931, 0.011805227, 1.3997905)\n",
      "decoder loss ratio: 0.000050, decoder SINDy loss  ratio: 0.000996\n",
      "Epoch 382\n",
      "   training loss 2.4365450371988118e-05, (8.827042e-06, 0.13098638, 0.01572643, 1.3965766)\n",
      "   validation loss 2.330828647245653e-05, (8.223854e-06, 0.08682329, 0.011186678, 1.3965766)\n",
      "decoder loss ratio: 0.000045, decoder SINDy loss  ratio: 0.000944\n",
      "Epoch 383\n",
      "   training loss 2.233779196103569e-05, (6.692223e-06, 0.128236, 0.017200384, 1.3925531)\n",
      "   validation loss 2.1067127818241715e-05, (5.927799e-06, 0.08778048, 0.012137983, 1.3925531)\n",
      "decoder loss ratio: 0.000033, decoder SINDy loss  ratio: 0.001024\n",
      "Epoch 384\n",
      "   training loss 3.447494236752391e-05, (1.8863093e-05, 0.13415813, 0.017121729, 1.3899678)\n",
      "   validation loss 3.356183151481673e-05, (1.8422643e-05, 0.09145981, 0.012395098, 1.3899678)\n",
      "decoder loss ratio: 0.000101, decoder SINDy loss  ratio: 0.001046\n",
      "Epoch 385\n",
      "   training loss 1.8229857232654467e-05, (2.880775e-06, 0.1289848, 0.01480967, 1.3868115)\n",
      "   validation loss 1.7423875760869123e-05, (2.528126e-06, 0.08375882, 0.010276356, 1.3868115)\n",
      "decoder loss ratio: 0.000014, decoder SINDy loss  ratio: 0.000867\n",
      "Epoch 386\n",
      "   training loss 4.304945468902588e-05, (2.7562348e-05, 0.12900221, 0.016404249, 1.3846682)\n",
      "   validation loss 4.1854975279420614e-05, (2.6862008e-05, 0.091429554, 0.011462855, 1.3846682)\n",
      "decoder loss ratio: 0.000148, decoder SINDy loss  ratio: 0.000967\n",
      "Epoch 387\n",
      "   training loss 1.7818263586377725e-05, (2.5248555e-06, 0.12521279, 0.014814723, 1.3811936)\n",
      "   validation loss 1.7065947758965194e-05, (2.2177362e-06, 0.08427673, 0.010362747, 1.3811936)\n",
      "decoder loss ratio: 0.000012, decoder SINDy loss  ratio: 0.000874\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_77504\\4054194948.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mresults_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mresults_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ianma\\Desktop\\Study\\Python\\CS4240_DeepLearning_Group41_ReproducibilityProject\\SindyAutoencoders-master\\src\\training.py\u001b[0m in \u001b[0;36mtrain_network\u001b[1;34m(training_data, val_data, params)\u001b[0m\n\u001b[0;32m     31\u001b[0m                 \u001b[0mbatch_idxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'batch_size'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'batch_size'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m                 \u001b[0mtrain_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_feed_dictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midxs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_idxs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m                 \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'print_progress'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'print_frequency'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m                 \u001b[0mvalidation_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msindy_predict_norm_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ianma\\anaconda3\\envs\\tfv1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 956\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    957\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ianma\\anaconda3\\envs\\tfv1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1180\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1181\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ianma\\anaconda3\\envs\\tfv1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1359\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ianma\\anaconda3\\envs\\tfv1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1363\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ianma\\anaconda3\\envs\\tfv1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1350\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ianma\\anaconda3\\envs\\tfv1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1443\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_experiments = 1\n",
    "df = pd.DataFrame()\n",
    "for i in range(num_experiments):\n",
    "    print('EXPERIMENT %d' % i)\n",
    "\n",
    "    params['coefficient_mask'] = np.ones((params['library_dim'], params['latent_dim']))\n",
    "\n",
    "    params['save_name'] = 'lorenz_' + datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S_%f\")\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    results_dict = train_network(training_data, validation_data, params)\n",
    "    df = df.append({**results_dict, **params}, ignore_index=True)\n",
    "\n",
    "df.to_pickle('experiment_results_' + datetime.datetime.now().strftime(\"%Y%m%d%H%M\") + '.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
