{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../src\")\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from example_lorenz import get_lorenz_data\n",
    "from sindy_utils import library_size\n",
    "from training import train_network\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate training, validation, testing data\n",
    "noise_strength = 1e-6\n",
    "training_data = get_lorenz_data(1024, noise_strength=noise_strength)\n",
    "validation_data = get_lorenz_data(20, noise_strength=noise_strength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up model and training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256000\n"
     ]
    }
   ],
   "source": [
    "params = {}\n",
    "\n",
    "params['input_dim'] = 128\n",
    "params['latent_dim'] = 3\n",
    "params['model_order'] = 1\n",
    "params['poly_order'] = 3\n",
    "params['include_sine'] = False\n",
    "params['library_dim'] = library_size(params['latent_dim'], params['poly_order'], params['include_sine'], True)\n",
    "\n",
    "# sequential thresholding parameters\n",
    "params['sequential_thresholding'] = True\n",
    "params['coefficient_threshold'] = 0.1\n",
    "params['threshold_frequency'] = 500\n",
    "params['coefficient_mask'] = np.ones((params['library_dim'], params['latent_dim']))\n",
    "params['coefficient_initialization'] = 'constant'\n",
    "\n",
    "# loss function weighting\n",
    "params['loss_weight_decoder'] = 1.0\n",
    "params['loss_weight_sindy_z'] = 0.0\n",
    "params['loss_weight_sindy_x'] = 1e-4\n",
    "params['loss_weight_sindy_regularization'] = 1e-5\n",
    "\n",
    "params['activation'] = 'sigmoid'\n",
    "params['widths'] = [64,32]\n",
    "\n",
    "# training parameters\n",
    "params['epoch_size'] = training_data['x'].shape[0]\n",
    "print(training_data['x'].shape[0])\n",
    "params['batch_size'] = 1024\n",
    "params['learning_rate'] = 1e-3\n",
    "\n",
    "params['data_path'] = os.getcwd() + '/'\n",
    "params['print_progress'] = True\n",
    "params['print_frequency'] = 1\n",
    "\n",
    "# training time cutoffs\n",
    "params['max_epochs'] = 5001\n",
    "params['refinement_epochs'] = 1001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT 0\n",
      "WARNING:tensorflow:From C:\\Users\\ianma\\AppData\\Local\\Temp\\ipykernel_32028\\4054194948.py:10: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From ../../src\\autoencoder.py:28: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From ../../src\\autoencoder.py:194: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From ../../src\\training.py:11: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\ianma\\anaconda3\\envs\\tfv1\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From ../../src\\training.py:13: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "WARNING:tensorflow:From ../../src\\training.py:13: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From ../../src\\training.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "TRAINING\n",
      "WARNING:tensorflow:From ../../src\\training.py:27: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From ../../src\\training.py:28: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "Epoch 0\n",
      "   training loss 0.029037723317742348, (0.027962368, 519.7591, 10.66411, 0.8944565)\n",
      "   validation loss 0.032073359936475754, (0.031057918, 558.9735, 10.064965, 0.8944565)\n",
      "decoder loss ratio: 0.168244, decoder SINDy loss  ratio: 1.017007\n",
      "Epoch 1\n",
      "   training loss 0.022818291559815407, (0.021753708, 563.2289, 10.561586, 0.8424104)\n",
      "   validation loss 0.025342537090182304, (0.024336245, 614.80853, 9.978666, 0.8424104)\n",
      "decoder loss ratio: 0.131832, decoder SINDy loss  ratio: 1.008287\n",
      "Epoch 2\n",
      "   training loss 0.010537042282521725, (0.009626428, 139.83049, 9.025419, 0.80722976)\n",
      "   validation loss 0.010892481543123722, (0.010025295, 149.86264, 8.591138, 0.80722976)\n",
      "decoder loss ratio: 0.054308, decoder SINDy loss  ratio: 0.868085\n",
      "Epoch 3\n",
      "   training loss 0.008561057038605213, (0.007743918, 95.980965, 8.089718, 0.81681335)\n",
      "   validation loss 0.00880043301731348, (0.008033933, 94.84547, 7.583325, 0.81681335)\n",
      "decoder loss ratio: 0.043521, decoder SINDy loss  ratio: 0.766251\n",
      "Epoch 4\n",
      "   training loss 0.007644352037459612, (0.006900756, 83.319984, 7.351258, 0.84699476)\n",
      "   validation loss 0.007909171283245087, (0.007224529, 81.26764, 6.7617197, 0.84699476)\n",
      "decoder loss ratio: 0.039136, decoder SINDy loss  ratio: 0.683233\n",
      "Epoch 5\n",
      "   training loss 0.006963226944208145, (0.0062803216, 75.23277, 6.7419343, 0.8712111)\n",
      "   validation loss 0.007217908278107643, (0.006601636, 72.71763, 6.0756035, 0.8712111)\n",
      "decoder loss ratio: 0.035762, decoder SINDy loss  ratio: 0.613905\n",
      "Epoch 6\n",
      "   training loss 0.006443579215556383, (0.0058130664, 64.29951, 6.2139893, 0.91139543)\n",
      "   validation loss 0.006682541687041521, (0.006127517, 60.14375, 5.459105, 0.91139543)\n",
      "decoder loss ratio: 0.033193, decoder SINDy loss  ratio: 0.551611\n",
      "Epoch 7\n",
      "   training loss 0.006080190185457468, (0.00549893, 54.809692, 5.7177587, 0.948447)\n",
      "   validation loss 0.006299152970314026, (0.005798276, 48.859634, 4.913924, 0.948447)\n",
      "decoder loss ratio: 0.031410, decoder SINDy loss  ratio: 0.496524\n",
      "Epoch 8\n",
      "   training loss 0.005773148033767939, (0.0052352324, 45.25531, 5.280691, 0.98462766)\n",
      "   validation loss 0.005976508371531963, (0.0055172388, 38.719936, 4.4942336, 0.98462766)\n",
      "decoder loss ratio: 0.029887, decoder SINDy loss  ratio: 0.454116\n",
      "Epoch 9\n",
      "   training loss 0.005304867867380381, (0.0048104026, 34.9773, 4.8432236, 1.0143034)\n",
      "   validation loss 0.005473933648318052, (0.0050498615, 30.24205, 4.139292, 1.0143034)\n",
      "decoder loss ratio: 0.027356, decoder SINDy loss  ratio: 0.418251\n",
      "Epoch 10\n",
      "   training loss 0.003533616429194808, (0.003085053, 26.398523, 4.3819466, 1.0368571)\n",
      "   validation loss 0.003615533234551549, (0.003208054, 23.50947, 3.971107, 1.0368571)\n",
      "decoder loss ratio: 0.017378, decoder SINDy loss  ratio: 0.401257\n",
      "Epoch 11\n",
      "   training loss 0.002285100519657135, (0.001876798, 19.272392, 3.9770107, 1.0601379)\n",
      "   validation loss 0.002248826902359724, (0.0018674503, 17.415987, 3.7077508, 1.0601379)\n",
      "decoder loss ratio: 0.010116, decoder SINDy loss  ratio: 0.374647\n",
      "Epoch 12\n",
      "   training loss 0.001810271292924881, (0.0014292446, 12.78905, 3.703249, 1.0701869)\n",
      "   validation loss 0.0017462106188759208, (0.0013889177, 12.784197, 3.4659114, 1.0701869)\n",
      "decoder loss ratio: 0.007524, decoder SINDy loss  ratio: 0.350210\n",
      "Epoch 13\n",
      "   training loss 0.0015203147195279598, (0.0011644918, 9.485455, 3.4496942, 1.0853524)\n",
      "   validation loss 0.0014601596631109715, (0.001126595, 10.020736, 3.2271118, 1.0853524)\n",
      "decoder loss ratio: 0.006103, decoder SINDy loss  ratio: 0.326081\n",
      "Epoch 14\n",
      "   training loss 0.0013202913105487823, (0.0009946917, 7.348364, 3.1448703, 1.1112485)\n",
      "   validation loss 0.0012690759031102061, (0.0009578985, 8.042422, 3.0006483, 1.1112485)\n",
      "decoder loss ratio: 0.005189, decoder SINDy loss  ratio: 0.303198\n",
      "Epoch 15\n",
      "   training loss 0.0012204345548525453, (0.0009230318, 5.912636, 2.8598661, 1.1416095)\n",
      "   validation loss 0.0011809744173660874, (0.0008893012, 6.702564, 2.8025713, 1.1416095)\n",
      "decoder loss ratio: 0.004817, decoder SINDy loss  ratio: 0.283184\n",
      "Epoch 16\n",
      "   training loss 0.001087347511202097, (0.00081659155, 5.123749, 2.5899267, 1.1763356)\n",
      "   validation loss 0.0010619350941851735, (0.0007896736, 5.80739, 2.6049821, 1.1763356)\n",
      "decoder loss ratio: 0.004278, decoder SINDy loss  ratio: 0.263218\n",
      "Epoch 17\n",
      "   training loss 0.0009898380376398563, (0.00074193435, 4.83483, 2.3582354, 1.2080154)\n",
      "   validation loss 0.0009753468330018222, (0.00072202005, 5.3496585, 2.4124663, 1.2080154)\n",
      "decoder loss ratio: 0.003911, decoder SINDy loss  ratio: 0.243766\n",
      "Epoch 18\n",
      "   training loss 0.0009301495738327503, (0.00070053374, 4.5439787, 2.1714177, 1.247406)\n",
      "   validation loss 0.0009282803512178361, (0.00068868336, 4.90676, 2.2712293, 1.247406)\n",
      "decoder loss ratio: 0.003731, decoder SINDy loss  ratio: 0.229495\n",
      "Epoch 19\n",
      "   training loss 0.0008847620920278132, (0.00067051756, 4.372772, 2.0136056, 1.2883952)\n",
      "   validation loss 0.0008918868261389434, (0.00066526455, 4.564118, 2.137383, 1.2883952)\n",
      "decoder loss ratio: 0.003604, decoder SINDy loss  ratio: 0.215970\n",
      "Epoch 20\n",
      "   training loss 0.0008309285622090101, (0.0006304097, 4.208739, 1.8725832, 1.3260592)\n",
      "   validation loss 0.0008468630840070546, (0.00063109625, 4.2740006, 2.0250626, 1.3260592)\n",
      "decoder loss ratio: 0.003419, decoder SINDy loss  ratio: 0.204621\n",
      "Epoch 21\n",
      "   training loss 0.0007952403975650668, (0.0006068648, 4.2004886, 1.7479612, 1.3579521)\n",
      "   validation loss 0.000816457555629313, (0.0006122993, 4.115391, 1.9057877, 1.3579521)\n",
      "decoder loss ratio: 0.003317, decoder SINDy loss  ratio: 0.192569\n",
      "Epoch 22\n",
      "   training loss 0.00075434212340042, (0.00057687867, 4.046368, 1.6357803, 1.388546)\n",
      "   validation loss 0.0007827256340533495, (0.0005873247, 3.894012, 1.8151547, 1.388546)\n",
      "decoder loss ratio: 0.003182, decoder SINDy loss  ratio: 0.183411\n",
      "Epoch 23\n",
      "   training loss 0.0007370600942522287, (0.0005696942, 3.9025536, 1.5320357, 1.4162335)\n",
      "   validation loss 0.0007709580240771174, (0.0005842142, 3.7060134, 1.7258147, 1.4162335)\n",
      "decoder loss ratio: 0.003165, decoder SINDy loss  ratio: 0.174384\n",
      "Epoch 24\n",
      "   training loss 0.0006936109275557101, (0.00053450157, 3.9006832, 1.4472108, 1.4388303)\n",
      "   validation loss 0.0007299839053303003, (0.00055270514, 3.606648, 1.6289048, 1.4388303)\n",
      "decoder loss ratio: 0.002994, decoder SINDy loss  ratio: 0.164591\n",
      "Epoch 25\n",
      "   training loss 0.0006621077191084623, (0.00051027094, 3.7016842, 1.3718681, 1.4649932)\n",
      "   validation loss 0.000703684869222343, (0.00053164613, 3.4138045, 1.5738881, 1.4649932)\n",
      "decoder loss ratio: 0.002880, decoder SINDy loss  ratio: 0.159032\n",
      "Epoch 26\n",
      "   training loss 0.0006340622203424573, (0.00048981677, 3.5478926, 1.2933948, 1.4905976)\n",
      "   validation loss 0.0006789281615056098, (0.00051512226, 3.2360413, 1.4889994, 1.4905976)\n",
      "decoder loss ratio: 0.002790, decoder SINDy loss  ratio: 0.150455\n",
      "Epoch 27\n",
      "   training loss 0.0006032256642356515, (0.00046474542, 3.4571373, 1.233467, 1.513354)\n",
      "   validation loss 0.0006495844572782516, (0.0004912136, 3.1114666, 1.4323733, 1.513354)\n",
      "decoder loss ratio: 0.002661, decoder SINDy loss  ratio: 0.144733\n",
      "Epoch 28\n",
      "   training loss 0.0005591204972006381, (0.00042705226, 3.295453, 1.1670499, 1.5363255)\n",
      "   validation loss 0.0006080968305468559, (0.00045741472, 2.9493532, 1.3531884, 1.5363255)\n",
      "decoder loss ratio: 0.002478, decoder SINDy loss  ratio: 0.136732\n",
      "Epoch 29\n",
      "   training loss 0.0005509505281224847, (0.00042430527, 3.1788042, 1.1106772, 1.557752)\n",
      "   validation loss 0.000600151892285794, (0.0004555279, 2.8424847, 1.2904649, 1.557752)\n",
      "decoder loss ratio: 0.002468, decoder SINDy loss  ratio: 0.130394\n",
      "Epoch 30\n",
      "   training loss 0.0005312109715305269, (0.00040961016, 3.040816, 1.058339, 1.5766895)\n",
      "   validation loss 0.0005799361388199031, (0.00044073086, 2.7409708, 1.2343843, 1.5766895)\n",
      "decoder loss ratio: 0.002387, decoder SINDy loss  ratio: 0.124727\n",
      "Epoch 31\n",
      "   training loss 0.0004672301292885095, (0.00035029952, 2.8767703, 1.0098354, 1.5947058)\n",
      "   validation loss 0.0005165779148228467, (0.00038216772, 2.6155994, 1.1846316, 1.5947058)\n",
      "decoder loss ratio: 0.002070, decoder SINDy loss  ratio: 0.119700\n",
      "Epoch 32\n",
      "   training loss 0.00045595233677886426, (0.00034370687, 2.766118, 0.96141106, 1.6104368)\n",
      "   validation loss 0.0005017171497456729, (0.00037294143, 2.5011718, 1.1267135, 1.6104368)\n",
      "decoder loss ratio: 0.002020, decoder SINDy loss  ratio: 0.113848\n",
      "Epoch 33\n",
      "   training loss 0.0004195116343908012, (0.0003113283, 2.6242554, 0.9191559, 1.6267765)\n",
      "   validation loss 0.00046335847582668066, (0.00033906873, 2.3859375, 1.08022, 1.6267765)\n",
      "decoder loss ratio: 0.001837, decoder SINDy loss  ratio: 0.109150\n",
      "Epoch 34\n",
      "   training loss 0.0003881854354403913, (0.0002836893, 2.5397058, 0.88090956, 1.640519)\n",
      "   validation loss 0.00042694591684266925, (0.0003063568, 2.3028421, 1.0418396, 1.640519)\n",
      "decoder loss ratio: 0.001660, decoder SINDy loss  ratio: 0.105272\n",
      "Epoch 35\n",
      "   training loss 0.00035678205313161016, (0.0002561756, 2.4185052, 0.8405324, 1.6553216)\n",
      "   validation loss 0.0003904402256011963, (0.00027519366, 2.204336, 0.98693377, 1.6553216)\n",
      "decoder loss ratio: 0.001491, decoder SINDy loss  ratio: 0.099724\n",
      "Epoch 36\n",
      "   training loss 0.000337398232659325, (0.00024003483, 2.3859284, 0.80695134, 1.6668248)\n",
      "   validation loss 0.0003642825176939368, (0.00025276715, 2.1529624, 0.94847107, 1.6668248)\n",
      "decoder loss ratio: 0.001369, decoder SINDy loss  ratio: 0.095838\n",
      "Epoch 37\n",
      "   training loss 0.00031629446311853826, (0.00022191506, 2.3022735, 0.77602977, 1.6776403)\n",
      "   validation loss 0.00033680599881336093, (0.00022956896, 2.0768607, 0.9046062, 1.6776403)\n",
      "decoder loss ratio: 0.001244, decoder SINDy loss  ratio: 0.091405\n",
      "Epoch 38\n",
      "   training loss 0.0002927063324023038, (0.00020116818, 2.2943993, 0.7467951, 1.6858654)\n",
      "   validation loss 0.0003058207221329212, (0.00020211707, 2.0398674, 0.86845, 1.6858654)\n",
      "decoder loss ratio: 0.001095, decoder SINDy loss  ratio: 0.087752\n",
      "Epoch 39\n",
      "   training loss 0.00030845843139104545, (0.00021932529, 2.2459888, 0.7219858, 1.6934552)\n",
      "   validation loss 0.0003131858538836241, (0.00021312747, 1.9922732, 0.8312384, 1.6934552)\n",
      "decoder loss ratio: 0.001155, decoder SINDy loss  ratio: 0.083992\n",
      "Epoch 40\n",
      "   training loss 0.00027126516215503216, (0.00018549507, 2.2087138, 0.68764776, 1.7005322)\n",
      "   validation loss 0.00026948199956677854, (0.00017370214, 1.9434493, 0.7877455, 1.7005322)\n",
      "decoder loss ratio: 0.000941, decoder SINDy loss  ratio: 0.079597\n",
      "Epoch 41\n",
      "   training loss 0.0002474159118719399, (0.00016377508, 2.2761717, 0.6660904, 1.7031806)\n",
      "   validation loss 0.00023976566444616765, (0.00014718281, 1.9350089, 0.7555105, 1.7031806)\n",
      "decoder loss ratio: 0.000797, decoder SINDy loss  ratio: 0.076340\n",
      "Epoch 42\n",
      "   training loss 0.0002326986432308331, (0.00015135562, 2.1961782, 0.6426589, 1.7077143)\n",
      "   validation loss 0.00021838751854375005, (0.00012891665, 1.8807789, 0.7239372, 1.7077143)\n",
      "decoder loss ratio: 0.000698, decoder SINDy loss  ratio: 0.073150\n",
      "Epoch 43\n",
      "   training loss 0.00023100193357095122, (0.00015164385, 2.1512496, 0.6220417, 1.715392)\n",
      "   validation loss 0.000210950878681615, (0.00012438043, 1.8316005, 0.6941653, 1.715392)\n",
      "decoder loss ratio: 0.000674, decoder SINDy loss  ratio: 0.070141\n",
      "Epoch 44\n",
      "   training loss 0.00021677110635209829, (0.00013898282, 2.1432314, 0.60580283, 1.7208003)\n",
      "   validation loss 0.00019164332479704171, (0.00010741987, 1.7931648, 0.67015445, 1.7208003)\n",
      "decoder loss ratio: 0.000582, decoder SINDy loss  ratio: 0.067715\n",
      "Epoch 45\n",
      "   training loss 0.00020953809143975377, (0.00013315005, 2.077437, 0.5910833, 1.7279713)\n",
      "   validation loss 0.0001809399254852906, (9.903257e-05, 1.7401987, 0.64627635, 1.7279713)\n",
      "decoder loss ratio: 0.000536, decoder SINDy loss  ratio: 0.065302\n",
      "Epoch 46\n",
      "   training loss 0.0002171293890569359, (0.00014217521, 2.0537953, 0.5760087, 1.7353299)\n",
      "   validation loss 0.0001850862754508853, (0.000105629755, 1.6931319, 0.6210322, 1.7353299)\n",
      "decoder loss ratio: 0.000572, decoder SINDy loss  ratio: 0.062752\n",
      "Epoch 47\n",
      "   training loss 0.0002131567889591679, (0.00013924329, 2.0200355, 0.56494087, 1.7419415)\n",
      "   validation loss 0.00017980762640945613, (0.000102307284, 1.6436347, 0.6008093, 1.7419415)\n",
      "decoder loss ratio: 0.000554, decoder SINDy loss  ratio: 0.060708\n",
      "Epoch 48\n",
      "   training loss 0.00019983250240329653, (0.00012691022, 1.9530026, 0.55427814, 1.7494477)\n",
      "   validation loss 0.0001642498973524198, (8.818804e-05, 1.5879606, 0.5856738, 1.7494477)\n",
      "decoder loss ratio: 0.000478, decoder SINDy loss  ratio: 0.059179\n",
      "Epoch 49\n",
      "   training loss 0.0001878649927675724, (0.00011625636, 1.9512018, 0.54047227, 1.7561419)\n",
      "   validation loss 0.00015083578182384372, (7.7339384e-05, 1.5489174, 0.5593499, 1.7561419)\n",
      "decoder loss ratio: 0.000419, decoder SINDy loss  ratio: 0.056519\n",
      "Epoch 50\n",
      "   training loss 0.00019516880274750292, (0.00012422957, 1.8541023, 0.5329807, 1.7641169)\n",
      "   validation loss 0.00015740119852125645, (8.489891e-05, 1.4859077, 0.54861116, 1.7641169)\n",
      "decoder loss ratio: 0.000460, decoder SINDy loss  ratio: 0.055434\n",
      "Epoch 51\n",
      "   training loss 0.00019458330643828958, (0.00012463843, 1.8409066, 0.5222849, 1.771639)\n",
      "   validation loss 0.0001551374007249251, (8.456253e-05, 1.447888, 0.5285848, 1.771639)\n",
      "decoder loss ratio: 0.000458, decoder SINDy loss  ratio: 0.053410\n",
      "Epoch 52\n",
      "   training loss 0.00019011089170817286, (0.00012110207, 1.7588897, 0.5121498, 1.779383)\n",
      "   validation loss 0.00015095266280695796, (8.160081e-05, 1.3906206, 0.5155802, 1.779383)\n",
      "decoder loss ratio: 0.000442, decoder SINDy loss  ratio: 0.052096\n",
      "Epoch 53\n",
      "   training loss 0.00018291911692358553, (0.000114680486, 1.7354351, 0.5037112, 1.7867509)\n",
      "   validation loss 0.0001426516246283427, (7.487841e-05, 1.352483, 0.49905702, 1.7867509)\n",
      "decoder loss ratio: 0.000406, decoder SINDy loss  ratio: 0.050427\n",
      "Epoch 54\n",
      "   training loss 0.0001781240716809407, (0.000110946996, 1.6642796, 0.49232495, 1.7944578)\n",
      "   validation loss 0.0001383740600431338, (7.1829374e-05, 1.3016845, 0.48600107, 1.7944578)\n",
      "decoder loss ratio: 0.000389, decoder SINDy loss  ratio: 0.049108\n",
      "Epoch 55\n",
      "   training loss 0.00021486682817339897, (0.00014835467, 1.6264299, 0.48491642, 1.8020521)\n",
      "   validation loss 0.00017490702157374471, (0.000109924265, 1.2660515, 0.46962225, 1.8020521)\n",
      "decoder loss ratio: 0.000595, decoder SINDy loss  ratio: 0.047453\n",
      "Epoch 56\n",
      "   training loss 0.0001833112328313291, (0.0001179051, 1.587813, 0.47321042, 1.80851)\n",
      "   validation loss 0.00014349978300742805, (7.9646845e-05, 1.2221537, 0.45767856, 1.80851)\n",
      "decoder loss ratio: 0.000431, decoder SINDy loss  ratio: 0.046246\n",
      "Epoch 57\n",
      "   training loss 0.0001997575454879552, (0.00013496794, 1.5489478, 0.46641654, 1.8147962)\n",
      "   validation loss 0.0001597164082340896, (9.681946e-05, 1.1873364, 0.4474899, 1.8147962)\n",
      "decoder loss ratio: 0.000524, decoder SINDy loss  ratio: 0.045216\n",
      "Epoch 58\n",
      "   training loss 0.0001618902024347335, (9.853241e-05, 1.4883904, 0.45139766, 1.8218033)\n",
      "   validation loss 0.00012284281547181308, (6.124082e-05, 1.1398878, 0.43383956, 1.8218033)\n",
      "decoder loss ratio: 0.000332, decoder SINDy loss  ratio: 0.043837\n",
      "Epoch 59\n",
      "   training loss 0.0001582291442900896, (9.608951e-05, 1.5725507, 0.43900514, 1.8239115)\n",
      "   validation loss 0.00011919010285055265, (5.983808e-05, 1.1526961, 0.4111291, 1.8239115)\n",
      "decoder loss ratio: 0.000324, decoder SINDy loss  ratio: 0.041542\n",
      "Epoch 60\n",
      "   training loss 0.00015536115097347647, (9.417253e-05, 1.4578177, 0.42904222, 1.8284409)\n",
      "   validation loss 0.00011723765055648983, (5.8288413e-05, 1.0840728, 0.4066483, 1.8284409)\n",
      "decoder loss ratio: 0.000316, decoder SINDy loss  ratio: 0.041089\n",
      "Epoch 61\n",
      "   training loss 0.000159575225552544, (9.960802e-05, 1.4120364, 0.4161976, 1.8347452)\n",
      "   validation loss 0.00012175632582511753, (6.41426e-05, 1.0451276, 0.3926628, 1.8347452)\n",
      "decoder loss ratio: 0.000347, decoder SINDy loss  ratio: 0.039676\n",
      "Epoch 62\n",
      "   training loss 0.00015063566388562322, (9.189294e-05, 1.3812133, 0.40325955, 1.8416765)\n",
      "   validation loss 0.00011288696259725839, (5.6327433e-05, 1.0096275, 0.38142765, 1.8416765)\n",
      "decoder loss ratio: 0.000305, decoder SINDy loss  ratio: 0.038541\n",
      "Epoch 63\n",
      "   training loss 0.00014367126277647913, (8.6153035e-05, 1.3494738, 0.39031756, 1.8486482)\n",
      "   validation loss 0.00010787277278723195, (5.2566014e-05, 0.97897106, 0.36820275, 1.8486482)\n",
      "decoder loss ratio: 0.000285, decoder SINDy loss  ratio: 0.037205\n",
      "Epoch 64\n",
      "   training loss 0.00013944151578471065, (8.3612686e-05, 1.3401732, 0.37272736, 1.855609)\n",
      "   validation loss 0.00010484157246537507, (5.118375e-05, 0.9506866, 0.3510173, 1.855609)\n",
      "decoder loss ratio: 0.000277, decoder SINDy loss  ratio: 0.035468\n",
      "Epoch 65\n",
      "   training loss 0.0001839032775023952, (0.00012711386, 1.3949542, 0.3820345, 1.8585964)\n",
      "   validation loss 0.00014617254782933742, (9.158549e-05, 0.9676478, 0.36001092, 1.8585964)\n",
      "decoder loss ratio: 0.000496, decoder SINDy loss  ratio: 0.036377\n",
      "Epoch 66\n",
      "   training loss 0.00012873388186562806, (7.491649e-05, 1.3503327, 0.3519965, 1.861774)\n",
      "   validation loss 9.6944670076482e-05, (4.5201956e-05, 0.92343944, 0.3312497, 1.861774)\n",
      "decoder loss ratio: 0.000245, decoder SINDy loss  ratio: 0.033471\n",
      "Epoch 67\n",
      "   training loss 0.00012499598960857838, (7.2756564e-05, 1.3167242, 0.33555806, 1.8683628)\n",
      "   validation loss 9.464591857977211e-05, (4.4137407e-05, 0.89065117, 0.31824887, 1.8683628)\n",
      "decoder loss ratio: 0.000239, decoder SINDy loss  ratio: 0.032157\n",
      "Epoch 68\n",
      "   training loss 0.0001209330657729879, (7.026385e-05, 1.2874607, 0.3191991, 1.8749316)\n",
      "   validation loss 9.34652634896338e-05, (4.4129254e-05, 0.86109203, 0.30586702, 1.8749316)\n",
      "decoder loss ratio: 0.000239, decoder SINDy loss  ratio: 0.030906\n",
      "Epoch 69\n",
      "   training loss 0.00011862151586683467, (6.967087e-05, 1.2761952, 0.3014409, 1.880656)\n",
      "   validation loss 9.479243453824893e-05, (4.6874055e-05, 0.84209937, 0.29111817, 1.880656)\n",
      "decoder loss ratio: 0.000254, decoder SINDy loss  ratio: 0.029416\n",
      "Epoch 70\n",
      "   training loss 0.00010897575702983886, (6.053642e-05, 1.4057794, 0.29648668, 1.8790671)\n",
      "   validation loss 8.511020860169083e-05, (3.8121925e-05, 0.8920821, 0.28197613, 1.8790671)\n",
      "decoder loss ratio: 0.000207, decoder SINDy loss  ratio: 0.028492\n",
      "Epoch 71\n",
      "   training loss 0.0001032680447679013, (5.6472985e-05, 1.3176624, 0.27985632, 1.8809428)\n",
      "   validation loss 8.17702675703913e-05, (3.5875295e-05, 0.83355016, 0.2708555, 1.8809428)\n",
      "decoder loss ratio: 0.000194, decoder SINDy loss  ratio: 0.027368\n",
      "Epoch 72\n",
      "   training loss 9.486000635661185e-05, (4.959937e-05, 1.2756604, 0.26404524, 1.8856107)\n",
      "   validation loss 7.5838717748411e-05, (3.0994055e-05, 0.8009621, 0.25988552, 1.8856107)\n",
      "decoder loss ratio: 0.000168, decoder SINDy loss  ratio: 0.026260\n",
      "Epoch 73\n",
      "   training loss 9.549805690767244e-05, (5.1801526e-05, 1.2356142, 0.24792676, 1.8903854)\n",
      "   validation loss 7.910310523584485e-05, (3.564174e-05, 0.774594, 0.24557514, 1.8903854)\n",
      "decoder loss ratio: 0.000193, decoder SINDy loss  ratio: 0.024814\n",
      "Epoch 74\n",
      "   training loss 8.867857104633003e-05, (4.599626e-05, 1.2072487, 0.23736565, 1.8945752)\n",
      "   validation loss 7.52180494600907e-05, (3.246852e-05, 0.7531626, 0.23803777, 1.8945752)\n",
      "decoder loss ratio: 0.000176, decoder SINDy loss  ratio: 0.024052\n",
      "Epoch 75\n",
      "   training loss 8.173286187229678e-05, (3.990953e-05, 1.2413205, 0.22879133, 1.8944201)\n",
      "   validation loss 6.929569644853473e-05, (2.763988e-05, 0.7651144, 0.22711614, 1.8944201)\n",
      "decoder loss ratio: 0.000150, decoder SINDy loss  ratio: 0.022949\n",
      "Epoch 76\n",
      "   training loss 7.380540773738176e-05, (3.348259e-05, 1.1691664, 0.21352291, 1.8970525)\n",
      "   validation loss 6.349411705741659e-05, (2.283193e-05, 0.7234457, 0.21691665, 1.8970525)\n",
      "decoder loss ratio: 0.000124, decoder SINDy loss  ratio: 0.021918\n",
      "Epoch 77\n",
      "   training loss 7.415804429911077e-05, (3.4966688e-05, 1.1175615, 0.20181927, 1.9009426)\n",
      "   validation loss 6.578629836440086e-05, (2.6133108e-05, 0.6945781, 0.20643769, 1.9009426)\n",
      "decoder loss ratio: 0.000142, decoder SINDy loss  ratio: 0.020859\n",
      "Epoch 78\n",
      "   training loss 6.872182711958885e-05, (3.0025301e-05, 1.0993662, 0.1965518, 1.9041345)\n",
      "   validation loss 6.139231845736504e-05, (2.2614173e-05, 0.6830933, 0.19736807, 1.9041345)\n",
      "decoder loss ratio: 0.000123, decoder SINDy loss  ratio: 0.019943\n",
      "Epoch 79\n",
      "   training loss 6.505133933387697e-05, (2.7445458e-05, 1.0284578, 0.18539566, 1.9066317)\n",
      "   validation loss 5.9305853937985376e-05, (2.1254673e-05, 0.6484413, 0.18984863, 1.9066317)\n",
      "decoder loss ratio: 0.000115, decoder SINDy loss  ratio: 0.019183\n",
      "Epoch 80\n",
      "   training loss 8.197419083444402e-05, (4.4538287e-05, 0.97432166, 0.18331149, 1.9104754)\n",
      "   validation loss 7.842294144211337e-05, (4.0423136e-05, 0.62239003, 0.18895054, 1.9104754)\n",
      "decoder loss ratio: 0.000219, decoder SINDy loss  ratio: 0.019092\n",
      "Epoch 81\n",
      "   training loss 5.9121484810020775e-05, (2.2769967e-05, 0.96218723, 0.17228644, 1.9122874)\n",
      "   validation loss 5.4657495638821274e-05, (1.8052135e-05, 0.6131795, 0.17482483, 1.9122874)\n",
      "decoder loss ratio: 0.000098, decoder SINDy loss  ratio: 0.017665\n",
      "Epoch 82\n",
      "   training loss 5.501187843037769e-05, (1.9289604e-05, 0.89817667, 0.16568, 1.9154277)\n",
      "   validation loss 5.144656461197883e-05, (1.5515992e-05, 0.5793491, 0.16776292, 1.9154277)\n",
      "decoder loss ratio: 0.000084, decoder SINDy loss  ratio: 0.016951\n",
      "Epoch 83\n",
      "   training loss 5.3872616263106465e-05, (1.8403167e-05, 0.86985904, 0.16286358, 1.918309)\n",
      "   validation loss 5.056755617260933e-05, (1.5068686e-05, 0.5642835, 0.16315785, 1.918309)\n",
      "decoder loss ratio: 0.000082, decoder SINDy loss  ratio: 0.016486\n",
      "Epoch 84\n",
      "   training loss 6.258454959606752e-05, (2.7326449e-05, 0.8384444, 0.16048801, 1.92093)\n",
      "   validation loss 5.934934597462416e-05, (2.429597e-05, 0.54796916, 0.15844077, 1.92093)\n",
      "decoder loss ratio: 0.000132, decoder SINDy loss  ratio: 0.016010\n",
      "Epoch 85\n",
      "   training loss 5.288639658829197e-05, (1.8258852e-05, 0.78839284, 0.15388118, 1.9239432)\n",
      "   validation loss 5.02767798025161e-05, (1.6011662e-05, 0.52147466, 0.15025692, 1.9239432)\n",
      "decoder loss ratio: 0.000087, decoder SINDy loss  ratio: 0.015183\n",
      "Epoch 86\n",
      "   training loss 4.9630973080638796e-05, (1.5068614e-05, 0.7681067, 0.15293455, 1.9268903)\n",
      "   validation loss 4.713951057055965e-05, (1.3043023e-05, 0.51061726, 0.14827584, 1.9268903)\n",
      "decoder loss ratio: 0.000071, decoder SINDy loss  ratio: 0.014982\n",
      "Epoch 87\n",
      "   training loss 5.0500424549682066e-05, (1.6566384e-05, 0.7264011, 0.146317, 1.9302341)\n",
      "   validation loss 4.855470615439117e-05, (1.5195123e-05, 0.48774844, 0.14057243, 1.9302341)\n",
      "decoder loss ratio: 0.000082, decoder SINDy loss  ratio: 0.014204\n",
      "Epoch 88\n",
      "   training loss 4.7327215725090355e-05, (1.3476738e-05, 0.6978369, 0.14513858, 1.9336619)\n",
      "   validation loss 4.5253116695676e-05, (1.2203357e-05, 0.47394142, 0.13713138, 1.9336619)\n",
      "decoder loss ratio: 0.000066, decoder SINDy loss  ratio: 0.013856\n",
      "Epoch 89\n",
      "   training loss 4.600315151037648e-05, (1.2385957e-05, 0.6706248, 0.14256375, 1.9360822)\n",
      "   validation loss 4.372781768324785e-05, (1.1048616e-05, 0.45554793, 0.1331838, 1.9360822)\n",
      "decoder loss ratio: 0.000060, decoder SINDy loss  ratio: 0.013457\n",
      "Epoch 90\n",
      "   training loss 5.340587813407183e-05, (2.0044792e-05, 0.68264705, 0.13992053, 1.9369031)\n",
      "   validation loss 5.137553671374917e-05, (1.9090903e-05, 0.46323678, 0.12915602, 1.9369031)\n",
      "decoder loss ratio: 0.000103, decoder SINDy loss  ratio: 0.013050\n",
      "Epoch 91\n",
      "   training loss 4.843153874389827e-05, (1.5362746e-05, 0.62463593, 0.13682911, 1.9385886)\n",
      "   validation loss 4.591122706187889e-05, (1.3878081e-05, 0.43072432, 0.12647259, 1.9385886)\n",
      "decoder loss ratio: 0.000075, decoder SINDy loss  ratio: 0.012779\n",
      "Epoch 92\n",
      "   training loss 4.6115543227642775e-05, (1.3462985e-05, 0.59659386, 0.13236669, 1.9415891)\n",
      "   validation loss 4.418865864863619e-05, (1.2557888e-05, 0.41499865, 0.12214878, 1.9415891)\n",
      "decoder loss ratio: 0.000068, decoder SINDy loss  ratio: 0.012342\n",
      "Epoch 93\n",
      "   training loss 5.603753015748225e-05, (2.3417868e-05, 0.5868663, 0.13185948, 1.9433717)\n",
      "   validation loss 5.424790651886724e-05, (2.2863655e-05, 0.4094713, 0.11950537, 1.9433717)\n",
      "decoder loss ratio: 0.000124, decoder SINDy loss  ratio: 0.012075\n",
      "Epoch 94\n",
      "   training loss 4.33986060670577e-05, (1.1272728e-05, 0.56913096, 0.1268012, 1.944576)\n",
      "   validation loss 4.1257568227592856e-05, (1.0334242e-05, 0.39568645, 0.11477566, 1.944576)\n",
      "decoder loss ratio: 0.000056, decoder SINDy loss  ratio: 0.011597\n",
      "Epoch 95\n",
      "   training loss 4.653635551221669e-05, (1.4857247e-05, 0.54293734, 0.12209709, 1.9469403)\n",
      "   validation loss 4.523176176007837e-05, (1.46481725e-05, 0.3808061, 0.11114186, 1.9469403)\n",
      "decoder loss ratio: 0.000079, decoder SINDy loss  ratio: 0.011230\n",
      "Epoch 96\n",
      "   training loss 4.706155596068129e-05, (1.5455009e-05, 0.5239158, 0.1211946, 1.9487087)\n",
      "   validation loss 4.506779805524275e-05, (1.4700269e-05, 0.36832604, 0.1088044, 1.9487087)\n",
      "decoder loss ratio: 0.000080, decoder SINDy loss  ratio: 0.010994\n",
      "Epoch 97\n",
      "   training loss 4.692404763773084e-05, (1.5470388e-05, 0.5422607, 0.1198743, 1.9466231)\n",
      "   validation loss 4.5333115849643946e-05, (1.5162559e-05, 0.37999955, 0.10704329, 1.9466231)\n",
      "decoder loss ratio: 0.000082, decoder SINDy loss  ratio: 0.010816\n",
      "Epoch 98\n",
      "   training loss 4.173353227088228e-05, (1.0665385e-05, 0.5065745, 0.116022974, 1.9465848)\n",
      "   validation loss 3.963339622714557e-05, (9.860152e-06, 0.35749316, 0.10307396, 1.9465848)\n",
      "decoder loss ratio: 0.000053, decoder SINDy loss  ratio: 0.010415\n",
      "Epoch 99\n",
      "   training loss 4.113359682378359e-05, (1.0281767e-05, 0.48288408, 0.11369965, 1.9481864)\n",
      "   validation loss 3.91276043956168e-05, (9.567864e-06, 0.34145716, 0.10077874, 1.9481864)\n",
      "decoder loss ratio: 0.000052, decoder SINDy loss  ratio: 0.010183\n",
      "Epoch 100\n",
      "   training loss 4.0052676922641695e-05, (9.611779e-06, 0.49395967, 0.10974381, 1.9466516)\n",
      "   validation loss 3.8135345675982535e-05, (8.922208e-06, 0.34492242, 0.09746622, 1.9466516)\n",
      "decoder loss ratio: 0.000048, decoder SINDy loss  ratio: 0.009848\n",
      "Epoch 101\n",
      "   training loss 4.018874460598454e-05, (1.0046433e-05, 0.46010375, 0.106726125, 1.9469701)\n",
      "   validation loss 3.8354577554855496e-05, (9.412217e-06, 0.32590082, 0.0947266, 1.9469701)\n",
      "decoder loss ratio: 0.000051, decoder SINDy loss  ratio: 0.009572\n",
      "Epoch 102\n",
      "   training loss 4.936892946716398e-05, (1.9057654e-05, 0.44035253, 0.1082845, 1.9482827)\n",
      "   validation loss 4.775853449245915e-05, (1.8563023e-05, 0.31504193, 0.09712683, 1.9482827)\n",
      "decoder loss ratio: 0.000101, decoder SINDy loss  ratio: 0.009814\n",
      "Epoch 103\n",
      "   training loss 4.739702490041964e-05, (1.759392e-05, 0.44532323, 0.10337463, 1.9465642)\n",
      "   validation loss 4.516993067227304e-05, (1.6732822e-05, 0.3150742, 0.089714654, 1.9465642)\n",
      "decoder loss ratio: 0.000091, decoder SINDy loss  ratio: 0.009065\n",
      "Epoch 104\n",
      "   training loss 4.654350777855143e-05, (1.6849142e-05, 0.42466187, 0.10225922, 1.9468443)\n",
      "   validation loss 4.4693668314721435e-05, (1.6190561e-05, 0.3023451, 0.09034662, 1.9468443)\n",
      "decoder loss ratio: 0.000088, decoder SINDy loss  ratio: 0.009129\n",
      "Epoch 105\n",
      "   training loss 4.2246858356520534e-05, (1.30754415e-05, 0.41397837, 0.0970944, 1.9461979)\n",
      "   validation loss 4.037779581267387e-05, (1.2350674e-05, 0.29443514, 0.08565143, 1.9461979)\n",
      "decoder loss ratio: 0.000067, decoder SINDy loss  ratio: 0.008655\n",
      "Epoch 106\n",
      "   training loss 3.852250665659085e-05, (9.578558e-06, 0.40403214, 0.09491007, 1.945294)\n",
      "   validation loss 3.664244286483154e-05, (8.881951e-06, 0.28467903, 0.08307551, 1.945294)\n",
      "decoder loss ratio: 0.000048, decoder SINDy loss  ratio: 0.008394\n",
      "Epoch 107\n",
      "   training loss 3.9179296436486766e-05, (1.0201367e-05, 0.44859886, 0.09583938, 1.9393994)\n",
      "   validation loss 3.711497993208468e-05, (9.391966e-06, 0.30354923, 0.08329021, 1.9393994)\n",
      "decoder loss ratio: 0.000051, decoder SINDy loss  ratio: 0.008416\n",
      "Epoch 108\n",
      "   training loss 3.9710892451694235e-05, (1.1081353e-05, 0.40354952, 0.09264946, 1.9364594)\n",
      "   validation loss 3.771451520151459e-05, (1.0281853e-05, 0.28429583, 0.080680676, 1.9364594)\n",
      "decoder loss ratio: 0.000056, decoder SINDy loss  ratio: 0.008152\n",
      "Epoch 109\n",
      "   training loss 4.2907056922558695e-05, (1.4422227e-05, 0.38009563, 0.09122506, 1.9362323)\n",
      "   validation loss 4.092729068361223e-05, (1.363195e-05, 0.27177206, 0.07933015, 1.9362323)\n",
      "decoder loss ratio: 0.000074, decoder SINDy loss  ratio: 0.008016\n",
      "Epoch 110\n",
      "   training loss 3.770989860640839e-05, (9.5750365e-06, 0.3716267, 0.08772313, 1.9362549)\n",
      "   validation loss 3.591214044718072e-05, (8.929533e-06, 0.26346827, 0.076200634, 1.9362549)\n",
      "decoder loss ratio: 0.000048, decoder SINDy loss  ratio: 0.007700\n",
      "Epoch 111\n",
      "   training loss 4.097767305211164e-05, (1.3084391e-05, 0.36540267, 0.0853906, 1.9354223)\n",
      "   validation loss 3.921952884411439e-05, (1.2471032e-05, 0.25811762, 0.07394276, 1.9354223)\n",
      "decoder loss ratio: 0.000068, decoder SINDy loss  ratio: 0.007471\n",
      "Epoch 112\n",
      "   training loss 5.321030766936019e-05, (2.5383648e-05, 0.36086264, 0.084886655, 1.9337999)\n",
      "   validation loss 5.109244375489652e-05, (2.4498158e-05, 0.2542008, 0.07256289, 1.9337999)\n",
      "decoder loss ratio: 0.000133, decoder SINDy loss  ratio: 0.007332\n",
      "Epoch 113\n",
      "   training loss 3.5302353353472427e-05, (7.769915e-06, 0.34860578, 0.0820646, 1.9325979)\n",
      "   validation loss 3.350114275235683e-05, (7.133813e-06, 0.24319474, 0.0704135, 1.9325979)\n",
      "decoder loss ratio: 0.000039, decoder SINDy loss  ratio: 0.007115\n",
      "Epoch 114\n",
      "   training loss 3.60169360646978e-05, (8.799254e-06, 0.3531293, 0.07938205, 1.9279476)\n",
      "   validation loss 3.409443161217496e-05, (7.936475e-06, 0.2465094, 0.0687848, 1.9279476)\n",
      "decoder loss ratio: 0.000043, decoder SINDy loss  ratio: 0.006950\n",
      "Epoch 115\n",
      "   training loss 3.9894912333693355e-05, (1.2686963e-05, 0.33471772, 0.07947053, 1.9260895)\n",
      "   validation loss 3.795181692112237e-05, (1.18779e-05, 0.23521045, 0.068130225, 1.9260895)\n",
      "decoder loss ratio: 0.000064, decoder SINDy loss  ratio: 0.006884\n",
      "Epoch 116\n",
      "   training loss 3.73014117940329e-05, (1.0399144e-05, 0.3212348, 0.076484784, 1.9253788)\n",
      "   validation loss 3.550107794580981e-05, (9.722016e-06, 0.22632135, 0.065252736, 1.9253788)\n",
      "decoder loss ratio: 0.000053, decoder SINDy loss  ratio: 0.006593\n",
      "Epoch 117\n",
      "   training loss 4.648053436540067e-05, (1.9801137e-05, 0.3239711, 0.07445435, 1.9233962)\n",
      "   validation loss 4.46270641987212e-05, (1.9053301e-05, 0.22465096, 0.06339804, 1.9233962)\n",
      "decoder loss ratio: 0.000103, decoder SINDy loss  ratio: 0.006406\n",
      "Epoch 118\n",
      "   training loss 3.768758324440569e-05, (1.1030459e-05, 0.3099554, 0.07434731, 1.9222392)\n",
      "   validation loss 3.61327693099156e-05, (1.05465815e-05, 0.2151187, 0.06363798, 1.9222392)\n",
      "decoder loss ratio: 0.000057, decoder SINDy loss  ratio: 0.006430\n",
      "Epoch 119\n",
      "   training loss 4.065873145009391e-05, (1.4250438e-05, 0.29909876, 0.07205929, 1.9202366)\n",
      "   validation loss 3.9024635043460876e-05, (1.3597123e-05, 0.21040589, 0.062251445, 1.9202366)\n",
      "decoder loss ratio: 0.000074, decoder SINDy loss  ratio: 0.006290\n",
      "Epoch 120\n",
      "   training loss 4.60211158497259e-05, (1.9786306e-05, 0.3017803, 0.07065663, 1.9169147)\n",
      "   validation loss 4.4022523070452735e-05, (1.8892533e-05, 0.21018222, 0.059608422, 1.9169147)\n",
      "decoder loss ratio: 0.000102, decoder SINDy loss  ratio: 0.006023\n",
      "Epoch 121\n",
      "   training loss 3.4735585359157994e-05, (8.656303e-06, 0.2923335, 0.069144756, 1.9164807)\n",
      "   validation loss 3.303512494312599e-05, (8.02247e-06, 0.20132916, 0.058478467, 1.9164807)\n",
      "decoder loss ratio: 0.000043, decoder SINDy loss  ratio: 0.005909\n",
      "Epoch 122\n",
      "   training loss 3.3739743230398744e-05, (7.816201e-06, 0.2926338, 0.067800224, 1.9143518)\n",
      "   validation loss 3.201016806997359e-05, (7.1364866e-06, 0.20148325, 0.057301622, 1.9143518)\n",
      "decoder loss ratio: 0.000039, decoder SINDy loss  ratio: 0.005790\n",
      "Epoch 123\n",
      "   training loss 3.5607263271231204e-05, (9.933759e-06, 0.2810943, 0.065386474, 1.9134856)\n",
      "   validation loss 3.3956624974962324e-05, (9.314866e-06, 0.19361657, 0.05506903, 1.9134856)\n",
      "decoder loss ratio: 0.000050, decoder SINDy loss  ratio: 0.005564\n",
      "Epoch 124\n",
      "   training loss 3.281606768723577e-05, (7.115355e-06, 0.2788007, 0.06584966, 1.9115748)\n",
      "   validation loss 3.114396531600505e-05, (6.535969e-06, 0.18948495, 0.054922484, 1.9115748)\n",
      "decoder loss ratio: 0.000035, decoder SINDy loss  ratio: 0.005550\n",
      "Epoch 125\n",
      "   training loss 3.3099480788223445e-05, (7.58556e-06, 0.27028683, 0.064070284, 1.9106891)\n",
      "   validation loss 3.132106940029189e-05, (6.8473255e-06, 0.18421988, 0.053668536, 1.9106891)\n",
      "decoder loss ratio: 0.000037, decoder SINDy loss  ratio: 0.005423\n",
      "Epoch 126\n",
      "   training loss 3.246268897783011e-05, (7.1158634e-06, 0.28169927, 0.062849775, 1.9061848)\n",
      "   validation loss 3.068979276577011e-05, (6.3854723e-06, 0.18930097, 0.052424744, 1.9061848)\n",
      "decoder loss ratio: 0.000035, decoder SINDy loss  ratio: 0.005297\n",
      "Epoch 127\n",
      "   training loss 3.951603139284998e-05, (1.4196209e-05, 0.26333472, 0.0627065, 1.9049171)\n",
      "   validation loss 3.776681114686653e-05, (1.3500142e-05, 0.17976373, 0.05217501, 1.9049171)\n",
      "decoder loss ratio: 0.000073, decoder SINDy loss  ratio: 0.005272\n",
      "Epoch 128\n",
      "   training loss 3.22243467962835e-05, (7.0425394e-06, 0.2613988, 0.06147041, 1.9034766)\n",
      "   validation loss 3.0647588573629037e-05, (6.532924e-06, 0.17590563, 0.050798994, 1.9034766)\n",
      "decoder loss ratio: 0.000035, decoder SINDy loss  ratio: 0.005133\n",
      "Epoch 129\n",
      "   training loss 3.295335409347899e-05, (7.919042e-06, 0.26099095, 0.060201705, 1.9014143)\n",
      "   validation loss 3.127135641989298e-05, (7.2739003e-06, 0.17417054, 0.049833138, 1.9014143)\n",
      "decoder loss ratio: 0.000039, decoder SINDy loss  ratio: 0.005035\n",
      "Epoch 130\n",
      "   training loss 3.413057856960222e-05, (9.196705e-06, 0.24730639, 0.059272334, 1.9006642)\n",
      "   validation loss 3.2472016755491495e-05, (8.534728e-06, 0.16625953, 0.04930647, 1.9006642)\n",
      "decoder loss ratio: 0.000046, decoder SINDy loss  ratio: 0.004982\n",
      "Epoch 131\n",
      "   training loss 3.6221892514731735e-05, (1.1382118e-05, 0.24846806, 0.058602944, 1.8979479)\n",
      "   validation loss 3.434334939811379e-05, (1.0618977e-05, 0.16623871, 0.047448944, 1.8979479)\n",
      "decoder loss ratio: 0.000058, decoder SINDy loss  ratio: 0.004794\n",
      "Epoch 132\n",
      "   training loss 3.798246325459331e-05, (1.330511e-05, 0.23850279, 0.05708798, 1.8968555)\n",
      "   validation loss 3.6592286051018164e-05, (1.2874337e-05, 0.15993993, 0.047493953, 1.8968555)\n",
      "decoder loss ratio: 0.000070, decoder SINDy loss  ratio: 0.004799\n",
      "Epoch 133\n",
      "   training loss 3.340107650728896e-05, (8.781209e-06, 0.25124475, 0.05702368, 1.8917499)\n",
      "   validation loss 3.172098149661906e-05, (8.143083e-06, 0.16284232, 0.04660401, 1.8917499)\n",
      "decoder loss ratio: 0.000044, decoder SINDy loss  ratio: 0.004709\n",
      "Epoch 134\n",
      "   training loss 3.537740121828392e-05, (1.082689e-05, 0.23350996, 0.05653029, 1.8897481)\n",
      "   validation loss 3.35486329277046e-05, (1.0076856e-05, 0.15496102, 0.04574299, 1.8897481)\n",
      "decoder loss ratio: 0.000055, decoder SINDy loss  ratio: 0.004622\n",
      "Epoch 135\n",
      "   training loss 3.1823110475670546e-05, (7.460728e-06, 0.22731353, 0.054706167, 1.8891767)\n",
      "   validation loss 3.0147653887979686e-05, (6.8054333e-06, 0.14939506, 0.044504534, 1.8891767)\n",
      "decoder loss ratio: 0.000037, decoder SINDy loss  ratio: 0.004497\n",
      "Epoch 136\n",
      "   training loss 3.333854692755267e-05, (9.0466265e-06, 0.22612655, 0.05429698, 1.8862221)\n",
      "   validation loss 3.144262154819444e-05, (8.231703e-06, 0.15028086, 0.043486968, 1.8862221)\n",
      "decoder loss ratio: 0.000045, decoder SINDy loss  ratio: 0.004394\n",
      "Epoch 137\n",
      "   training loss 3.797230601776391e-05, (1.3788604e-05, 0.21984744, 0.05332124, 1.8851577)\n",
      "   validation loss 3.6441400879994035e-05, (1.3228487e-05, 0.14498654, 0.04361337, 1.8851577)\n",
      "decoder loss ratio: 0.000072, decoder SINDy loss  ratio: 0.004407\n",
      "Epoch 138\n",
      "   training loss 4.544759576674551e-05, (2.1373435e-05, 0.2153907, 0.052533623, 1.8820801)\n",
      "   validation loss 4.364487904240377e-05, (2.0567342e-05, 0.14294106, 0.042567372, 1.8820801)\n",
      "decoder loss ratio: 0.000111, decoder SINDy loss  ratio: 0.004301\n",
      "Epoch 139\n",
      "   training loss 3.079463931499049e-05, (6.84699e-06, 0.21924472, 0.051539883, 1.8793662)\n",
      "   validation loss 2.938114630524069e-05, (6.50002e-06, 0.13930579, 0.040874656, 1.8793662)\n",
      "decoder loss ratio: 0.000035, decoder SINDy loss  ratio: 0.004130\n",
      "Epoch 140\n",
      "   training loss 3.447894414421171e-05, (1.0576652e-05, 0.2079102, 0.051232785, 1.8779017)\n",
      "   validation loss 3.2953212212305516e-05, (1.0034338e-05, 0.13472107, 0.04139861, 1.8779017)\n",
      "decoder loss ratio: 0.000054, decoder SINDy loss  ratio: 0.004183\n",
      "Epoch 141\n",
      "   training loss 3.364351869095117e-05, (9.915774e-06, 0.20624067, 0.04967376, 1.8760371)\n",
      "   validation loss 3.2004179956857115e-05, (9.255217e-06, 0.1324304, 0.039885934, 1.8760371)\n",
      "decoder loss ratio: 0.000050, decoder SINDy loss  ratio: 0.004030\n",
      "Epoch 142\n",
      "   training loss 3.426926559768617e-05, (1.062553e-05, 0.20948692, 0.0493504, 1.8708698)\n",
      "   validation loss 3.2550771720707417e-05, (9.926862e-06, 0.1345031, 0.039152134, 1.8708698)\n",
      "decoder loss ratio: 0.000054, decoder SINDy loss  ratio: 0.003956\n",
      "Epoch 143\n",
      "   training loss 3.700530942296609e-05, (1.3424796e-05, 0.20048688, 0.048922166, 1.8688297)\n",
      "   validation loss 3.550710607669316e-05, (1.2926067e-05, 0.12909555, 0.038927417, 1.8688297)\n",
      "decoder loss ratio: 0.000070, decoder SINDy loss  ratio: 0.003933\n",
      "Epoch 144\n",
      "   training loss 2.9838054615538567e-05, (6.289534e-06, 0.21679915, 0.04897579, 1.8650941)\n",
      "   validation loss 2.84241159533849e-05, (5.903019e-06, 0.13373713, 0.03870157, 1.8650941)\n",
      "decoder loss ratio: 0.000032, decoder SINDy loss  ratio: 0.003911\n",
      "Epoch 145\n",
      "   training loss 3.073101834161207e-05, (7.341853e-06, 0.1972316, 0.047724545, 1.861671)\n",
      "   validation loss 2.9192286092438735e-05, (6.834717e-06, 0.12526573, 0.0374086, 1.861671)\n",
      "decoder loss ratio: 0.000037, decoder SINDy loss  ratio: 0.003780\n",
      "Epoch 146\n",
      "   training loss 3.847335756290704e-05, (1.520528e-05, 0.19251858, 0.04663644, 1.8604434)\n",
      "   validation loss 3.680320514831692e-05, (1.4576605e-05, 0.121868104, 0.036221668, 1.8604434)\n",
      "decoder loss ratio: 0.000079, decoder SINDy loss  ratio: 0.003660\n",
      "Epoch 147\n",
      "   training loss 3.6800032830797136e-05, (1.3495148e-05, 0.19040632, 0.04723826, 1.8581059)\n",
      "   validation loss 3.4824686736101285e-05, (1.2579423e-05, 0.121648684, 0.036642056, 1.8581059)\n",
      "decoder loss ratio: 0.000068, decoder SINDy loss  ratio: 0.003702\n",
      "Epoch 148\n",
      "   training loss 3.688915239763446e-05, (1.3722514e-05, 0.1844184, 0.046051696, 1.8561469)\n",
      "   validation loss 3.5295350244268775e-05, (1.3128786e-05, 0.11753051, 0.036050986, 1.8561469)\n",
      "decoder loss ratio: 0.000071, decoder SINDy loss  ratio: 0.003643\n",
      "Epoch 149\n",
      "   training loss 5.718358443118632e-05, (3.4024073e-05, 0.18522523, 0.046233386, 1.8536173)\n",
      "   validation loss 5.498142127180472e-05, (3.29071e-05, 0.117523536, 0.035381507, 1.8536173)\n",
      "decoder loss ratio: 0.000178, decoder SINDy loss  ratio: 0.003575\n",
      "Epoch 150\n",
      "   training loss 3.555280272848904e-05, (1.2584751e-05, 0.17451674, 0.044547692, 1.8513284)\n",
      "   validation loss 3.398132685106248e-05, (1.1989194e-05, 0.110284775, 0.03478851, 1.8513284)\n",
      "decoder loss ratio: 0.000065, decoder SINDy loss  ratio: 0.003515\n",
      "Epoch 151\n",
      "   training loss 3.676591222756542e-05, (1.3831629e-05, 0.17221808, 0.044496693, 1.8484614)\n",
      "   validation loss 3.527100489009172e-05, (1.3289163e-05, 0.10949638, 0.034972306, 1.8484614)\n",
      "decoder loss ratio: 0.000072, decoder SINDy loss  ratio: 0.003534\n",
      "Epoch 152\n",
      "   training loss 2.9319035093067214e-05, (6.4891187e-06, 0.17235123, 0.04379996, 1.844992)\n",
      "   validation loss 2.7812537155114114e-05, (5.997593e-06, 0.10686946, 0.033650246, 1.844992)\n",
      "decoder loss ratio: 0.000032, decoder SINDy loss  ratio: 0.003400\n",
      "Epoch 153\n",
      "   training loss 3.599045885493979e-05, (1.3286359e-05, 0.16676553, 0.042830124, 1.842109)\n",
      "   validation loss 3.4624667023308575e-05, (1.2909004e-05, 0.10429502, 0.03294572, 1.842109)\n",
      "decoder loss ratio: 0.000070, decoder SINDy loss  ratio: 0.003329\n",
      "Epoch 154\n",
      "   training loss 2.8454811399569735e-05, (5.7778634e-06, 0.16639133, 0.042862423, 1.8390706)\n",
      "   validation loss 2.7247042453382164e-05, (5.5820547e-06, 0.101769246, 0.032742817, 1.8390706)\n",
      "decoder loss ratio: 0.000030, decoder SINDy loss  ratio: 0.003308\n",
      "Epoch 155\n",
      "   training loss 3.167943577864207e-05, (9.146461e-06, 0.16388941, 0.041608937, 1.8372082)\n",
      "   validation loss 3.057440699194558e-05, (9.048026e-06, 0.10026498, 0.031542998, 1.8372082)\n",
      "decoder loss ratio: 0.000049, decoder SINDy loss  ratio: 0.003187\n",
      "Epoch 156\n",
      "   training loss 2.77496892522322e-05, (5.274189e-06, 0.17675506, 0.041574627, 1.8318038)\n",
      "   validation loss 2.6446843548910692e-05, (4.9572645e-06, 0.10195512, 0.031715408, 1.8318038)\n",
      "decoder loss ratio: 0.000027, decoder SINDy loss  ratio: 0.003205\n",
      "Epoch 157\n",
      "   training loss 3.218699566787109e-05, (9.759702e-06, 0.15873766, 0.041468997, 1.8280394)\n",
      "   validation loss 3.059549635509029e-05, (9.17277e-06, 0.097884305, 0.03142332, 1.8280394)\n",
      "decoder loss ratio: 0.000050, decoder SINDy loss  ratio: 0.003175\n",
      "Epoch 158\n",
      "   training loss 3.0158676963765174e-05, (7.815279e-06, 0.15516262, 0.040818967, 1.8261503)\n",
      "   validation loss 2.882250919356011e-05, (7.4790696e-06, 0.09503083, 0.030819364, 1.8261503)\n",
      "decoder loss ratio: 0.000041, decoder SINDy loss  ratio: 0.003114\n",
      "Epoch 159\n",
      "   training loss 4.3585034291027114e-05, (2.1143966e-05, 0.15888655, 0.042128365, 1.8228234)\n",
      "   validation loss 4.155826900387183e-05, (2.0175781e-05, 0.09624796, 0.031542543, 1.8228234)\n",
      "decoder loss ratio: 0.000109, decoder SINDy loss  ratio: 0.003187\n",
      "Epoch 160\n",
      "   training loss 3.47794994013384e-05, (1.2596839e-05, 0.14940357, 0.0397461, 1.8208051)\n",
      "   validation loss 3.32794661517255e-05, (1.2091595e-05, 0.091113694, 0.029798234, 1.8208051)\n",
      "decoder loss ratio: 0.000066, decoder SINDy loss  ratio: 0.003011\n",
      "Epoch 161\n",
      "   training loss 3.0346000130521134e-05, (8.152974e-06, 0.15433718, 0.04028686, 1.816434)\n",
      "   validation loss 2.9145869120839052e-05, (7.935333e-06, 0.09201091, 0.030461973, 1.816434)\n",
      "decoder loss ratio: 0.000043, decoder SINDy loss  ratio: 0.003078\n",
      "Epoch 162\n",
      "   training loss 3.522036786307581e-05, (1.3104712e-05, 0.14406288, 0.039822057, 1.8133451)\n",
      "   validation loss 3.35137483489234e-05, (1.2417202e-05, 0.08893868, 0.029630952, 1.8133451)\n",
      "decoder loss ratio: 0.000067, decoder SINDy loss  ratio: 0.002994\n",
      "Epoch 163\n",
      "   training loss 3.6839937820332125e-05, (1.480398e-05, 0.14210711, 0.03920786, 1.8115172)\n",
      "   validation loss 3.570111948647536e-05, (1.4673541e-05, 0.086599395, 0.029124066, 1.8115172)\n",
      "decoder loss ratio: 0.000079, decoder SINDy loss  ratio: 0.002943\n",
      "Epoch 164\n",
      "   training loss 3.6013167118653655e-05, (1.4112341e-05, 0.14351024, 0.038239628, 1.8076866)\n",
      "   validation loss 3.465516783762723e-05, (1.3791288e-05, 0.086120285, 0.02787016, 1.8076866)\n",
      "decoder loss ratio: 0.000075, decoder SINDy loss  ratio: 0.002816\n",
      "Epoch 165\n",
      "   training loss 3.864329482894391e-05, (1.6731192e-05, 0.13761102, 0.038584508, 1.8053651)\n",
      "   validation loss 3.736810322152451e-05, (1.6460492e-05, 0.083599135, 0.028539605, 1.8053651)\n",
      "decoder loss ratio: 0.000089, decoder SINDy loss  ratio: 0.002884\n",
      "Epoch 166\n",
      "   training loss 2.9256898415042087e-05, (7.4243108e-06, 0.13666482, 0.038055595, 1.8027028)\n",
      "   validation loss 2.8029446184518747e-05, (7.215968e-06, 0.081895456, 0.027864518, 1.8027028)\n",
      "decoder loss ratio: 0.000039, decoder SINDy loss  ratio: 0.002816\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_32028\\4054194948.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mresults_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mresults_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ianma\\Desktop\\Study\\Python\\CS4240_DeepLearning_Group41_ReproducibilityProject\\SindyAutoencoders-master\\src\\training.py\u001b[0m in \u001b[0;36mtrain_network\u001b[1;34m(training_data, val_data, params)\u001b[0m\n\u001b[0;32m     31\u001b[0m                 \u001b[0mbatch_idxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'batch_size'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'batch_size'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m                 \u001b[0mtrain_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_feed_dictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midxs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_idxs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m                 \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'print_progress'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'print_frequency'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ianma\\anaconda3\\envs\\tfv1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 956\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    957\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ianma\\anaconda3\\envs\\tfv1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1180\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1181\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ianma\\anaconda3\\envs\\tfv1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1359\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ianma\\anaconda3\\envs\\tfv1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1363\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ianma\\anaconda3\\envs\\tfv1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1350\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ianma\\anaconda3\\envs\\tfv1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1443\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_experiments = 1\n",
    "df = pd.DataFrame()\n",
    "for i in range(num_experiments):\n",
    "    print('EXPERIMENT %d' % i)\n",
    "\n",
    "    params['coefficient_mask'] = np.ones((params['library_dim'], params['latent_dim']))\n",
    "\n",
    "    params['save_name'] = 'lorenz_' + datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S_%f\")\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    results_dict = train_network(training_data, validation_data, params)\n",
    "    df = df.append({**results_dict, **params}, ignore_index=True)\n",
    "\n",
    "df.to_pickle('experiment_results_' + datetime.datetime.now().strftime(\"%Y%m%d%H%M\") + '.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
