{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../src\")\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from example_lorenz import get_lorenz_data\n",
    "from sindy_utils import library_size\n",
    "from training import train_network\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate training, validation, testing data\n",
    "noise_strength = 1e-6\n",
    "training_data = get_lorenz_data(1024, noise_strength=noise_strength)\n",
    "validation_data = get_lorenz_data(20, noise_strength=noise_strength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up model and training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256000\n"
     ]
    }
   ],
   "source": [
    "params = {}\n",
    "\n",
    "params['input_dim'] = 128\n",
    "params['latent_dim'] = 3\n",
    "params['model_order'] = 1\n",
    "params['poly_order'] = 3\n",
    "params['include_sine'] = False\n",
    "params['library_dim'] = library_size(params['latent_dim'], params['poly_order'], params['include_sine'], True)\n",
    "\n",
    "# sequential thresholding parameters\n",
    "params['sequential_thresholding'] = True\n",
    "params['coefficient_threshold'] = 0.1\n",
    "params['threshold_frequency'] = 500\n",
    "params['coefficient_mask'] = np.ones((params['library_dim'], params['latent_dim']))\n",
    "params['coefficient_initialization'] = 'constant'\n",
    "\n",
    "# loss function weighting\n",
    "params['loss_weight_decoder'] = 1.0\n",
    "params['loss_weight_sindy_z'] = 0.0\n",
    "params['loss_weight_sindy_x'] = 1e-4\n",
    "params['loss_weight_sindy_regularization'] = 1e-5\n",
    "\n",
    "params['activation'] = 'sigmoid'\n",
    "params['widths'] = [64,32]\n",
    "\n",
    "# training parameters\n",
    "params['epoch_size'] = training_data['x'].shape[0]\n",
    "print(training_data['x'].shape[0])\n",
    "params['batch_size'] = 1024\n",
    "params['learning_rate'] = 1e-3\n",
    "\n",
    "params['data_path'] = os.getcwd() + '/'\n",
    "params['print_progress'] = True\n",
    "params['print_frequency'] = 1\n",
    "\n",
    "# training time cutoffs\n",
    "params['max_epochs'] = 5001\n",
    "params['refinement_epochs'] = 1001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT 0\n",
      "TRAINING\n",
      "Epoch 0\n",
      "   training loss 0.055861566215753555, (0.054228835, 1598.4082, 16.254604, 0.7272231)\n",
      "   validation loss 0.04030901566147804, (0.038666036, 1803.2072, 16.357077, 0.7272231)\n",
      "decoder loss ratio: 0.202899, decoder SINDy loss  ratio: 1.244901\n",
      "Epoch 1\n",
      "   training loss 0.04706043750047684, (0.045581795, 189.15039, 14.728531, 0.5788514)\n",
      "   validation loss 0.032366104423999786, (0.030966012, 202.4324, 13.943036, 0.5788514)\n",
      "decoder loss ratio: 0.162493, decoder SINDy loss  ratio: 1.061174\n",
      "Epoch 2\n",
      "   training loss 0.026412704959511757, (0.02503476, 139.51094, 13.730327, 0.49129364)\n",
      "   validation loss 0.0190883819013834, (0.017741453, 141.95781, 13.42015, 0.49129364)\n",
      "decoder loss ratio: 0.093098, decoder SINDy loss  ratio: 1.021378\n",
      "Epoch 3\n",
      "   training loss 0.01424498576670885, (0.013033007, 80.96227, 12.072458, 0.47325712)\n",
      "   validation loss 0.012188449501991272, (0.011020565, 80.18498, 11.631509, 0.47325712)\n",
      "decoder loss ratio: 0.057830, decoder SINDy loss  ratio: 0.885249\n",
      "Epoch 4\n",
      "   training loss 0.011244961991906166, (0.010157105, 67.50332, 10.825941, 0.5263877)\n",
      "   validation loss 0.010406680405139923, (0.009373924, 61.41828, 10.274925, 0.5263877)\n",
      "decoder loss ratio: 0.049189, decoder SINDy loss  ratio: 0.782002\n",
      "Epoch 5\n",
      "   training loss 0.009707671590149403, (0.008773097, 58.84267, 9.287208, 0.58538324)\n",
      "   validation loss 0.009311478585004807, (0.008410278, 41.568733, 8.953464, 0.58538324)\n",
      "decoder loss ratio: 0.044133, decoder SINDy loss  ratio: 0.681428\n",
      "Epoch 6\n",
      "   training loss 0.008475027047097683, (0.0077010808, 50.745533, 7.677371, 0.62093556)\n",
      "   validation loss 0.008395658805966377, (0.0076176696, 32.917908, 7.717798, 0.62093556)\n",
      "decoder loss ratio: 0.039974, decoder SINDy loss  ratio: 0.587385\n",
      "Epoch 7\n",
      "   training loss 0.007603723555803299, (0.0069107898, 52.106762, 6.8656144, 0.63727456)\n",
      "   validation loss 0.00772625207901001, (0.0070165987, 32.6852, 7.0328064, 0.63727456)\n",
      "decoder loss ratio: 0.036819, decoder SINDy loss  ratio: 0.535251\n",
      "Epoch 8\n",
      "   training loss 0.0066972766071558, (0.006055532, 45.739746, 6.352257, 0.6518573)\n",
      "   validation loss 0.0069631608203053474, (0.006277457, 31.986116, 6.79185, 0.6518573)\n",
      "decoder loss ratio: 0.032941, decoder SINDy loss  ratio: 0.516913\n",
      "Epoch 9\n",
      "   training loss 0.003978463355451822, (0.0033219024, 40.930805, 6.4981985, 0.67411965)\n",
      "   validation loss 0.004545548930764198, (0.0038250987, 37.121372, 7.137087, 0.67411965)\n",
      "decoder loss ratio: 0.020072, decoder SINDy loss  ratio: 0.543188\n",
      "Epoch 10\n",
      "   training loss 0.002566675189882517, (0.0020013116, 32.875935, 5.5846014, 0.6903396)\n",
      "   validation loss 0.003281201934441924, (0.002606978, 31.808992, 6.6732063, 0.6903396)\n",
      "decoder loss ratio: 0.013680, decoder SINDy loss  ratio: 0.507883\n",
      "Epoch 11\n",
      "   training loss 0.0019361600279808044, (0.001426277, 27.188217, 5.0280495, 0.7078044)\n",
      "   validation loss 0.002663522958755493, (0.002016293, 26.459745, 6.40152, 0.7078044)\n",
      "decoder loss ratio: 0.010580, decoder SINDy loss  ratio: 0.487206\n",
      "Epoch 12\n",
      "   training loss 0.0016167514258995652, (0.0011465009, 22.002447, 4.6296587, 0.7284668)\n",
      "   validation loss 0.00232673273421824, (0.0017059324, 23.476603, 6.135156, 0.7284668)\n",
      "decoder loss ratio: 0.008952, decoder SINDy loss  ratio: 0.466933\n",
      "Epoch 13\n",
      "   training loss 0.0014227773062884808, (0.000983886, 18.693232, 4.312819, 0.7609459)\n",
      "   validation loss 0.0020876803901046515, (0.0014941045, 20.972305, 5.859665, 0.7609459)\n",
      "decoder loss ratio: 0.007840, decoder SINDy loss  ratio: 0.445966\n",
      "Epoch 14\n",
      "   training loss 0.0013105907710269094, (0.000901541, 16.427965, 4.01077, 0.7972792)\n",
      "   validation loss 0.00194061198271811, (0.0013766048, 18.797234, 5.5603447, 0.7972792)\n",
      "decoder loss ratio: 0.007224, decoder SINDy loss  ratio: 0.423186\n",
      "Epoch 15\n",
      "   training loss 0.0012045566691085696, (0.0008233192, 14.320709, 3.7286334, 0.83740485)\n",
      "   validation loss 0.0018020345596596599, (0.0012657302, 16.95087, 5.2793016, 0.83740485)\n",
      "decoder loss ratio: 0.006642, decoder SINDy loss  ratio: 0.401796\n",
      "Epoch 16\n",
      "   training loss 0.0011458639055490494, (0.0007878548, 12.6013565, 3.4921036, 0.87988347)\n",
      "   validation loss 0.0017162672011181712, (0.0012062578, 15.177502, 5.012105, 0.87988347)\n",
      "decoder loss ratio: 0.006330, decoder SINDy loss  ratio: 0.381460\n",
      "Epoch 17\n",
      "   training loss 0.0010820056777447462, (0.00074362464, 11.059143, 3.2917783, 0.9203205)\n",
      "   validation loss 0.001625234610401094, (0.0011387606, 13.504017, 4.7727084, 0.9203205)\n",
      "decoder loss ratio: 0.005976, decoder SINDy loss  ratio: 0.363240\n",
      "Epoch 18\n",
      "   training loss 0.0010306613985449076, (0.00070963963, 9.640836, 3.1143632, 0.95854163)\n",
      "   validation loss 0.0015517451101914048, (0.0010860487, 12.078977, 4.5611105, 0.95854163)\n",
      "decoder loss ratio: 0.005699, decoder SINDy loss  ratio: 0.347136\n",
      "Epoch 19\n",
      "   training loss 0.0009708987199701369, (0.00066616095, 8.477199, 2.9478884, 0.9948919)\n",
      "   validation loss 0.0014660084852948785, (0.0010190809, 10.995838, 4.3697863, 0.9948919)\n",
      "decoder loss ratio: 0.005348, decoder SINDy loss  ratio: 0.332575\n",
      "Epoch 20\n",
      "   training loss 0.0009178135078400373, (0.00062815665, 7.537761, 2.7936053, 1.029634)\n",
      "   validation loss 0.0013893918367102742, (0.0009591817, 10.100472, 4.1991377, 1.029634)\n",
      "decoder loss ratio: 0.005033, decoder SINDy loss  ratio: 0.319587\n",
      "Epoch 21\n",
      "   training loss 0.0008753843139857054, (0.0006007652, 6.7603106, 2.639769, 1.0642238)\n",
      "   validation loss 0.0013317186385393143, (0.00091773126, 9.280998, 4.0334516, 1.0642238)\n",
      "decoder loss ratio: 0.004816, decoder SINDy loss  ratio: 0.306977\n",
      "Epoch 22\n",
      "   training loss 0.0008464220445603132, (0.0005832848, 6.180163, 2.5214343, 1.099382)\n",
      "   validation loss 0.0012855095556005836, (0.00088546047, 8.602817, 3.8905532, 1.099382)\n",
      "decoder loss ratio: 0.004646, decoder SINDy loss  ratio: 0.296101\n",
      "Epoch 23\n",
      "   training loss 0.0008146482286974788, (0.0005623956, 5.7262073, 2.4091287, 1.133979)\n",
      "   validation loss 0.001241203397512436, (0.0008549021, 8.1044855, 3.749616, 1.133979)\n",
      "decoder loss ratio: 0.004486, decoder SINDy loss  ratio: 0.285375\n",
      "Epoch 24\n",
      "   training loss 0.000774313579313457, (0.0005338911, 5.3503594, 2.2874517, 1.1677337)\n",
      "   validation loss 0.0011912576155737042, (0.00081894256, 7.7447686, 3.6063766, 1.1677337)\n",
      "decoder loss ratio: 0.004297, decoder SINDy loss  ratio: 0.274473\n",
      "Epoch 25\n",
      "   training loss 0.0007476415485143661, (0.0005176325, 5.054761, 2.1798308, 1.2025933)\n",
      "   validation loss 0.0011523024877533317, (0.0007932189, 7.485827, 3.4705763, 1.2025933)\n",
      "decoder loss ratio: 0.004162, decoder SINDy loss  ratio: 0.264138\n",
      "Epoch 26\n",
      "   training loss 0.000705527956597507, (0.000486621, 4.77882, 2.0654907, 1.2357901)\n",
      "   validation loss 0.0010964099783450365, (0.0007504928, 7.4125, 3.3355923, 1.2357901)\n",
      "decoder loss ratio: 0.003938, decoder SINDy loss  ratio: 0.253865\n",
      "Epoch 27\n",
      "   training loss 0.0006705195410177112, (0.0004626981, 4.490978, 1.9514166, 1.2679753)\n",
      "   validation loss 0.00104592798743397, (0.0007132389, 7.297218, 3.200094, 1.2679753)\n",
      "decoder loss ratio: 0.003743, decoder SINDy loss  ratio: 0.243552\n",
      "Epoch 28\n",
      "   training loss 0.0006368636386469007, (0.0004415213, 4.206837, 1.8235538, 1.2986946)\n",
      "   validation loss 0.0009932597167789936, (0.000674956, 7.2417545, 3.0531678, 1.2986946)\n",
      "decoder loss ratio: 0.003542, decoder SINDy loss  ratio: 0.232370\n",
      "Epoch 29\n",
      "   training loss 0.0006016569677740335, (0.00041928206, 3.9347677, 1.6909859, 1.3276356)\n",
      "   validation loss 0.0009370428742840886, (0.0006315823, 7.3400636, 2.9218426, 1.3276356)\n",
      "decoder loss ratio: 0.003314, decoder SINDy loss  ratio: 0.222375\n",
      "Epoch 30\n",
      "   training loss 0.0005540834390558302, (0.00038417644, 3.6573284, 1.5634931, 1.3557665)\n",
      "   validation loss 0.0008675417047925293, (0.0005761076, 7.231927, 2.7787647, 1.3557665)\n",
      "decoder loss ratio: 0.003023, decoder SINDy loss  ratio: 0.211486\n",
      "Epoch 31\n",
      "   training loss 0.0005140972207300365, (0.00035723764, 3.4540498, 1.4301999, 1.3839608)\n",
      "   validation loss 0.0008047932060435414, (0.00052700413, 7.1708603, 2.6394944, 1.3839608)\n",
      "decoder loss ratio: 0.002765, decoder SINDy loss  ratio: 0.200886\n",
      "Epoch 32\n",
      "   training loss 0.0004801964678335935, (0.0003368519, 3.2613544, 1.2923023, 1.4114337)\n",
      "   validation loss 0.0007469090633094311, (0.0004825442, 7.081908, 2.5025058, 1.4114337)\n",
      "decoder loss ratio: 0.002532, decoder SINDy loss  ratio: 0.190460\n",
      "Epoch 33\n",
      "   training loss 0.0004792647378053516, (0.00034861016, 3.1279523, 1.1625477, 1.4399791)\n",
      "   validation loss 0.0007174730999395251, (0.00046593946, 7.0248604, 2.3713384, 1.4399791)\n",
      "decoder loss ratio: 0.002445, decoder SINDy loss  ratio: 0.180477\n",
      "Epoch 34\n",
      "   training loss 0.0004543144314084202, (0.00033446698, 2.9962661, 1.0517291, 1.4674522)\n",
      "   validation loss 0.0006728071020916104, (0.00043316957, 6.9361587, 2.2496307, 1.4674522)\n",
      "decoder loss ratio: 0.002273, decoder SINDy loss  ratio: 0.171214\n",
      "Epoch 35\n",
      "   training loss 0.00040006553172133863, (0.00028987636, 2.8665974, 0.95238745, 1.4950445)\n",
      "   validation loss 0.0006023364840075374, (0.00037462197, 6.882816, 2.1276405, 1.4950445)\n",
      "decoder loss ratio: 0.001966, decoder SINDy loss  ratio: 0.161930\n",
      "Epoch 36\n",
      "   training loss 0.0003926226927433163, (0.00029095018, 2.7526467, 0.8645389, 1.5218613)\n",
      "   validation loss 0.000574242090806365, (0.00035744553, 6.8171144, 2.015779, 1.5218613)\n",
      "decoder loss ratio: 0.001876, decoder SINDy loss  ratio: 0.153417\n",
      "Epoch 37\n",
      "   training loss 0.0003533725393936038, (0.00025799032, 2.6435235, 0.7990992, 1.5472289)\n",
      "   validation loss 0.000523061549756676, (0.0003159588, 6.73547, 1.9163043, 1.5472289)\n",
      "decoder loss ratio: 0.001658, decoder SINDy loss  ratio: 0.145846\n",
      "Epoch 38\n",
      "   training loss 0.00032094286871142685, (0.000228266, 2.552664, 0.769646, 1.5712245)\n",
      "   validation loss 0.0004860330082010478, (0.00028647284, 6.5070343, 1.8384789, 1.5712245)\n",
      "decoder loss ratio: 0.001503, decoder SINDy loss  ratio: 0.139923\n",
      "Epoch 39\n",
      "   training loss 0.0002959828998427838, (0.0002092048, 2.4566839, 0.7084118, 1.5936936)\n",
      "   validation loss 0.000445791520178318, (0.0002556929, 6.469958, 1.7416168, 1.5936936)\n",
      "decoder loss ratio: 0.001342, decoder SINDy loss  ratio: 0.132551\n",
      "Epoch 40\n",
      "   training loss 0.00028637656942009926, (0.00020167479, 2.358598, 0.68544364, 1.6157433)\n",
      "   validation loss 0.00042948572081513703, (0.0002442488, 6.226754, 1.690795, 1.6157433)\n",
      "decoder loss ratio: 0.001282, decoder SINDy loss  ratio: 0.128683\n",
      "Epoch 41\n",
      "   training loss 0.0002548604679759592, (0.00017412896, 2.2616298, 0.64361626, 1.6369876)\n",
      "   validation loss 0.00038449486601166427, (0.00020691614, 6.1592097, 1.6120883, 1.6369876)\n",
      "decoder loss ratio: 0.001086, decoder SINDy loss  ratio: 0.122693\n",
      "Epoch 42\n",
      "   training loss 0.0002546926261857152, (0.00017600009, 2.097523, 0.6212571, 1.6566821)\n",
      "   validation loss 0.0003814410883933306, (0.00020696974, 5.9404345, 1.579045, 1.6566821)\n",
      "decoder loss ratio: 0.001086, decoder SINDy loss  ratio: 0.120178\n",
      "Epoch 43\n",
      "   training loss 0.00022434470884036273, (0.0001480964, 1.9812666, 0.594884, 1.6759924)\n",
      "   validation loss 0.0003389335179235786, (0.0001712399, 5.7850714, 1.5093371, 1.6759924)\n",
      "decoder loss ratio: 0.000899, decoder SINDy loss  ratio: 0.114872\n",
      "Epoch 44\n",
      "   training loss 0.00025639194063842297, (0.00018192822, 1.8289875, 0.5751314, 1.6950585)\n",
      "   validation loss 0.00037546196836046875, (0.00021215499, 5.686804, 1.4635639, 1.6950585)\n",
      "decoder loss ratio: 0.001113, decoder SINDy loss  ratio: 0.111389\n",
      "Epoch 45\n",
      "   training loss 0.00021388891036622226, (0.0001417896, 1.6828848, 0.54974097, 1.7125226)\n",
      "   validation loss 0.00031435437267646194, (0.00015452786, 5.3879585, 1.427013, 1.7125226)\n",
      "decoder loss ratio: 0.000811, decoder SINDy loss  ratio: 0.108607\n",
      "Epoch 46\n",
      "   training loss 0.000201821094378829, (0.00013127133, 1.5782591, 0.53253675, 1.729608)\n",
      "   validation loss 0.00029868559795431793, (0.00014258212, 5.225503, 1.3880739, 1.729608)\n",
      "decoder loss ratio: 0.000748, decoder SINDy loss  ratio: 0.105643\n",
      "Epoch 47\n",
      "   training loss 0.00021884095622226596, (0.00015091393, 1.4701027, 0.50455016, 1.7472019)\n",
      "   validation loss 0.00030436369706876576, (0.00015144753, 5.0036564, 1.3544415, 1.7472019)\n",
      "decoder loss ratio: 0.000795, decoder SINDy loss  ratio: 0.103084\n",
      "Epoch 48\n",
      "   training loss 0.00020656699780374765, (0.0001397511, 1.3749814, 0.49175063, 1.7640846)\n",
      "   validation loss 0.0002866825961973518, (0.00013675666, 4.8453255, 1.322851, 1.7640846)\n",
      "decoder loss ratio: 0.000718, decoder SINDy loss  ratio: 0.100679\n",
      "Epoch 49\n",
      "   training loss 0.00036558194551616907, (0.00029302842, 1.2820499, 0.5474802, 1.7805479)\n",
      "   validation loss 0.00043722859118133783, (0.0002874004, 4.8374953, 1.3202269, 1.7805479)\n",
      "decoder loss ratio: 0.001508, decoder SINDy loss  ratio: 0.100480\n",
      "Epoch 50\n",
      "   training loss 0.00017006734560709447, (0.00010445309, 1.2142558, 0.47650647, 1.7963618)\n",
      "   validation loss 0.00024827956804074347, (0.00010332127, 4.581632, 1.269947, 1.7963618)\n",
      "decoder loss ratio: 0.000542, decoder SINDy loss  ratio: 0.096653\n",
      "Epoch 51\n",
      "   training loss 0.0002006822032853961, (0.00013497926, 1.1283075, 0.47578892, 1.812406)\n",
      "   validation loss 0.00027179272728972137, (0.00012933431, 4.4939733, 1.2433436, 1.812406)\n",
      "decoder loss ratio: 0.000679, decoder SINDy loss  ratio: 0.094628\n",
      "Epoch 52\n",
      "   training loss 0.00018220467609353364, (0.00011882846, 1.0957605, 0.45101368, 1.8274845)\n",
      "   validation loss 0.0002576782426331192, (0.00011803182, 4.368498, 1.2137157, 1.8274845)\n",
      "decoder loss ratio: 0.000619, decoder SINDy loss  ratio: 0.092373\n",
      "Epoch 53\n",
      "   training loss 0.0001621884439373389, (0.00010015587, 1.0276861, 0.43610817, 1.8421752)\n",
      "   validation loss 0.000232946258620359, (9.507458e-05, 4.2503476, 1.1944994, 1.8421752)\n",
      "decoder loss ratio: 0.000499, decoder SINDy loss  ratio: 0.090911\n",
      "Epoch 54\n",
      "   training loss 0.00015989357780199498, (9.8516306e-05, 0.9630968, 0.42808175, 1.8569099)\n",
      "   validation loss 0.00022895836445968598, (9.252106e-05, 4.1653657, 1.1786821, 1.8569099)\n",
      "decoder loss ratio: 0.000486, decoder SINDy loss  ratio: 0.089707\n",
      "Epoch 55\n",
      "   training loss 0.00016136678459588438, (9.9917816e-05, 0.93224853, 0.42735785, 1.871318)\n",
      "   validation loss 0.0002294087316840887, (9.5330106e-05, 4.0576563, 1.1536546, 1.871318)\n",
      "decoder loss ratio: 0.000500, decoder SINDy loss  ratio: 0.087802\n",
      "Epoch 56\n",
      "   training loss 0.00018158098100684583, (0.00012148837, 0.9470973, 0.41241544, 1.8851072)\n",
      "   validation loss 0.00026351926499046385, (0.00013227263, 3.9931047, 1.1239556, 1.8851072)\n",
      "decoder loss ratio: 0.000694, decoder SINDy loss  ratio: 0.085542\n",
      "Epoch 57\n",
      "   training loss 0.0001901041978271678, (0.00013027534, 0.8476642, 0.40852386, 1.8976464)\n",
      "   validation loss 0.000251628749538213, (0.00012122173, 3.9649682, 1.1143055, 1.8976464)\n",
      "decoder loss ratio: 0.000636, decoder SINDy loss  ratio: 0.084807\n",
      "Epoch 58\n",
      "   training loss 0.0001296074769925326, (7.2925926e-05, 0.816675, 0.37583923, 1.909763)\n",
      "   validation loss 0.00019618972146417946, (6.8462614e-05, 3.858916, 1.0862948, 1.909763)\n",
      "decoder loss ratio: 0.000359, decoder SINDy loss  ratio: 0.082676\n",
      "Epoch 59\n",
      "   training loss 0.00014718883903697133, (9.041125e-05, 0.82871795, 0.37553036, 1.9224547)\n",
      "   validation loss 0.00022834014089312404, (0.00010260408, 3.7895741, 1.0651151, 1.9224547)\n",
      "decoder loss ratio: 0.000538, decoder SINDy loss  ratio: 0.081064\n",
      "Epoch 60\n",
      "   training loss 0.00011519120016600937, (6.064083e-05, 0.76331234, 0.3521729, 1.9333078)\n",
      "   validation loss 0.0001829817338148132, (5.9090275e-05, 3.7260284, 1.0455838, 1.9333078)\n",
      "decoder loss ratio: 0.000310, decoder SINDy loss  ratio: 0.079577\n",
      "Epoch 61\n",
      "   training loss 0.00014045847638044506, (8.74956e-05, 0.74488974, 0.33512846, 1.9450033)\n",
      "   validation loss 0.0002071154012810439, (8.557171e-05, 3.6866734, 1.0209366, 1.9450033)\n",
      "decoder loss ratio: 0.000449, decoder SINDy loss  ratio: 0.077701\n",
      "Epoch 62\n",
      "   training loss 0.0001578016090206802, (0.000105285006, 0.7525075, 0.32954782, 1.9561824)\n",
      "   validation loss 0.00022016727598384023, (0.000100159894, 3.6458938, 1.0044557, 1.9561824)\n",
      "decoder loss ratio: 0.000526, decoder SINDy loss  ratio: 0.076447\n",
      "Epoch 63\n",
      "   training loss 0.00011350856220815331, (6.2492785e-05, 0.69046307, 0.31347832, 1.9667944)\n",
      "   validation loss 0.00018530258967075497, (6.690683e-05, 3.5862167, 0.98727804, 1.9667944)\n",
      "decoder loss ratio: 0.000351, decoder SINDy loss  ratio: 0.075140\n",
      "Epoch 64\n",
      "   training loss 0.0001991426688618958, (0.00014629497, 0.67598647, 0.33082062, 1.9765636)\n",
      "   validation loss 0.0002663549967110157, (0.00014802178, 3.565811, 0.9856759, 1.9765636)\n",
      "decoder loss ratio: 0.000777, decoder SINDy loss  ratio: 0.075018\n",
      "Epoch 65\n",
      "   training loss 0.00010761345765786245, (5.833657e-05, 0.6417293, 0.2942131, 1.9855578)\n",
      "   validation loss 0.000176436806214042, (6.0970277e-05, 3.477474, 0.9561096, 1.9855578)\n",
      "decoder loss ratio: 0.000320, decoder SINDy loss  ratio: 0.072767\n",
      "Epoch 66\n",
      "   training loss 0.0001230800844496116, (7.526898e-05, 0.63464695, 0.2785958, 1.9951522)\n",
      "   validation loss 0.00018780633399728686, (7.462259e-05, 3.4378798, 0.93232226, 1.9951522)\n",
      "decoder loss ratio: 0.000392, decoder SINDy loss  ratio: 0.070957\n",
      "Epoch 67\n",
      "   training loss 0.00012759710079990327, (7.914857e-05, 0.60470337, 0.28411156, 2.0037372)\n",
      "   validation loss 0.00019918984617106616, (8.6309476e-05, 3.407469, 0.92842996, 2.0037372)\n",
      "decoder loss ratio: 0.000453, decoder SINDy loss  ratio: 0.070661\n",
      "Epoch 68\n",
      "   training loss 9.933394176186994e-05, (5.2920357e-05, 0.57981056, 0.26297644, 2.0115943)\n",
      "   validation loss 0.00016902403149288148, (5.838343e-05, 3.3340669, 0.9052467, 2.0115943)\n",
      "decoder loss ratio: 0.000306, decoder SINDy loss  ratio: 0.068896\n",
      "Epoch 69\n",
      "   training loss 0.00010671377094695345, (6.1720806e-05, 0.55725414, 0.24793465, 2.0199502)\n",
      "   validation loss 0.0001723433524603024, (6.377706e-05, 3.2874255, 0.88366807, 2.0199502)\n",
      "decoder loss ratio: 0.000335, decoder SINDy loss  ratio: 0.067254\n",
      "Epoch 70\n",
      "   training loss 8.682030602358282e-05, (4.2334024e-05, 0.5501634, 0.24208497, 2.0277789)\n",
      "   validation loss 0.00015660488861612976, (4.9001315e-05, 3.2552137, 0.87325794, 2.0277789)\n",
      "decoder loss ratio: 0.000257, decoder SINDy loss  ratio: 0.066462\n",
      "Epoch 71\n",
      "   training loss 9.689669968793169e-05, (5.3125525e-05, 0.53500164, 0.23421532, 2.0349648)\n",
      "   validation loss 0.00016512478759977967, (5.942288e-05, 3.1996024, 0.85352266, 2.0349648)\n",
      "decoder loss ratio: 0.000312, decoder SINDy loss  ratio: 0.064960\n",
      "Epoch 72\n",
      "   training loss 9.825175220612437e-05, (5.5510005e-05, 0.49923208, 0.22324002, 2.041775)\n",
      "   validation loss 0.00017229218792635947, (6.747099e-05, 3.1309319, 0.8440345, 2.041775)\n",
      "decoder loss ratio: 0.000354, decoder SINDy loss  ratio: 0.064238\n",
      "Epoch 73\n",
      "   training loss 8.398877980653197e-05, (4.2022824e-05, 0.48686945, 0.2147929, 2.0486667)\n",
      "   validation loss 0.00015133764827623963, (4.8772068e-05, 3.084336, 0.82078916, 2.0486667)\n",
      "decoder loss ratio: 0.000256, decoder SINDy loss  ratio: 0.062468\n",
      "Epoch 74\n",
      "   training loss 0.00022570198052562773, (0.00018131715, 0.49680886, 0.23834004, 2.0550823)\n",
      "   validation loss 0.0003008672210853547, (0.00019778628, 3.0713675, 0.8253013, 2.0550823)\n",
      "decoder loss ratio: 0.001038, decoder SINDy loss  ratio: 0.062812\n",
      "Epoch 75\n",
      "   training loss 6.509477680083364e-05, (2.427381e-05, 0.44986323, 0.20229876, 2.0591094)\n",
      "   validation loss 0.00013260547711979598, (3.2652297e-05, 2.9921145, 0.7936209, 2.0591094)\n",
      "decoder loss ratio: 0.000171, decoder SINDy loss  ratio: 0.060401\n",
      "Epoch 76\n",
      "   training loss 7.438301690854132e-05, (3.444733e-05, 0.43521154, 0.19283268, 2.0652418)\n",
      "   validation loss 0.00013987378042656928, (4.1415744e-05, 2.9274437, 0.7780562, 2.0652418)\n",
      "decoder loss ratio: 0.000217, decoder SINDy loss  ratio: 0.059216\n",
      "Epoch 77\n",
      "   training loss 8.419984806096181e-05, (4.508101e-05, 0.42054144, 0.18401769, 2.071707)\n",
      "   validation loss 0.0001461201172787696, (4.9708437e-05, 2.8783264, 0.7569462, 2.071707)\n",
      "decoder loss ratio: 0.000261, decoder SINDy loss  ratio: 0.057610\n",
      "Epoch 78\n",
      "   training loss 0.00012523877376224846, (8.424961e-05, 0.44754633, 0.2021095, 2.077821)\n",
      "   validation loss 0.00019602719112299383, (9.983307e-05, 2.7989209, 0.754159, 2.077821)\n",
      "decoder loss ratio: 0.000524, decoder SINDy loss  ratio: 0.057397\n",
      "Epoch 79\n",
      "   training loss 0.00020757151651196182, (0.00016668068, 0.42565405, 0.20066658, 2.0824177)\n",
      "   validation loss 0.00029228374478407204, (0.00019627463, 2.796265, 0.7518495, 2.0824177)\n",
      "decoder loss ratio: 0.001030, decoder SINDy loss  ratio: 0.057222\n",
      "Epoch 80\n",
      "   training loss 5.7741992350202054e-05, (2.0031166e-05, 0.37300208, 0.16852584, 2.0858245)\n",
      "   validation loss 0.0001238770637428388, (3.1356452e-05, 2.7091417, 0.71662366, 2.0858245)\n",
      "decoder loss ratio: 0.000165, decoder SINDy loss  ratio: 0.054541\n",
      "Epoch 81\n",
      "   training loss 6.635121826548129e-05, (2.918248e-05, 0.35630623, 0.16260245, 2.0908499)\n",
      "   validation loss 0.0001307840138906613, (3.959396e-05, 2.6424506, 0.7028156, 2.0908499)\n",
      "decoder loss ratio: 0.000208, decoder SINDy loss  ratio: 0.053490\n",
      "Epoch 82\n",
      "   training loss 7.378088048426434e-05, (3.7467187e-05, 0.34530446, 0.1535549, 2.0958204)\n",
      "   validation loss 0.00013355178816709667, (4.4252178e-05, 2.6029859, 0.6834141, 2.0958204)\n",
      "decoder loss ratio: 0.000232, decoder SINDy loss  ratio: 0.052013\n",
      "Epoch 83\n",
      "   training loss 0.00015731749590486288, (0.00011984153, 0.3623236, 0.16467929, 2.1008043)\n",
      "   validation loss 0.000234521139645949, (0.00014510173, 2.5783844, 0.68411374, 2.1008043)\n",
      "decoder loss ratio: 0.000761, decoder SINDy loss  ratio: 0.052066\n",
      "Epoch 84\n",
      "   training loss 5.462967965286225e-05, (1.8866162e-05, 0.3197578, 0.14730524, 2.1032996)\n",
      "   validation loss 0.0001165258654509671, (2.9873741e-05, 2.4959798, 0.6561913, 2.1032996)\n",
      "decoder loss ratio: 0.000157, decoder SINDy loss  ratio: 0.049941\n",
      "Epoch 85\n",
      "   training loss 5.746819078922272e-05, (2.239088e-05, 0.3139619, 0.14001039, 2.1076272)\n",
      "   validation loss 0.00011705189535859972, (3.2080185e-05, 2.4294882, 0.6389544, 2.1076272)\n",
      "decoder loss ratio: 0.000168, decoder SINDy loss  ratio: 0.048629\n",
      "Epoch 86\n",
      "   training loss 6.716603820677847e-05, (3.204526e-05, 0.31340888, 0.13996616, 2.1124163)\n",
      "   validation loss 0.00012715697812382132, (4.3376767e-05, 2.3874505, 0.6265605, 2.1124163)\n",
      "decoder loss ratio: 0.000228, decoder SINDy loss  ratio: 0.047686\n",
      "Epoch 87\n",
      "   training loss 0.0002735465532168746, (0.00023610686, 0.3327016, 0.16277649, 2.1162026)\n",
      "   validation loss 0.00036469835322350264, (0.0002791366, 2.3671033, 0.6439969, 2.1162026)\n",
      "decoder loss ratio: 0.001465, decoder SINDy loss  ratio: 0.049013\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_60396\\4054194948.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mresults_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mresults_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ianma\\Desktop\\Study\\Python\\CS4240_DeepLearning_Group41_ReproducibilityProject\\SindyAutoencoders-master\\src\\training.py\u001b[0m in \u001b[0;36mtrain_network\u001b[1;34m(training_data, val_data, params)\u001b[0m\n\u001b[0;32m     31\u001b[0m                 \u001b[0mbatch_idxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'batch_size'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'batch_size'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m                 \u001b[0mtrain_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_feed_dictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midxs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_idxs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m                 \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'print_progress'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'print_frequency'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m                 \u001b[0mvalidation_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msindy_predict_norm_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ianma\\anaconda3\\envs\\tfv1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 956\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    957\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ianma\\anaconda3\\envs\\tfv1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1180\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1181\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ianma\\anaconda3\\envs\\tfv1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1359\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ianma\\anaconda3\\envs\\tfv1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1363\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ianma\\anaconda3\\envs\\tfv1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1350\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ianma\\anaconda3\\envs\\tfv1\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1443\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_experiments = 1\n",
    "df = pd.DataFrame()\n",
    "for i in range(num_experiments):\n",
    "    print('EXPERIMENT %d' % i)\n",
    "\n",
    "    params['coefficient_mask'] = np.ones((params['library_dim'], params['latent_dim']))\n",
    "\n",
    "    params['save_name'] = 'lorenz_' + datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S_%f\")\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    results_dict = train_network(training_data, validation_data, params)\n",
    "    df = df.append({**results_dict, **params}, ignore_index=True)\n",
    "\n",
    "df.to_pickle('experiment_results_' + datetime.datetime.now().strftime(\"%Y%m%d%H%M\") + '.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
