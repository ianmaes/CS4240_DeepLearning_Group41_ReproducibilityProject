{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from example_lorenz import get_lorenz_data\n",
    "from sindy_utils import library_size\n",
    "from training import train_network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate training, validation, testing data\n",
    "noise_strength = 1e-6\n",
    "training_data = get_lorenz_data(1024, noise_strength=noise_strength)\n",
    "validation_data = get_lorenz_data(20, noise_strength=noise_strength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up model and training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {}\n",
    "\n",
    "params['input_dim'] = 128\n",
    "params['latent_dim'] = 3\n",
    "params['model_order'] = 1\n",
    "params['poly_order'] = 3\n",
    "params['include_sine'] = False\n",
    "params['library_dim'] = library_size(params['latent_dim'], params['poly_order'], params['include_sine'], True)\n",
    "\n",
    "# sequential thresholding parameters\n",
    "params['sequential_thresholding'] = True\n",
    "params['coefficient_threshold'] = 0.1\n",
    "params['threshold_frequency'] = 500\n",
    "params['coefficient_mask'] = np.ones((params['library_dim'], params['latent_dim']))\n",
    "params['coefficient_initialization'] = 'constant'\n",
    "\n",
    "# loss function weighting\n",
    "params['loss_weight_decoder'] = 1.0\n",
    "params['loss_weight_sindy_z'] = 0.0\n",
    "params['loss_weight_sindy_x'] = 1e-4\n",
    "params['loss_weight_sindy_regularization'] = 1e-5\n",
    "\n",
    "params['activation'] = 'sigmoid'\n",
    "params['widths'] = [64,32]\n",
    "\n",
    "# training parameters\n",
    "params['epoch_size'] = training_data['x'].shape[0]\n",
    "params['batch_size'] = 1024\n",
    "params['learning_rate'] = 1e-3\n",
    "\n",
    "params['data_path'] = os.getcwd() + '/'\n",
    "params['print_progress'] = True\n",
    "params['print_frequency'] = 1\n",
    "\n",
    "# training time cutoffs\n",
    "params['max_epochs'] = 5001\n",
    "params['refinement_epochs'] = 1001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT 0\n",
      "TRAINING\n",
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ianma\\Desktop\\Study\\Python\\CS4240_DeepLearning_Group41_ReproducibilityProject\\Autoencoders-Reproduction\\examples\\../src\\training.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss_val, _, _ = autoencoder_network.define_loss(torch.tensor(train_dict['x'], dtype=torch.float32), torch.tensor(train_dict['dx'], dtype=torch.float32), params=params)\n",
      "c:\\Users\\ianma\\Desktop\\Study\\Python\\CS4240_DeepLearning_Group41_ReproducibilityProject\\Autoencoders-Reproduction\\examples\\../src\\training.py:125: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_total_loss, train_losses, _ = network.define_loss( torch.tensor(train_dict['x'], dtype=torch.float32) , torch.tensor(train_dict['dx'], dtype=torch.float32), params=params)\n",
      "c:\\Users\\ianma\\Desktop\\Study\\Python\\CS4240_DeepLearning_Group41_ReproducibilityProject\\Autoencoders-Reproduction\\examples\\../src\\training.py:128: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_total_loss, val_losses, _ = network.define_loss( torch.tensor(validation_dict['x'], dtype=torch.float32), torch.tensor(validation_dict['x'], dtype=torch.float32),  params=params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "   Training Total Loss: 0.005688742734491825\n",
      "   Training decoder Loss: 0.005075056105852127\n",
      "   Training sindy_z Loss: 8.380980491638184\n",
      "   Training sindy_x Loss: 6.057415962219238\n",
      "   Training sindy_regularization Loss: 0.7945341467857361\n",
      "   Validation Total Loss: 0.011230173520743847\n",
      "   Validation decoder Loss: 0.011204272508621216\n",
      "   Validation sindy_z Loss: 0.7979886531829834\n",
      "   Validation sindy_x Loss: 0.17955882847309113\n",
      "   Validation sindy_regularization Loss: 0.7945341467857361\n",
      "Decoder Loss Ratio: 0.058202, Decoder SINDy Loss Ratio: 0.011487\n",
      "Epoch 1\n",
      "Epoch 1\n",
      "   Training Total Loss: 0.005281791090965271\n",
      "   Training decoder Loss: 0.004670129623264074\n",
      "   Training sindy_z Loss: 6.845315933227539\n",
      "   Training sindy_x Loss: 6.053071975708008\n",
      "   Training sindy_regularization Loss: 0.6354314684867859\n",
      "   Validation Total Loss: 0.009879706427454948\n",
      "   Validation decoder Loss: 0.00985450204461813\n",
      "   Validation sindy_z Loss: 0.6222403645515442\n",
      "   Validation sindy_x Loss: 0.18849940598011017\n",
      "   Validation sindy_regularization Loss: 0.6354314684867859\n",
      "Decoder Loss Ratio: 0.051190, Decoder SINDy Loss Ratio: 0.012059\n",
      "Epoch 2\n",
      "Epoch 2\n",
      "   Training Total Loss: 0.002848335774615407\n",
      "   Training decoder Loss: 0.0022394012194126844\n",
      "   Training sindy_z Loss: 6.4490275382995605\n",
      "   Training sindy_x Loss: 6.040679454803467\n",
      "   Training sindy_regularization Loss: 0.48665714263916016\n",
      "   Validation Total Loss: 0.004222425166517496\n",
      "   Validation decoder Loss: 0.00419861264526844\n",
      "   Validation sindy_z Loss: 0.4850783944129944\n",
      "   Validation sindy_x Loss: 0.18945904076099396\n",
      "   Validation sindy_regularization Loss: 0.48665714263916016\n",
      "Decoder Loss Ratio: 0.021810, Decoder SINDy Loss Ratio: 0.012120\n",
      "Epoch 3\n",
      "Epoch 3\n",
      "   Training Total Loss: 0.002351389965042472\n",
      "   Training decoder Loss: 0.0017445455305278301\n",
      "   Training sindy_z Loss: 5.663045406341553\n",
      "   Training sindy_x Loss: 6.029786109924316\n",
      "   Training sindy_regularization Loss: 0.38658851385116577\n",
      "   Validation Total Loss: 0.003134526079520583\n",
      "   Validation decoder Loss: 0.003111611818894744\n",
      "   Validation sindy_z Loss: 0.4321189522743225\n",
      "   Validation sindy_x Loss: 0.19048258662223816\n",
      "   Validation sindy_regularization Loss: 0.38658851385116577\n",
      "Decoder Loss Ratio: 0.016164, Decoder SINDy Loss Ratio: 0.012185\n",
      "Epoch 4\n",
      "Epoch 4\n",
      "   Training Total Loss: 0.0022255710791796446\n",
      "   Training decoder Loss: 0.0016197378281503916\n",
      "   Training sindy_z Loss: 5.424263000488281\n",
      "   Training sindy_x Loss: 6.022118091583252\n",
      "   Training sindy_regularization Loss: 0.36213383078575134\n",
      "   Validation Total Loss: 0.0029031753074377775\n",
      "   Validation decoder Loss: 0.0028802466113120317\n",
      "   Validation sindy_z Loss: 0.46811896562576294\n",
      "   Validation sindy_x Loss: 0.19307292997837067\n",
      "   Validation sindy_regularization Loss: 0.36213383078575134\n",
      "Decoder Loss Ratio: 0.014962, Decoder SINDy Loss Ratio: 0.012351\n",
      "Epoch 5\n",
      "Epoch 5\n",
      "   Training Total Loss: 0.0020582873839884996\n",
      "   Training decoder Loss: 0.0014532357454299927\n",
      "   Training sindy_z Loss: 5.251926422119141\n",
      "   Training sindy_x Loss: 6.014401912689209\n",
      "   Training sindy_regularization Loss: 0.36114880442619324\n",
      "   Validation Total Loss: 0.0027060701977461576\n",
      "   Validation decoder Loss: 0.002682930091395974\n",
      "   Validation sindy_z Loss: 0.5004799365997314\n",
      "   Validation sindy_x Loss: 0.19528701901435852\n",
      "   Validation sindy_regularization Loss: 0.36114880442619324\n",
      "Decoder Loss Ratio: 0.013937, Decoder SINDy Loss Ratio: 0.012493\n",
      "Epoch 6\n",
      "Epoch 6\n",
      "   Training Total Loss: 0.0023537129163742065\n",
      "   Training decoder Loss: 0.0017492379993200302\n",
      "   Training sindy_z Loss: 5.0435309410095215\n",
      "   Training sindy_x Loss: 6.007218360900879\n",
      "   Training sindy_regularization Loss: 0.37531691789627075\n",
      "   Validation Total Loss: 0.0029857968911528587\n",
      "   Validation decoder Loss: 0.0029624130111187696\n",
      "   Validation sindy_z Loss: 0.5214252471923828\n",
      "   Validation sindy_x Loss: 0.19630645215511322\n",
      "   Validation sindy_regularization Loss: 0.37531691789627075\n",
      "Decoder Loss Ratio: 0.015389, Decoder SINDy Loss Ratio: 0.012558\n",
      "Epoch 7\n",
      "Epoch 7\n",
      "   Training Total Loss: 0.0022257270757108927\n",
      "   Training decoder Loss: 0.0016218931414186954\n",
      "   Training sindy_z Loss: 4.990791320800781\n",
      "   Training sindy_x Loss: 5.998674392700195\n",
      "   Training sindy_regularization Loss: 0.39664426445961\n",
      "   Validation Total Loss: 0.0028495248407125473\n",
      "   Validation decoder Loss: 0.0028258468955755234\n",
      "   Validation sindy_z Loss: 0.5527461767196655\n",
      "   Validation sindy_x Loss: 0.1971133053302765\n",
      "   Validation sindy_regularization Loss: 0.39664426445961\n",
      "Decoder Loss Ratio: 0.014679, Decoder SINDy Loss Ratio: 0.012610\n",
      "Epoch 8\n",
      "Epoch 8\n",
      "   Training Total Loss: 0.0024207851383835077\n",
      "   Training decoder Loss: 0.001817363896407187\n",
      "   Training sindy_z Loss: 5.001265048980713\n",
      "   Training sindy_x Loss: 5.991793155670166\n",
      "   Training sindy_regularization Loss: 0.42419853806495667\n",
      "   Validation Total Loss: 0.003021469572558999\n",
      "   Validation decoder Loss: 0.002997556235641241\n",
      "   Validation sindy_z Loss: 0.5846427083015442\n",
      "   Validation sindy_x Loss: 0.19671432673931122\n",
      "   Validation sindy_regularization Loss: 0.42419853806495667\n",
      "Decoder Loss Ratio: 0.015571, Decoder SINDy Loss Ratio: 0.012584\n",
      "Epoch 9\n",
      "Epoch 9\n",
      "   Training Total Loss: 0.002051770221441984\n",
      "   Training decoder Loss: 0.00144881138112396\n",
      "   Training sindy_z Loss: 5.056053638458252\n",
      "   Training sindy_x Loss: 5.983928680419922\n",
      "   Training sindy_regularization Loss: 0.4566084146499634\n",
      "   Validation Total Loss: 0.0026936165522783995\n",
      "   Validation decoder Loss: 0.0026694023981690407\n",
      "   Validation sindy_z Loss: 0.6143291592597961\n",
      "   Validation sindy_x Loss: 0.19648072123527527\n",
      "   Validation sindy_regularization Loss: 0.4566084146499634\n",
      "Decoder Loss Ratio: 0.013866, Decoder SINDy Loss Ratio: 0.012569\n",
      "Epoch 10\n",
      "Epoch 10\n",
      "   Training Total Loss: 0.0020698991138488054\n",
      "   Training decoder Loss: 0.0014673470286652446\n",
      "   Training sindy_z Loss: 4.9744873046875\n",
      "   Training sindy_x Loss: 5.976639747619629\n",
      "   Training sindy_regularization Loss: 0.488809734582901\n",
      "   Validation Total Loss: 0.0027206959202885628\n",
      "   Validation decoder Loss: 0.0026961257681250572\n",
      "   Validation sindy_z Loss: 0.6394752860069275\n",
      "   Validation sindy_x Loss: 0.19682098925113678\n",
      "   Validation sindy_regularization Loss: 0.488809734582901\n",
      "Decoder Loss Ratio: 0.014005, Decoder SINDy Loss Ratio: 0.012591\n",
      "Epoch 11\n",
      "Epoch 11\n",
      "   Training Total Loss: 0.002335200784727931\n",
      "   Training decoder Loss: 0.001733162673190236\n",
      "   Training sindy_z Loss: 4.972990989685059\n",
      "   Training sindy_x Loss: 5.968202590942383\n",
      "   Training sindy_regularization Loss: 0.521807074546814\n",
      "   Validation Total Loss: 0.0029511835891753435\n",
      "   Validation decoder Loss: 0.0029262390453368425\n",
      "   Validation sindy_z Loss: 0.6876778602600098\n",
      "   Validation sindy_x Loss: 0.1972668468952179\n",
      "   Validation sindy_regularization Loss: 0.521807074546814\n",
      "Decoder Loss Ratio: 0.015201, Decoder SINDy Loss Ratio: 0.012619\n",
      "Epoch 12\n",
      "Epoch 12\n",
      "   Training Total Loss: 0.002066770102828741\n",
      "   Training decoder Loss: 0.0014651448000222445\n",
      "   Training sindy_z Loss: 5.064436435699463\n",
      "   Training sindy_x Loss: 5.960722923278809\n",
      "   Training sindy_regularization Loss: 0.5553123950958252\n",
      "   Validation Total Loss: 0.0027088767383247614\n",
      "   Validation decoder Loss: 0.002683722646906972\n",
      "   Validation sindy_z Loss: 0.7371352314949036\n",
      "   Validation sindy_x Loss: 0.19601166248321533\n",
      "   Validation sindy_regularization Loss: 0.5553123950958252\n",
      "Decoder Loss Ratio: 0.013941, Decoder SINDy Loss Ratio: 0.012539\n",
      "Epoch 13\n",
      "Epoch 13\n",
      "   Training Total Loss: 0.002265570219606161\n",
      "   Training decoder Loss: 0.0016644448041915894\n",
      "   Training sindy_z Loss: 5.062422275543213\n",
      "   Training sindy_x Loss: 5.952337265014648\n",
      "   Training sindy_regularization Loss: 0.5891688466072083\n",
      "   Validation Total Loss: 0.002896952675655484\n",
      "   Validation decoder Loss: 0.002871379954740405\n",
      "   Validation sindy_z Loss: 0.7826485633850098\n",
      "   Validation sindy_x Loss: 0.1968102902173996\n",
      "   Validation sindy_regularization Loss: 0.5891688466072083\n",
      "Decoder Loss Ratio: 0.014916, Decoder SINDy Loss Ratio: 0.012590\n",
      "Epoch 14\n",
      "Epoch 14\n",
      "   Training Total Loss: 0.002042029984295368\n",
      "   Training decoder Loss: 0.0014412235468626022\n",
      "   Training sindy_z Loss: 5.199628829956055\n",
      "   Training sindy_x Loss: 5.945682525634766\n",
      "   Training sindy_regularization Loss: 0.6238242387771606\n",
      "   Validation Total Loss: 0.002688918262720108\n",
      "   Validation decoder Loss: 0.0026631075888872147\n",
      "   Validation sindy_z Loss: 0.8452352285385132\n",
      "   Validation sindy_x Loss: 0.19572417438030243\n",
      "   Validation sindy_regularization Loss: 0.6238242387771606\n",
      "Decoder Loss Ratio: 0.013834, Decoder SINDy Loss Ratio: 0.012521\n",
      "Epoch 15\n",
      "Epoch 15\n",
      "   Training Total Loss: 0.0020985789597034454\n",
      "   Training decoder Loss: 0.0014983215369284153\n",
      "   Training sindy_z Loss: 5.150862693786621\n",
      "   Training sindy_x Loss: 5.936667442321777\n",
      "   Training sindy_regularization Loss: 0.6590626835823059\n",
      "   Validation Total Loss: 0.002752854721620679\n",
      "   Validation decoder Loss: 0.002726558595895767\n",
      "   Validation sindy_z Loss: 0.9037119746208191\n",
      "   Validation sindy_x Loss: 0.19705358147621155\n",
      "   Validation sindy_regularization Loss: 0.6590626835823059\n",
      "Decoder Loss Ratio: 0.014163, Decoder SINDy Loss Ratio: 0.012606\n",
      "Epoch 16\n",
      "Epoch 16\n",
      "   Training Total Loss: 0.002194552216678858\n",
      "   Training decoder Loss: 0.0015946158673614264\n",
      "   Training sindy_z Loss: 5.178450107574463\n",
      "   Training sindy_x Loss: 5.92985200881958\n",
      "   Training sindy_regularization Loss: 0.6951053142547607\n",
      "   Validation Total Loss: 0.0028266925364732742\n",
      "   Validation decoder Loss: 0.002800019457936287\n",
      "   Validation sindy_z Loss: 0.9908745884895325\n",
      "   Validation sindy_x Loss: 0.1972191482782364\n",
      "   Validation sindy_regularization Loss: 0.6951053142547607\n",
      "Decoder Loss Ratio: 0.014545, Decoder SINDy Loss Ratio: 0.012616\n",
      "Epoch 17\n",
      "Epoch 17\n",
      "   Training Total Loss: 0.0020451932214200497\n",
      "   Training decoder Loss: 0.0014456965727731586\n",
      "   Training sindy_z Loss: 5.287374496459961\n",
      "   Training sindy_x Loss: 5.921637058258057\n",
      "   Training sindy_regularization Loss: 0.7332956194877625\n",
      "   Validation Total Loss: 0.0026919262018054724\n",
      "   Validation decoder Loss: 0.0026648452039808035\n",
      "   Validation sindy_z Loss: 1.0795856714248657\n",
      "   Validation sindy_x Loss: 0.19748112559318542\n",
      "   Validation sindy_regularization Loss: 0.7332956194877625\n",
      "Decoder Loss Ratio: 0.013843, Decoder SINDy Loss Ratio: 0.012633\n",
      "Epoch 18\n",
      "Epoch 18\n",
      "   Training Total Loss: 0.0021364728454500437\n",
      "   Training decoder Loss: 0.0015372789930552244\n",
      "   Training sindy_z Loss: 5.277833938598633\n",
      "   Training sindy_x Loss: 5.914663791656494\n",
      "   Training sindy_regularization Loss: 0.7727454900741577\n",
      "   Validation Total Loss: 0.002788569312542677\n",
      "   Validation decoder Loss: 0.002760931383818388\n",
      "   Validation sindy_z Loss: 1.1531866788864136\n",
      "   Validation sindy_x Loss: 0.1991053968667984\n",
      "   Validation sindy_regularization Loss: 0.7727454900741577\n",
      "Decoder Loss Ratio: 0.014342, Decoder SINDy Loss Ratio: 0.012737\n",
      "Epoch 19\n",
      "Epoch 19\n",
      "   Training Total Loss: 0.0020591034553945065\n",
      "   Training decoder Loss: 0.0014603791059926152\n",
      "   Training sindy_z Loss: 5.310556888580322\n",
      "   Training sindy_x Loss: 5.905889987945557\n",
      "   Training sindy_regularization Loss: 0.813522219657898\n",
      "   Validation Total Loss: 0.0027127359062433243\n",
      "   Validation decoder Loss: 0.002684612525627017\n",
      "   Validation sindy_z Loss: 1.2570958137512207\n",
      "   Validation sindy_x Loss: 0.19987936317920685\n",
      "   Validation sindy_regularization Loss: 0.813522219657898\n",
      "Decoder Loss Ratio: 0.013945, Decoder SINDy Loss Ratio: 0.012787\n",
      "Epoch 20\n",
      "Epoch 20\n",
      "   Training Total Loss: 0.0023186244070529938\n",
      "   Training decoder Loss: 0.0017206418560817838\n",
      "   Training sindy_z Loss: 5.400848388671875\n",
      "   Training sindy_x Loss: 5.8943190574646\n",
      "   Training sindy_regularization Loss: 0.8550633788108826\n",
      "   Validation Total Loss: 0.002957106102257967\n",
      "   Validation decoder Loss: 0.0029284022748470306\n",
      "   Validation sindy_z Loss: 1.4266057014465332\n",
      "   Validation sindy_x Loss: 0.20153187215328217\n",
      "   Validation sindy_regularization Loss: 0.8550633788108826\n",
      "Decoder Loss Ratio: 0.015212, Decoder SINDy Loss Ratio: 0.012892\n",
      "Epoch 21\n",
      "Epoch 21\n",
      "   Training Total Loss: 0.0022521899081766605\n",
      "   Training decoder Loss: 0.0016544151585549116\n",
      "   Training sindy_z Loss: 5.5352630615234375\n",
      "   Training sindy_x Loss: 5.88804817199707\n",
      "   Training sindy_regularization Loss: 0.8969966173171997\n",
      "   Validation Total Loss: 0.0028737264219671488\n",
      "   Validation decoder Loss: 0.0028445704374462366\n",
      "   Validation sindy_z Loss: 1.6000728607177734\n",
      "   Validation sindy_x Loss: 0.20185859501361847\n",
      "   Validation sindy_regularization Loss: 0.8969966173171997\n",
      "Decoder Loss Ratio: 0.014776, Decoder SINDy Loss Ratio: 0.012913\n",
      "Epoch 22\n",
      "Epoch 22\n",
      "   Training Total Loss: 0.002069554291665554\n",
      "   Training decoder Loss: 0.0014723363565281034\n",
      "   Training sindy_z Loss: 5.6970343589782715\n",
      "   Training sindy_x Loss: 5.878201007843018\n",
      "   Training sindy_regularization Loss: 0.939792275428772\n",
      "   Validation Total Loss: 0.0027114299591630697\n",
      "   Validation decoder Loss: 0.002681601792573929\n",
      "   Validation sindy_z Loss: 1.740456461906433\n",
      "   Validation sindy_x Loss: 0.20430299639701843\n",
      "   Validation sindy_regularization Loss: 0.939792275428772\n",
      "Decoder Loss Ratio: 0.013930, Decoder SINDy Loss Ratio: 0.013070\n",
      "Epoch 23\n",
      "Epoch 23\n",
      "   Training Total Loss: 0.002435038797557354\n",
      "   Training decoder Loss: 0.0018381308764219284\n",
      "   Training sindy_z Loss: 5.7598958015441895\n",
      "   Training sindy_x Loss: 5.870794296264648\n",
      "   Training sindy_regularization Loss: 0.9828451871871948\n",
      "   Validation Total Loss: 0.0030448460020124912\n",
      "   Validation decoder Loss: 0.003014309797435999\n",
      "   Validation sindy_z Loss: 1.9192614555358887\n",
      "   Validation sindy_x Loss: 0.2070770561695099\n",
      "   Validation sindy_regularization Loss: 0.9828451871871948\n",
      "Decoder Loss Ratio: 0.015658, Decoder SINDy Loss Ratio: 0.013247\n",
      "Epoch 24\n",
      "Epoch 24\n",
      "   Training Total Loss: 0.0022250686306506395\n",
      "   Training decoder Loss: 0.0016290037892758846\n",
      "   Training sindy_z Loss: 5.942260265350342\n",
      "   Training sindy_x Loss: 5.858005046844482\n",
      "   Training sindy_regularization Loss: 1.026425838470459\n",
      "   Validation Total Loss: 0.0028806321788579226\n",
      "   Validation decoder Loss: 0.0028492470737546682\n",
      "   Validation sindy_z Loss: 2.1685187816619873\n",
      "   Validation sindy_x Loss: 0.21120832860469818\n",
      "   Validation sindy_regularization Loss: 1.026425838470459\n",
      "Decoder Loss Ratio: 0.014801, Decoder SINDy Loss Ratio: 0.013511\n",
      "Epoch 25\n",
      "Epoch 25\n",
      "   Training Total Loss: 0.0021146507933735847\n",
      "   Training decoder Loss: 0.0015190282138064504\n",
      "   Training sindy_z Loss: 6.050576686859131\n",
      "   Training sindy_x Loss: 5.849080562591553\n",
      "   Training sindy_regularization Loss: 1.07142972946167\n",
      "   Validation Total Loss: 0.002771929604932666\n",
      "   Validation decoder Loss: 0.0027397372759878635\n",
      "   Validation sindy_z Loss: 2.4528493881225586\n",
      "   Validation sindy_x Loss: 0.21477892994880676\n",
      "   Validation sindy_regularization Loss: 1.07142972946167\n",
      "Decoder Loss Ratio: 0.014232, Decoder SINDy Loss Ratio: 0.013740\n",
      "Epoch 26\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m     params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoefficient_mask\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlibrary_dim\u001b[39m\u001b[38;5;124m'\u001b[39m], params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatent_dim\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m      8\u001b[0m     params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlorenz_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS_\u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m     results_dict \u001b[38;5;241m=\u001b[39m train_network(training_data, validation_data, params)\n\u001b[0;32m     11\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresults_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams}, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     13\u001b[0m df\u001b[38;5;241m.\u001b[39mto_pickle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexperiment_results_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ianma\\Desktop\\Study\\Python\\CS4240_DeepLearning_Group41_ReproducibilityProject\\Autoencoders-Reproduction\\examples\\../src\\training.py:34\u001b[0m, in \u001b[0;36mtrain_network\u001b[1;34m(training_data, val_data, params)\u001b[0m\n\u001b[0;32m     32\u001b[0m train_dict \u001b[38;5;241m=\u001b[39m create_feed_dictionary(training_data, params, idxs\u001b[38;5;241m=\u001b[39mbatch_idxs)\n\u001b[0;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 34\u001b[0m output \u001b[38;5;241m=\u001b[39m autoencoder_network(train_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m], train_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdx\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     35\u001b[0m loss_val, _, _ \u001b[38;5;241m=\u001b[39m autoencoder_network\u001b[38;5;241m.\u001b[39mdefine_loss(torch\u001b[38;5;241m.\u001b[39mtensor(train_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32), torch\u001b[38;5;241m.\u001b[39mtensor(train_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdx\u001b[39m\u001b[38;5;124m'\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32), params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m     36\u001b[0m loss_val\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\ianma\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ianma\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ianma\\Desktop\\Study\\Python\\CS4240_DeepLearning_Group41_ReproducibilityProject\\Autoencoders-Reproduction\\examples\\../src\\autoencoder.py:46\u001b[0m, in \u001b[0;36mFullNetwork.forward\u001b[1;34m(self, x, dx, ddx)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, dx, ddx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 46\u001b[0m     z, x_decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautoencoder(x)\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# Derivative computation\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_order \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ianma\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ianma\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ianma\\Desktop\\Study\\Python\\CS4240_DeepLearning_Group41_ReproducibilityProject\\Autoencoders-Reproduction\\examples\\../src\\autoencoder.py:148\u001b[0m, in \u001b[0;36mNonLinearAutoencoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 148\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[0;32m    149\u001b[0m     x_decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(z)\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m z, x_decode\n",
      "File \u001b[1;32mc:\\Users\\ianma\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ianma\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ianma\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ianma\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ianma\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ianma\\Desktop\\Study\\Python\\CS4240_DeepLearning_Group41_ReproducibilityProject\\Autoencoders-Reproduction\\examples\\../src\\autoencoder.py:129\u001b[0m, in \u001b[0;36mCustomLayer.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    127\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 129\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(x)\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\ianma\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:1981\u001b[0m, in \u001b[0;36msigmoid\u001b[1;34m(input)\u001b[0m\n\u001b[0;32m   1974\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msigmoid\u001b[39m(\u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m   1975\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"sigmoid(input) -> Tensor\u001b[39;00m\n\u001b[0;32m   1976\u001b[0m \n\u001b[0;32m   1977\u001b[0m \u001b[38;5;124;03m    Applies the element-wise function :math:`\\text{Sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}`\u001b[39;00m\n\u001b[0;32m   1978\u001b[0m \n\u001b[0;32m   1979\u001b[0m \u001b[38;5;124;03m    See :class:`~torch.nn.Sigmoid` for more details.\u001b[39;00m\n\u001b[0;32m   1980\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1981\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_experiments = 1\n",
    "df = pd.DataFrame()\n",
    "for i in range(num_experiments):\n",
    "    print('EXPERIMENT %d' % i)\n",
    "\n",
    "    params['coefficient_mask'] = np.ones((params['library_dim'], params['latent_dim']))\n",
    "\n",
    "    params['save_name'] = 'lorenz_' + datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S_%f\")\n",
    "    \n",
    "    results_dict = train_network(training_data, validation_data, params)\n",
    "    df = df.append({**results_dict, **params}, ignore_index=True)\n",
    "\n",
    "df.to_pickle('experiment_results_' + datetime.datetime.now().strftime(\"%Y%m%d%H%M\") + '.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
