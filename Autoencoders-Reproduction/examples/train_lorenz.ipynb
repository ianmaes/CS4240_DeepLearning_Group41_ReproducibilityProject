{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from example_lorenz import get_lorenz_data\n",
    "from sindy_utils import library_size\n",
    "from training import train_network\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate training, validation, testing data\n",
    "noise_strength = 1e-6\n",
    "training_data = get_lorenz_data(1024, noise_strength=noise_strength)\n",
    "validation_data = get_lorenz_data(20, noise_strength=noise_strength)\n",
    "training_data['x'] = torch.tensor(training_data['x']).float().to('cuda')\n",
    "training_data['dx'] = torch.tensor(training_data['dx']).float().to('cuda')\n",
    "training_data['t'] = torch.tensor(training_data['t']).float().to('cuda')\n",
    "training_data['z'] = torch.tensor(training_data['z']).float().to('cuda')\n",
    "training_data['dz'] = torch.tensor(training_data['dz']).float().to('cuda')\n",
    "training_data['ddx'] = None\n",
    "\n",
    "\n",
    "validation_data['x'] = torch.tensor(validation_data['x']).float().to('cuda')\n",
    "validation_data['dx'] = torch.tensor(validation_data['dx']).float().to('cuda')\n",
    "validation_data['t'] = torch.tensor(validation_data['t']).float().to('cuda')\n",
    "validation_data['z'] = torch.tensor(validation_data['z']).float().to('cuda')\n",
    "validation_data['dz'] = torch.tensor(validation_data['dz']).float().to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up model and training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 64]\n"
     ]
    }
   ],
   "source": [
    "params = {}\n",
    "\n",
    "params['input_dim'] = 128\n",
    "params['latent_dim'] = 3\n",
    "params['model_order'] = 1\n",
    "params['poly_order'] = 3\n",
    "params['include_sine'] = False\n",
    "params['library_dim'] = library_size(params['latent_dim'], params['poly_order'], params['include_sine'], True)\n",
    "\n",
    "# sequential thresholding parameters\n",
    "params['sequential_thresholding'] = True\n",
    "params['coefficient_threshold'] = 0.1\n",
    "params['threshold_frequency'] = 500\n",
    "params['coefficient_mask'] = torch.ones((params['library_dim'], params['latent_dim']))\n",
    "params['coefficient_initialization'] = 'constant'\n",
    "\n",
    "# loss function weighting\n",
    "params['loss_weight_decoder'] = 1.0\n",
    "params['loss_weight_sindy_z'] = 0\n",
    "params['loss_weight_sindy_x'] = 1e-4\n",
    "params['loss_weight_sindy_regularization'] = 1e-5\n",
    "\n",
    "params['activation'] = 'sigmoid'\n",
    "params['widths'] = [64,32]\n",
    "print(params['widths'][::-1])\n",
    "# training parameters\n",
    "params['epoch_size'] = training_data['x'].shape[0]\n",
    "params['batch_size'] = 1024\n",
    "params['learning_rate'] = 1e-3\n",
    "\n",
    "params['data_path'] = os.getcwd() + '/'\n",
    "params['print_progress'] = True\n",
    "params['print_frequency'] = 1\n",
    "\n",
    "# training time cutoffs\n",
    "params['max_epochs'] = 5001\n",
    "params['refinement_epochs'] = 1001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT 0\n",
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ianma\\Desktop\\Study\\Python\\CS4240_DeepLearning_Group41_ReproducibilityProject\\Autoencoders-Reproduction\\examples\\../src\\training.py:161: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'x': torch.tensor(data['x'][idxs], dtype=torch.float32).to(params['device']),\n",
      "c:\\Users\\ianma\\Desktop\\Study\\Python\\CS4240_DeepLearning_Group41_ReproducibilityProject\\Autoencoders-Reproduction\\examples\\../src\\training.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'dx': torch.tensor(data['dx'][idxs], dtype=torch.float32).to(params['device'])\n",
      "c:\\Users\\ianma\\Desktop\\Study\\Python\\CS4240_DeepLearning_Group41_ReproducibilityProject\\Autoencoders-Reproduction\\examples\\../src\\training.py:167: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  feed_dict['coefficient_mask'] = torch.tensor(params['coefficient_mask'], dtype=torch.float32).to(params['device'])\n",
      "c:\\Users\\ianma\\Desktop\\Study\\Python\\CS4240_DeepLearning_Group41_ReproducibilityProject\\Autoencoders-Reproduction\\examples\\../src\\training.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sindy_model_terms = [torch.sum(torch.tensor(params['coefficient_mask']))]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Epoch 0\n",
      "Epoch 0\n",
      "   Training Total Loss: 0.041392602026462555\n",
      "   Training decoder Loss: 0.039569005370140076\n",
      "   Training sindy_z Loss: 2898.18603515625\n",
      "   Training sindy_x Loss: 18.169336318969727\n",
      "   Training sindy_regularization Loss: 0.6664645671844482\n",
      "   Validation Total Loss: 0.04102460294961929\n",
      "   Validation decoder Loss: 0.039228081703186035\n",
      "   Validation sindy_z Loss: 3040.649169921875\n",
      "   Validation sindy_x Loss: 17.898569107055664\n",
      "   Validation sindy_regularization Loss: 0.6664645671844482\n",
      "Decoder Loss Ratio: 0.208162, Decoder SINDy Loss Ratio: 1.564686\n",
      "Epoch 1\n",
      "Epoch 1\n",
      "   Training Total Loss: 0.030414585024118423\n",
      "   Training decoder Loss: 0.02910543978214264\n",
      "   Training sindy_z Loss: 178.7343292236328\n",
      "   Training sindy_x Loss: 13.038774490356445\n",
      "   Training sindy_regularization Loss: 0.526718020439148\n",
      "   Validation Total Loss: 0.030080921947956085\n",
      "   Validation decoder Loss: 0.028837474063038826\n",
      "   Validation sindy_z Loss: 178.42562866210938\n",
      "   Validation sindy_x Loss: 12.38180160522461\n",
      "   Validation sindy_regularization Loss: 0.526718020439148\n",
      "Decoder Loss Ratio: 0.153025, Decoder SINDy Loss Ratio: 1.082413\n",
      "Epoch 2\n",
      "Epoch 2\n",
      "   Training Total Loss: 0.013293368741869926\n",
      "   Training decoder Loss: 0.012119225226342678\n",
      "   Training sindy_z Loss: 97.41983032226562\n",
      "   Training sindy_x Loss: 11.696297645568848\n",
      "   Training sindy_regularization Loss: 0.4513848125934601\n",
      "   Validation Total Loss: 0.012646335177123547\n",
      "   Validation decoder Loss: 0.011537650600075722\n",
      "   Validation sindy_z Loss: 94.36087799072266\n",
      "   Validation sindy_x Loss: 11.041705131530762\n",
      "   Validation sindy_regularization Loss: 0.4513848125934601\n",
      "Decoder Loss Ratio: 0.061224, Decoder SINDy Loss Ratio: 0.965262\n",
      "Epoch 3\n",
      "Epoch 3\n",
      "   Training Total Loss: 0.010347087867558002\n",
      "   Training decoder Loss: 0.009256651625037193\n",
      "   Training sindy_z Loss: 71.6446533203125\n",
      "   Training sindy_x Loss: 10.861143112182617\n",
      "   Training sindy_regularization Loss: 0.4322212338447571\n",
      "   Validation Total Loss: 0.009557802230119705\n",
      "   Validation decoder Loss: 0.008528471924364567\n",
      "   Validation sindy_z Loss: 65.227783203125\n",
      "   Validation sindy_x Loss: 10.250083923339844\n",
      "   Validation sindy_regularization Loss: 0.4322212338447571\n",
      "Decoder Loss Ratio: 0.045256, Decoder SINDy Loss Ratio: 0.896059\n",
      "Epoch 4\n",
      "Epoch 4\n",
      "   Training Total Loss: 0.009152103215456009\n",
      "   Training decoder Loss: 0.008139092475175858\n",
      "   Training sindy_z Loss: 55.4232292175293\n",
      "   Training sindy_x Loss: 10.083820343017578\n",
      "   Training sindy_regularization Loss: 0.4628309905529022\n",
      "   Validation Total Loss: 0.008386224508285522\n",
      "   Validation decoder Loss: 0.007421577349305153\n",
      "   Validation sindy_z Loss: 54.652645111083984\n",
      "   Validation sindy_x Loss: 9.60018253326416\n",
      "   Validation sindy_regularization Loss: 0.4628309905529022\n",
      "Decoder Loss Ratio: 0.039382, Decoder SINDy Loss Ratio: 0.839244\n",
      "Epoch 5\n",
      "Epoch 5\n",
      "   Training Total Loss: 0.008224323391914368\n",
      "   Training decoder Loss: 0.00728152971714735\n",
      "   Training sindy_z Loss: 46.731468200683594\n",
      "   Training sindy_x Loss: 9.377925872802734\n",
      "   Training sindy_regularization Loss: 0.5001339316368103\n",
      "   Validation Total Loss: 0.0074891396798193455\n",
      "   Validation decoder Loss: 0.0065805548802018166\n",
      "   Validation sindy_z Loss: 47.97507095336914\n",
      "   Validation sindy_x Loss: 9.03583812713623\n",
      "   Validation sindy_regularization Loss: 0.5001339316368103\n",
      "Decoder Loss Ratio: 0.034919, Decoder SINDy Loss Ratio: 0.789910\n",
      "Epoch 6\n",
      "Epoch 6\n",
      "   Training Total Loss: 0.007456534542143345\n",
      "   Training decoder Loss: 0.006594895385205746\n",
      "   Training sindy_z Loss: 39.05149459838867\n",
      "   Training sindy_x Loss: 8.561627388000488\n",
      "   Training sindy_regularization Loss: 0.5476741790771484\n",
      "   Validation Total Loss: 0.006743818521499634\n",
      "   Validation decoder Loss: 0.005899860057979822\n",
      "   Validation sindy_z Loss: 41.1567497253418\n",
      "   Validation sindy_x Loss: 8.384818077087402\n",
      "   Validation sindy_regularization Loss: 0.5476741790771484\n",
      "Decoder Loss Ratio: 0.031307, Decoder SINDy Loss Ratio: 0.732998\n",
      "Epoch 7\n",
      "Epoch 7\n",
      "   Training Total Loss: 0.006827980279922485\n",
      "   Training decoder Loss: 0.006071789655834436\n",
      "   Training sindy_z Loss: 30.137561798095703\n",
      "   Training sindy_x Loss: 7.502054691314697\n",
      "   Training sindy_regularization Loss: 0.5985111594200134\n",
      "   Validation Total Loss: 0.0061411503702402115\n",
      "   Validation decoder Loss: 0.005379156209528446\n",
      "   Validation sindy_z Loss: 33.59281539916992\n",
      "   Validation sindy_x Loss: 7.5600905418396\n",
      "   Validation sindy_regularization Loss: 0.5985111594200134\n",
      "Decoder Loss Ratio: 0.028544, Decoder SINDy Loss Ratio: 0.660900\n",
      "Epoch 8\n",
      "Epoch 8\n",
      "   Training Total Loss: 0.0061904871836304665\n",
      "   Training decoder Loss: 0.005531751085072756\n",
      "   Training sindy_z Loss: 23.022703170776367\n",
      "   Training sindy_x Loss: 6.522849082946777\n",
      "   Training sindy_regularization Loss: 0.6451210379600525\n",
      "   Validation Total Loss: 0.005556995049118996\n",
      "   Validation decoder Loss: 0.0048753563314676285\n",
      "   Validation sindy_z Loss: 29.117359161376953\n",
      "   Validation sindy_x Loss: 6.751876354217529\n",
      "   Validation sindy_regularization Loss: 0.6451210379600525\n",
      "Decoder Loss Ratio: 0.025871, Decoder SINDy Loss Ratio: 0.590247\n",
      "Epoch 9\n",
      "Epoch 9\n",
      "   Training Total Loss: 0.0039838096126914024\n",
      "   Training decoder Loss: 0.003394976258277893\n",
      "   Training sindy_z Loss: 24.07337760925293\n",
      "   Training sindy_x Loss: 5.820155620574951\n",
      "   Training sindy_regularization Loss: 0.6817624568939209\n",
      "   Validation Total Loss: 0.0035656585823744535\n",
      "   Validation decoder Loss: 0.002952185459434986\n",
      "   Validation sindy_z Loss: 30.50592803955078\n",
      "   Validation sindy_x Loss: 6.066555976867676\n",
      "   Validation sindy_regularization Loss: 0.6817624568939209\n",
      "Decoder Loss Ratio: 0.015666, Decoder SINDy Loss Ratio: 0.530336\n",
      "Epoch 10\n",
      "Epoch 10\n",
      "   Training Total Loss: 0.0024669314734637737\n",
      "   Training decoder Loss: 0.001967725809663534\n",
      "   Training sindy_z Loss: 19.299240112304688\n",
      "   Training sindy_x Loss: 4.920406818389893\n",
      "   Training sindy_regularization Loss: 0.7165063619613647\n",
      "   Validation Total Loss: 0.0022625159472227097\n",
      "   Validation decoder Loss: 0.0017043612897396088\n",
      "   Validation sindy_z Loss: 21.729249954223633\n",
      "   Validation sindy_x Loss: 5.509894847869873\n",
      "   Validation sindy_regularization Loss: 0.7165063619613647\n",
      "Decoder Loss Ratio: 0.009044, Decoder SINDy Loss Ratio: 0.481673\n",
      "Epoch 11\n",
      "Epoch 11\n",
      "   Training Total Loss: 0.0018887447658926249\n",
      "   Training decoder Loss: 0.001427429961040616\n",
      "   Training sindy_z Loss: 15.096957206726074\n",
      "   Training sindy_x Loss: 4.538754940032959\n",
      "   Training sindy_regularization Loss: 0.7439277768135071\n",
      "   Validation Total Loss: 0.0017852758755907416\n",
      "   Validation decoder Loss: 0.001247003092430532\n",
      "   Validation sindy_z Loss: 17.598833084106445\n",
      "   Validation sindy_x Loss: 5.3083343505859375\n",
      "   Validation sindy_regularization Loss: 0.7439277768135071\n",
      "Decoder Loss Ratio: 0.006617, Decoder SINDy Loss Ratio: 0.464053\n",
      "Epoch 12\n",
      "Epoch 12\n",
      "   Training Total Loss: 0.0015973265981301665\n",
      "   Training decoder Loss: 0.001163223059847951\n",
      "   Training sindy_z Loss: 12.790275573730469\n",
      "   Training sindy_x Loss: 4.2639241218566895\n",
      "   Training sindy_regularization Loss: 0.7711161971092224\n",
      "   Validation Total Loss: 0.0015480502042919397\n",
      "   Validation decoder Loss: 0.0010306535987183452\n",
      "   Validation sindy_z Loss: 15.01286792755127\n",
      "   Validation sindy_x Loss: 5.096855640411377\n",
      "   Validation sindy_regularization Loss: 0.7711161971092224\n",
      "Decoder Loss Ratio: 0.005469, Decoder SINDy Loss Ratio: 0.445565\n",
      "Epoch 13\n",
      "Epoch 13\n",
      "   Training Total Loss: 0.0014160256832838058\n",
      "   Training decoder Loss: 0.001010711072012782\n",
      "   Training sindy_z Loss: 11.012823104858398\n",
      "   Training sindy_x Loss: 3.9727087020874023\n",
      "   Training sindy_regularization Loss: 0.804370641708374\n",
      "   Validation Total Loss: 0.001397284329868853\n",
      "   Validation decoder Loss: 0.0009070956730283797\n",
      "   Validation sindy_z Loss: 12.860875129699707\n",
      "   Validation sindy_x Loss: 4.8214497566223145\n",
      "   Validation sindy_regularization Loss: 0.804370641708374\n",
      "Decoder Loss Ratio: 0.004813, Decoder SINDy Loss Ratio: 0.421489\n",
      "Epoch 14\n",
      "Epoch 14\n",
      "   Training Total Loss: 0.0012994562275707722\n",
      "   Training decoder Loss: 0.0009192730649374425\n",
      "   Training sindy_z Loss: 9.684757232666016\n",
      "   Training sindy_x Loss: 3.7174782752990723\n",
      "   Training sindy_regularization Loss: 0.8435391187667847\n",
      "   Validation Total Loss: 0.0012965811183676124\n",
      "   Validation decoder Loss: 0.0008327321847900748\n",
      "   Validation sindy_z Loss: 11.109025001525879\n",
      "   Validation sindy_x Loss: 4.554136753082275\n",
      "   Validation sindy_regularization Loss: 0.8435391187667847\n",
      "Decoder Loss Ratio: 0.004419, Decoder SINDy Loss Ratio: 0.398121\n",
      "Epoch 15\n",
      "Epoch 15\n",
      "   Training Total Loss: 0.0012257912894710898\n",
      "   Training decoder Loss: 0.0008670875104144216\n",
      "   Training sindy_z Loss: 8.70527458190918\n",
      "   Training sindy_x Loss: 3.4988763332366943\n",
      "   Training sindy_regularization Loss: 0.8816075325012207\n",
      "   Validation Total Loss: 0.001235723728314042\n",
      "   Validation decoder Loss: 0.0007952048326842487\n",
      "   Validation sindy_z Loss: 9.697781562805176\n",
      "   Validation sindy_x Loss: 4.317028045654297\n",
      "   Validation sindy_regularization Loss: 0.8816075325012207\n",
      "Decoder Loss Ratio: 0.004220, Decoder SINDy Loss Ratio: 0.377393\n",
      "Epoch 16\n",
      "Epoch 16\n",
      "   Training Total Loss: 0.0011485604336485267\n",
      "   Training decoder Loss: 0.0008059600368142128\n",
      "   Training sindy_z Loss: 7.927404880523682\n",
      "   Training sindy_x Loss: 3.3341403007507324\n",
      "   Training sindy_regularization Loss: 0.9186385273933411\n",
      "   Validation Total Loss: 0.0011643816251307726\n",
      "   Validation decoder Loss: 0.0007411143160425127\n",
      "   Validation sindy_z Loss: 8.660387992858887\n",
      "   Validation sindy_x Loss: 4.140810012817383\n",
      "   Validation sindy_regularization Loss: 0.9186385273933411\n",
      "Decoder Loss Ratio: 0.003933, Decoder SINDy Loss Ratio: 0.361988\n",
      "Epoch 17\n",
      "Epoch 17\n",
      "   Training Total Loss: 0.0010903466027230024\n",
      "   Training decoder Loss: 0.0007626393344253302\n",
      "   Training sindy_z Loss: 7.256087779998779\n",
      "   Training sindy_x Loss: 3.1814098358154297\n",
      "   Training sindy_regularization Loss: 0.956633448600769\n",
      "   Validation Total Loss: 0.0011130098719149828\n",
      "   Validation decoder Loss: 0.000706756953150034\n",
      "   Validation sindy_z Loss: 7.704753875732422\n",
      "   Validation sindy_x Loss: 3.9668655395507812\n",
      "   Validation sindy_regularization Loss: 0.956633448600769\n",
      "Decoder Loss Ratio: 0.003750, Decoder SINDy Loss Ratio: 0.346782\n",
      "Epoch 18\n",
      "Epoch 18\n",
      "   Training Total Loss: 0.0010441264603286982\n",
      "   Training decoder Loss: 0.0007303544552996755\n",
      "   Training sindy_z Loss: 6.707887172698975\n",
      "   Training sindy_x Loss: 3.0384950637817383\n",
      "   Training sindy_regularization Loss: 0.9922528862953186\n",
      "   Validation Total Loss: 0.0010712802177295089\n",
      "   Validation decoder Loss: 0.000681524514220655\n",
      "   Validation sindy_z Loss: 6.946739673614502\n",
      "   Validation sindy_x Loss: 3.7983312606811523\n",
      "   Validation sindy_regularization Loss: 0.9922528862953186\n",
      "Decoder Loss Ratio: 0.003616, Decoder SINDy Loss Ratio: 0.332049\n",
      "Epoch 19\n",
      "Epoch 19\n",
      "   Training Total Loss: 0.0010088412091135979\n",
      "   Training decoder Loss: 0.0007076039910316467\n",
      "   Training sindy_z Loss: 6.323765754699707\n",
      "   Training sindy_x Loss: 2.9098706245422363\n",
      "   Training sindy_regularization Loss: 1.0250164270401\n",
      "   Validation Total Loss: 0.001038524555042386\n",
      "   Validation decoder Loss: 0.0006640224019065499\n",
      "   Validation sindy_z Loss: 6.459242343902588\n",
      "   Validation sindy_x Loss: 3.6425201892852783\n",
      "   Validation sindy_regularization Loss: 1.0250164270401\n",
      "Decoder Loss Ratio: 0.003524, Decoder SINDy Loss Ratio: 0.318428\n",
      "Epoch 20\n",
      "Epoch 20\n",
      "   Training Total Loss: 0.0009659090428613126\n",
      "   Training decoder Loss: 0.000674665963742882\n",
      "   Training sindy_z Loss: 5.915572643280029\n",
      "   Training sindy_x Loss: 2.8067758083343506\n",
      "   Training sindy_regularization Loss: 1.0565505027770996\n",
      "   Validation Total Loss: 0.0009994899155572057\n",
      "   Validation decoder Loss: 0.0006365053122863173\n",
      "   Validation sindy_z Loss: 5.995734214782715\n",
      "   Validation sindy_x Loss: 3.524191379547119\n",
      "   Validation sindy_regularization Loss: 1.0565505027770996\n",
      "Decoder Loss Ratio: 0.003378, Decoder SINDy Loss Ratio: 0.308084\n",
      "Epoch 21\n",
      "Epoch 21\n",
      "   Training Total Loss: 0.0009339869720861316\n",
      "   Training decoder Loss: 0.0006520005990751088\n",
      "   Training sindy_z Loss: 5.557581424713135\n",
      "   Training sindy_x Loss: 2.711085081100464\n",
      "   Training sindy_regularization Loss: 1.0877845287322998\n",
      "   Validation Total Loss: 0.0009682497475296259\n",
      "   Validation decoder Loss: 0.0006173440488055348\n",
      "   Validation sindy_z Loss: 5.60662317276001\n",
      "   Validation sindy_x Loss: 3.4002788066864014\n",
      "   Validation sindy_regularization Loss: 1.0877845287322998\n",
      "Decoder Loss Ratio: 0.003276, Decoder SINDy Loss Ratio: 0.297251\n",
      "Epoch 22\n",
      "Epoch 22\n",
      "   Training Total Loss: 0.0009026406332850456\n",
      "   Training decoder Loss: 0.0006293767946772277\n",
      "   Training sindy_z Loss: 5.247440814971924\n",
      "   Training sindy_x Loss: 2.620882987976074\n",
      "   Training sindy_regularization Loss: 1.1175516843795776\n",
      "   Validation Total Loss: 0.0009374885121360421\n",
      "   Validation decoder Loss: 0.0005995997344143689\n",
      "   Validation sindy_z Loss: 5.271766662597656\n",
      "   Validation sindy_x Loss: 3.267132520675659\n",
      "   Validation sindy_regularization Loss: 1.1175516843795776\n",
      "Decoder Loss Ratio: 0.003182, Decoder SINDy Loss Ratio: 0.285612\n",
      "Epoch 23\n",
      "Epoch 23\n",
      "   Training Total Loss: 0.0008661827305331826\n",
      "   Training decoder Loss: 0.0006031592492945492\n",
      "   Training sindy_z Loss: 5.065949440002441\n",
      "   Training sindy_x Loss: 2.515695095062256\n",
      "   Training sindy_regularization Loss: 1.1453996896743774\n",
      "   Validation Total Loss: 0.0009041568846441805\n",
      "   Validation decoder Loss: 0.000577678089030087\n",
      "   Validation sindy_z Loss: 5.11802339553833\n",
      "   Validation sindy_x Loss: 3.1502480506896973\n",
      "   Validation sindy_regularization Loss: 1.1453996896743774\n",
      "Decoder Loss Ratio: 0.003065, Decoder SINDy Loss Ratio: 0.275394\n",
      "Epoch 24\n",
      "Epoch 24\n",
      "   Training Total Loss: 0.0008298977045342326\n",
      "   Training decoder Loss: 0.0005740545457229018\n",
      "   Training sindy_z Loss: 4.863842487335205\n",
      "   Training sindy_x Loss: 2.4412238597869873\n",
      "   Training sindy_regularization Loss: 1.1720787286758423\n",
      "   Validation Total Loss: 0.0008686675573699176\n",
      "   Validation decoder Loss: 0.0005533670773729682\n",
      "   Validation sindy_z Loss: 4.889804363250732\n",
      "   Validation sindy_x Loss: 3.035796880722046\n",
      "   Validation sindy_regularization Loss: 1.1720787286758423\n",
      "Decoder Loss Ratio: 0.002936, Decoder SINDy Loss Ratio: 0.265388\n",
      "Epoch 25\n",
      "Epoch 25\n",
      "   Training Total Loss: 0.000794945633970201\n",
      "   Training decoder Loss: 0.0005480003310367465\n",
      "   Training sindy_z Loss: 4.803097724914551\n",
      "   Training sindy_x Loss: 2.3497486114501953\n",
      "   Training sindy_regularization Loss: 1.1970449686050415\n",
      "   Validation Total Loss: 0.0008367654518224299\n",
      "   Validation decoder Loss: 0.0005321730277501047\n",
      "   Validation sindy_z Loss: 4.809129238128662\n",
      "   Validation sindy_x Loss: 2.926219940185547\n",
      "   Validation sindy_regularization Loss: 1.1970449686050415\n",
      "Decoder Loss Ratio: 0.002824, Decoder SINDy Loss Ratio: 0.255809\n",
      "Epoch 26\n",
      "Epoch 26\n",
      "   Training Total Loss: 0.0007528239511884749\n",
      "   Training decoder Loss: 0.0005130601348355412\n",
      "   Training sindy_z Loss: 4.756547451019287\n",
      "   Training sindy_x Loss: 2.275571346282959\n",
      "   Training sindy_regularization Loss: 1.2206681966781616\n",
      "   Validation Total Loss: 0.0007962209056131542\n",
      "   Validation decoder Loss: 0.0005018533556722105\n",
      "   Validation sindy_z Loss: 4.684998989105225\n",
      "   Validation sindy_x Loss: 2.8216092586517334\n",
      "   Validation sindy_regularization Loss: 1.2206681966781616\n",
      "Decoder Loss Ratio: 0.002663, Decoder SINDy Loss Ratio: 0.246664\n",
      "Epoch 27\n",
      "Epoch 27\n",
      "   Training Total Loss: 0.0007161783869378269\n",
      "   Training decoder Loss: 0.0004844512732233852\n",
      "   Training sindy_z Loss: 4.793375492095947\n",
      "   Training sindy_x Loss: 2.192869186401367\n",
      "   Training sindy_regularization Loss: 1.2440180778503418\n",
      "   Validation Total Loss: 0.0007617379887960851\n",
      "   Validation decoder Loss: 0.0004779733426403254\n",
      "   Validation sindy_z Loss: 4.58685827255249\n",
      "   Validation sindy_x Loss: 2.7132444381713867\n",
      "   Validation sindy_regularization Loss: 1.2440180778503418\n",
      "Decoder Loss Ratio: 0.002536, Decoder SINDy Loss Ratio: 0.237191\n",
      "Epoch 28\n",
      "Epoch 28\n",
      "   Training Total Loss: 0.0006707364809699357\n",
      "   Training decoder Loss: 0.00044627132592722774\n",
      "   Training sindy_z Loss: 4.820249080657959\n",
      "   Training sindy_x Loss: 2.1178534030914307\n",
      "   Training sindy_regularization Loss: 1.2679840326309204\n",
      "   Validation Total Loss: 0.000718595867510885\n",
      "   Validation decoder Loss: 0.000444768404122442\n",
      "   Validation sindy_z Loss: 4.572652339935303\n",
      "   Validation sindy_x Loss: 2.611476421356201\n",
      "   Validation sindy_regularization Loss: 1.2679840326309204\n",
      "Decoder Loss Ratio: 0.002360, Decoder SINDy Loss Ratio: 0.228294\n",
      "Epoch 29\n",
      "Epoch 29\n",
      "   Training Total Loss: 0.0006372841307893395\n",
      "   Training decoder Loss: 0.0004203181597404182\n",
      "   Training sindy_z Loss: 4.864172458648682\n",
      "   Training sindy_x Loss: 2.0406253337860107\n",
      "   Training sindy_regularization Loss: 1.2903501987457275\n",
      "   Validation Total Loss: 0.0006880097789689898\n",
      "   Validation decoder Loss: 0.00042448710883036256\n",
      "   Validation sindy_z Loss: 4.598572254180908\n",
      "   Validation sindy_x Loss: 2.506192445755005\n",
      "   Validation sindy_regularization Loss: 1.2903501987457275\n",
      "Decoder Loss Ratio: 0.002253, Decoder SINDy Loss Ratio: 0.219090\n",
      "Epoch 30\n",
      "Epoch 30\n",
      "   Training Total Loss: 0.0005828390712849796\n",
      "   Training decoder Loss: 0.0003742953122127801\n",
      "   Training sindy_z Loss: 4.886298656463623\n",
      "   Training sindy_x Loss: 1.9539512395858765\n",
      "   Training sindy_regularization Loss: 1.3148658275604248\n",
      "   Validation Total Loss: 0.0006337031372822821\n",
      "   Validation decoder Loss: 0.00038141850382089615\n",
      "   Validation sindy_z Loss: 4.608516216278076\n",
      "   Validation sindy_x Loss: 2.39136004447937\n",
      "   Validation sindy_regularization Loss: 1.3148658275604248\n",
      "Decoder Loss Ratio: 0.002024, Decoder SINDy Loss Ratio: 0.209052\n",
      "Epoch 31\n",
      "Epoch 31\n",
      "   Training Total Loss: 0.0005496377125382423\n",
      "   Training decoder Loss: 0.0003492140385787934\n",
      "   Training sindy_z Loss: 5.003091335296631\n",
      "   Training sindy_x Loss: 1.870459794998169\n",
      "   Training sindy_regularization Loss: 1.337769865989685\n",
      "   Validation Total Loss: 0.0006023321184329689\n",
      "   Validation decoder Loss: 0.0003608257102314383\n",
      "   Validation sindy_z Loss: 4.701279163360596\n",
      "   Validation sindy_x Loss: 2.2812869548797607\n",
      "   Validation sindy_regularization Loss: 1.337769865989685\n",
      "Decoder Loss Ratio: 0.001915, Decoder SINDy Loss Ratio: 0.199429\n",
      "Epoch 32\n",
      "Epoch 32\n",
      "   Training Total Loss: 0.0004987670690752566\n",
      "   Training decoder Loss: 0.0003059325972571969\n",
      "   Training sindy_z Loss: 4.904337406158447\n",
      "   Training sindy_x Loss: 1.7921475172042847\n",
      "   Training sindy_regularization Loss: 1.3619741201400757\n",
      "   Validation Total Loss: 0.0005494785727933049\n",
      "   Validation decoder Loss: 0.0003176610334776342\n",
      "   Validation sindy_z Loss: 4.625545024871826\n",
      "   Validation sindy_x Loss: 2.181978464126587\n",
      "   Validation sindy_regularization Loss: 1.3619741201400757\n",
      "Decoder Loss Ratio: 0.001686, Decoder SINDy Loss Ratio: 0.190748\n",
      "Epoch 33\n",
      "Epoch 33\n",
      "   Training Total Loss: 0.00046607854892499745\n",
      "   Training decoder Loss: 0.0002810332225635648\n",
      "   Training sindy_z Loss: 4.813379287719727\n",
      "   Training sindy_x Loss: 1.7118428945541382\n",
      "   Training sindy_regularization Loss: 1.3861061334609985\n",
      "   Validation Total Loss: 0.0005172556848265231\n",
      "   Validation decoder Loss: 0.000295278470730409\n",
      "   Validation sindy_z Loss: 4.538968086242676\n",
      "   Validation sindy_x Loss: 2.0811617374420166\n",
      "   Validation sindy_regularization Loss: 1.3861061334609985\n",
      "Decoder Loss Ratio: 0.001567, Decoder SINDy Loss Ratio: 0.181934\n",
      "Epoch 34\n",
      "Epoch 34\n",
      "   Training Total Loss: 0.00045952253276482224\n",
      "   Training decoder Loss: 0.0002816090709529817\n",
      "   Training sindy_z Loss: 4.660959243774414\n",
      "   Training sindy_x Loss: 1.6381860971450806\n",
      "   Training sindy_regularization Loss: 1.4094878435134888\n",
      "   Validation Total Loss: 0.0005087723257020116\n",
      "   Validation decoder Loss: 0.00029661122243851423\n",
      "   Validation sindy_z Loss: 4.396931171417236\n",
      "   Validation sindy_x Loss: 1.98066246509552\n",
      "   Validation sindy_regularization Loss: 1.4094878435134888\n",
      "Decoder Loss Ratio: 0.001574, Decoder SINDy Loss Ratio: 0.173149\n",
      "Epoch 35\n",
      "Epoch 35\n",
      "   Training Total Loss: 0.0004076839832123369\n",
      "   Training decoder Loss: 0.00023699263692833483\n",
      "   Training sindy_z Loss: 4.5011796951293945\n",
      "   Training sindy_x Loss: 1.5637056827545166\n",
      "   Training sindy_regularization Loss: 1.4320815801620483\n",
      "   Validation Total Loss: 0.0004575277853291482\n",
      "   Validation decoder Loss: 0.0002540349087212235\n",
      "   Validation sindy_z Loss: 4.189517974853516\n",
      "   Validation sindy_x Loss: 1.8917210102081299\n",
      "   Validation sindy_regularization Loss: 1.4320815801620483\n",
      "Decoder Loss Ratio: 0.001348, Decoder SINDy Loss Ratio: 0.165374\n",
      "Epoch 36\n",
      "Epoch 36\n",
      "   Training Total Loss: 0.00037068501114845276\n",
      "   Training decoder Loss: 0.00020740431500598788\n",
      "   Training sindy_z Loss: 4.458435535430908\n",
      "   Training sindy_x Loss: 1.4874415397644043\n",
      "   Training sindy_regularization Loss: 1.4536539316177368\n",
      "   Validation Total Loss: 0.0004206496523693204\n",
      "   Validation decoder Loss: 0.00022475131845567375\n",
      "   Validation sindy_z Loss: 4.113340854644775\n",
      "   Validation sindy_x Loss: 1.8136179447174072\n",
      "   Validation sindy_regularization Loss: 1.4536539316177368\n",
      "Decoder Loss Ratio: 0.001193, Decoder SINDy Loss Ratio: 0.158546\n",
      "Epoch 37\n",
      "Epoch 37\n",
      "   Training Total Loss: 0.0003470805531833321\n",
      "   Training decoder Loss: 0.00019167899154126644\n",
      "   Training sindy_z Loss: 4.264710903167725\n",
      "   Training sindy_x Loss: 1.4064090251922607\n",
      "   Training sindy_regularization Loss: 1.4760687351226807\n",
      "   Validation Total Loss: 0.00039680770714767277\n",
      "   Validation decoder Loss: 0.00021028115588705987\n",
      "   Validation sindy_z Loss: 3.814450263977051\n",
      "   Validation sindy_x Loss: 1.7176589965820312\n",
      "   Validation sindy_regularization Loss: 1.4760687351226807\n",
      "Decoder Loss Ratio: 0.001116, Decoder SINDy Loss Ratio: 0.150157\n",
      "Epoch 38\n",
      "Epoch 38\n",
      "   Training Total Loss: 0.00037391463411040604\n",
      "   Training decoder Loss: 0.00022268775501288474\n",
      "   Training sindy_z Loss: 4.042675018310547\n",
      "   Training sindy_x Loss: 1.3624459505081177\n",
      "   Training sindy_regularization Loss: 1.498226523399353\n",
      "   Validation Total Loss: 0.00041990974568761885\n",
      "   Validation decoder Loss: 0.00023923425760585815\n",
      "   Validation sindy_z Loss: 3.557581901550293\n",
      "   Validation sindy_x Loss: 1.6569323539733887\n",
      "   Validation sindy_regularization Loss: 1.498226523399353\n",
      "Decoder Loss Ratio: 0.001269, Decoder SINDy Loss Ratio: 0.144848\n",
      "Epoch 39\n",
      "Epoch 39\n",
      "   Training Total Loss: 0.0003039886069018394\n",
      "   Training decoder Loss: 0.0001596547372173518\n",
      "   Training sindy_z Loss: 3.965606689453125\n",
      "   Training sindy_x Loss: 1.2914735078811646\n",
      "   Training sindy_regularization Loss: 1.5186526775360107\n",
      "   Validation Total Loss: 0.0003515079151839018\n",
      "   Validation decoder Loss: 0.0001771872048266232\n",
      "   Validation sindy_z Loss: 3.429983377456665\n",
      "   Validation sindy_x Loss: 1.5913418531417847\n",
      "   Validation sindy_regularization Loss: 1.5186526775360107\n",
      "Decoder Loss Ratio: 0.000940, Decoder SINDy Loss Ratio: 0.139115\n",
      "Epoch 40\n",
      "Epoch 40\n",
      "   Training Total Loss: 0.00029176531825214624\n",
      "   Training decoder Loss: 0.00015280650404747576\n",
      "   Training sindy_z Loss: 3.800839900970459\n",
      "   Training sindy_x Loss: 1.2355984449386597\n",
      "   Training sindy_regularization Loss: 1.539894938468933\n",
      "   Validation Total Loss: 0.0003377100802026689\n",
      "   Validation decoder Loss: 0.00016947889525908977\n",
      "   Validation sindy_z Loss: 3.198446750640869\n",
      "   Validation sindy_x Loss: 1.5283223390579224\n",
      "   Validation sindy_regularization Loss: 1.539894938468933\n",
      "Decoder Loss Ratio: 0.000899, Decoder SINDy Loss Ratio: 0.133605\n",
      "Epoch 41\n",
      "Epoch 41\n",
      "   Training Total Loss: 0.0002960052224807441\n",
      "   Training decoder Loss: 0.00016086615505628288\n",
      "   Training sindy_z Loss: 3.5424892902374268\n",
      "   Training sindy_x Loss: 1.1953459978103638\n",
      "   Training sindy_regularization Loss: 1.5604480504989624\n",
      "   Validation Total Loss: 0.0003394790692254901\n",
      "   Validation decoder Loss: 0.00017717198352329433\n",
      "   Validation sindy_z Loss: 2.8949317932128906\n",
      "   Validation sindy_x Loss: 1.4670261144638062\n",
      "   Validation sindy_regularization Loss: 1.5604480504989624\n",
      "Decoder Loss Ratio: 0.000940, Decoder SINDy Loss Ratio: 0.128247\n",
      "Epoch 42\n",
      "Epoch 42\n",
      "   Training Total Loss: 0.0002634613192640245\n",
      "   Training decoder Loss: 0.0001347675424767658\n",
      "   Training sindy_z Loss: 3.457357406616211\n",
      "   Training sindy_x Loss: 1.1289931535720825\n",
      "   Training sindy_regularization Loss: 1.5794461965560913\n",
      "   Validation Total Loss: 0.00030762096866965294\n",
      "   Validation decoder Loss: 0.0001517127238912508\n",
      "   Validation sindy_z Loss: 2.799603223800659\n",
      "   Validation sindy_x Loss: 1.4011379480361938\n",
      "   Validation sindy_regularization Loss: 1.5794461965560913\n",
      "Decoder Loss Ratio: 0.000805, Decoder SINDy Loss Ratio: 0.122487\n",
      "Epoch 43\n",
      "Epoch 43\n",
      "   Training Total Loss: 0.0002738668699748814\n",
      "   Training decoder Loss: 0.0001481565850554034\n",
      "   Training sindy_z Loss: 3.229008674621582\n",
      "   Training sindy_x Loss: 1.097276210784912\n",
      "   Training sindy_regularization Loss: 1.5982667207717896\n",
      "   Validation Total Loss: 0.00031517865136265755\n",
      "   Validation decoder Loss: 0.00016387207142543048\n",
      "   Validation sindy_z Loss: 2.5695714950561523\n",
      "   Validation sindy_x Loss: 1.3532391786575317\n",
      "   Validation sindy_regularization Loss: 1.5982667207717896\n",
      "Decoder Loss Ratio: 0.000870, Decoder SINDy Loss Ratio: 0.118300\n",
      "Epoch 44\n",
      "Epoch 44\n",
      "   Training Total Loss: 0.00024202860367950052\n",
      "   Training decoder Loss: 0.0001215106385643594\n",
      "   Training sindy_z Loss: 3.1460957527160645\n",
      "   Training sindy_x Loss: 1.0436092615127563\n",
      "   Training sindy_regularization Loss: 1.6157034635543823\n",
      "   Validation Total Loss: 0.0002831611782312393\n",
      "   Validation decoder Loss: 0.00013636834046337754\n",
      "   Validation sindy_z Loss: 2.518421173095703\n",
      "   Validation sindy_x Loss: 1.3063582181930542\n",
      "   Validation sindy_regularization Loss: 1.6157034635543823\n",
      "Decoder Loss Ratio: 0.000724, Decoder SINDy Loss Ratio: 0.114201\n",
      "Epoch 45\n",
      "Epoch 45\n",
      "   Training Total Loss: 0.00026985156000591815\n",
      "   Training decoder Loss: 0.0001523172395536676\n",
      "   Training sindy_z Loss: 2.9332571029663086\n",
      "   Training sindy_x Loss: 1.012101650238037\n",
      "   Training sindy_regularization Loss: 1.6324151754379272\n",
      "   Validation Total Loss: 0.00030866998713463545\n",
      "   Validation decoder Loss: 0.00016725763271097094\n",
      "   Validation sindy_z Loss: 2.30886173248291\n",
      "   Validation sindy_x Loss: 1.2508817911148071\n",
      "   Validation sindy_regularization Loss: 1.6324151754379272\n",
      "Decoder Loss Ratio: 0.000888, Decoder SINDy Loss Ratio: 0.109352\n",
      "Epoch 46\n",
      "Epoch 46\n",
      "   Training Total Loss: 0.0002257780870422721\n",
      "   Training decoder Loss: 0.00011250645184190944\n",
      "   Training sindy_z Loss: 2.806194543838501\n",
      "   Training sindy_x Loss: 0.967876672744751\n",
      "   Training sindy_regularization Loss: 1.6483975648880005\n",
      "   Validation Total Loss: 0.00026486426941119134\n",
      "   Validation decoder Loss: 0.00012796479859389365\n",
      "   Validation sindy_z Loss: 2.210787296295166\n",
      "   Validation sindy_x Loss: 1.2041550874710083\n",
      "   Validation sindy_regularization Loss: 1.6483975648880005\n",
      "Decoder Loss Ratio: 0.000679, Decoder SINDy Loss Ratio: 0.105267\n",
      "Epoch 47\n",
      "Epoch 47\n",
      "   Training Total Loss: 0.0002253552374895662\n",
      "   Training decoder Loss: 0.00011562628787942231\n",
      "   Training sindy_z Loss: 2.671035051345825\n",
      "   Training sindy_x Loss: 0.9308763742446899\n",
      "   Training sindy_regularization Loss: 1.664130687713623\n",
      "   Validation Total Loss: 0.00026377217727713287\n",
      "   Validation decoder Loss: 0.00013054122973699123\n",
      "   Validation sindy_z Loss: 2.1044793128967285\n",
      "   Validation sindy_x Loss: 1.1658962965011597\n",
      "   Validation sindy_regularization Loss: 1.664130687713623\n",
      "Decoder Loss Ratio: 0.000693, Decoder SINDy Loss Ratio: 0.101922\n",
      "Epoch 48\n",
      "Epoch 48\n",
      "   Training Total Loss: 0.0002503723662812263\n",
      "   Training decoder Loss: 0.0001441770582459867\n",
      "   Training sindy_z Loss: 2.5479583740234375\n",
      "   Training sindy_x Loss: 0.8940778374671936\n",
      "   Training sindy_regularization Loss: 1.6787532567977905\n",
      "   Validation Total Loss: 0.00028949009720236063\n",
      "   Validation decoder Loss: 0.000160950658028014\n",
      "   Validation sindy_z Loss: 2.013148307800293\n",
      "   Validation sindy_x Loss: 1.1175192594528198\n",
      "   Validation sindy_regularization Loss: 1.6787532567977905\n",
      "Decoder Loss Ratio: 0.000854, Decoder SINDy Loss Ratio: 0.097693\n",
      "Epoch 49\n",
      "Epoch 49\n",
      "   Training Total Loss: 0.0002030555478995666\n",
      "   Training decoder Loss: 0.0001007965111057274\n",
      "   Training sindy_z Loss: 2.5071756839752197\n",
      "   Training sindy_x Loss: 0.8534574508666992\n",
      "   Training sindy_regularization Loss: 1.6913294792175293\n",
      "   Validation Total Loss: 0.00024161238980013877\n",
      "   Validation decoder Loss: 0.00011579434794839472\n",
      "   Validation sindy_z Loss: 2.042034864425659\n",
      "   Validation sindy_x Loss: 1.0890475511550903\n",
      "   Validation sindy_regularization Loss: 1.6913294792175293\n",
      "Decoder Loss Ratio: 0.000614, Decoder SINDy Loss Ratio: 0.095204\n",
      "Epoch 50\n",
      "Epoch 50\n",
      "   Training Total Loss: 0.0001942479284480214\n",
      "   Training decoder Loss: 9.3472859589383e-05\n",
      "   Training sindy_z Loss: 2.3555610179901123\n",
      "   Training sindy_x Loss: 0.8372422456741333\n",
      "   Training sindy_regularization Loss: 1.705085039138794\n",
      "   Validation Total Loss: 0.0002305934322066605\n",
      "   Validation decoder Loss: 0.00010801265307236463\n",
      "   Validation sindy_z Loss: 1.886783242225647\n",
      "   Validation sindy_x Loss: 1.0552994012832642\n",
      "   Validation sindy_regularization Loss: 1.705085039138794\n",
      "Decoder Loss Ratio: 0.000573, Decoder SINDy Loss Ratio: 0.092254\n",
      "Epoch 51\n",
      "Epoch 51\n",
      "   Training Total Loss: 0.00022142006491776556\n",
      "   Training decoder Loss: 0.00012275126937311143\n",
      "   Training sindy_z Loss: 2.233588933944702\n",
      "   Training sindy_x Loss: 0.814693033695221\n",
      "   Training sindy_regularization Loss: 1.71994948387146\n",
      "   Validation Total Loss: 0.00025661668041720986\n",
      "   Validation decoder Loss: 0.00013783654139842838\n",
      "   Validation sindy_z Loss: 1.776854395866394\n",
      "   Validation sindy_x Loss: 1.0158065557479858\n",
      "   Validation sindy_regularization Loss: 1.71994948387146\n",
      "Decoder Loss Ratio: 0.000731, Decoder SINDy Loss Ratio: 0.088801\n",
      "Epoch 52\n",
      "Epoch 52\n",
      "   Training Total Loss: 0.0001826319785322994\n",
      "   Training decoder Loss: 8.670458919368684e-05\n",
      "   Training sindy_z Loss: 2.161919593811035\n",
      "   Training sindy_x Loss: 0.785862922668457\n",
      "   Training sindy_regularization Loss: 1.7341104745864868\n",
      "   Validation Total Loss: 0.00021786801517009735\n",
      "   Validation decoder Loss: 0.00010163024126086384\n",
      "   Validation sindy_z Loss: 1.7414817810058594\n",
      "   Validation sindy_x Loss: 0.9889667630195618\n",
      "   Validation sindy_regularization Loss: 1.7341104745864868\n",
      "Decoder Loss Ratio: 0.000539, Decoder SINDy Loss Ratio: 0.086455\n",
      "Epoch 53\n",
      "Epoch 53\n",
      "   Training Total Loss: 0.000179573122295551\n",
      "   Training decoder Loss: 8.578284905524924e-05\n",
      "   Training sindy_z Loss: 2.102541446685791\n",
      "   Training sindy_x Loss: 0.7631874084472656\n",
      "   Training sindy_regularization Loss: 1.7471532821655273\n",
      "   Validation Total Loss: 0.0002150567452190444\n",
      "   Validation decoder Loss: 0.00010021798516390845\n",
      "   Validation sindy_z Loss: 1.7208302021026611\n",
      "   Validation sindy_x Loss: 0.9736722707748413\n",
      "   Validation sindy_regularization Loss: 1.7471532821655273\n",
      "Decoder Loss Ratio: 0.000532, Decoder SINDy Loss Ratio: 0.085118\n",
      "Epoch 54\n",
      "Epoch 54\n",
      "   Training Total Loss: 0.0001979190274141729\n",
      "   Training decoder Loss: 0.00010743604798335582\n",
      "   Training sindy_z Loss: 2.0152719020843506\n",
      "   Training sindy_x Loss: 0.7288938760757446\n",
      "   Training sindy_regularization Loss: 1.7593581676483154\n",
      "   Validation Total Loss: 0.00023112469352781773\n",
      "   Validation decoder Loss: 0.00011991986684734002\n",
      "   Validation sindy_z Loss: 1.6526693105697632\n",
      "   Validation sindy_x Loss: 0.9361124038696289\n",
      "   Validation sindy_regularization Loss: 1.7593581676483154\n",
      "Decoder Loss Ratio: 0.000636, Decoder SINDy Loss Ratio: 0.081835\n",
      "Epoch 55\n",
      "Epoch 55\n",
      "   Training Total Loss: 0.0001988634467124939\n",
      "   Training decoder Loss: 0.00010885754454648122\n",
      "   Training sindy_z Loss: 1.9005494117736816\n",
      "   Training sindy_x Loss: 0.7230013608932495\n",
      "   Training sindy_regularization Loss: 1.7705769538879395\n",
      "   Validation Total Loss: 0.00023197969130706042\n",
      "   Validation decoder Loss: 0.0001240875426447019\n",
      "   Validation sindy_z Loss: 1.532089352607727\n",
      "   Validation sindy_x Loss: 0.9018638730049133\n",
      "   Validation sindy_regularization Loss: 1.7705769538879395\n",
      "Decoder Loss Ratio: 0.000658, Decoder SINDy Loss Ratio: 0.078841\n",
      "Epoch 56\n",
      "Epoch 56\n",
      "   Training Total Loss: 0.0001669961930019781\n",
      "   Training decoder Loss: 7.926820399006829e-05\n",
      "   Training sindy_z Loss: 1.8494462966918945\n",
      "   Training sindy_x Loss: 0.6990736126899719\n",
      "   Training sindy_regularization Loss: 1.7820645570755005\n",
      "   Validation Total Loss: 0.00020108143507968634\n",
      "   Validation decoder Loss: 9.498856525169685e-05\n",
      "   Validation sindy_z Loss: 1.5112351179122925\n",
      "   Validation sindy_x Loss: 0.8827224373817444\n",
      "   Validation sindy_regularization Loss: 1.7820645570755005\n",
      "Decoder Loss Ratio: 0.000504, Decoder SINDy Loss Ratio: 0.077167\n",
      "Epoch 57\n",
      "Epoch 57\n",
      "   Training Total Loss: 0.00017149235645774752\n",
      "   Training decoder Loss: 8.674626587890089e-05\n",
      "   Training sindy_z Loss: 1.8150758743286133\n",
      "   Training sindy_x Loss: 0.6681455969810486\n",
      "   Training sindy_regularization Loss: 1.7931522130966187\n",
      "   Validation Total Loss: 0.0002048946189461276\n",
      "   Validation decoder Loss: 0.00010089970601256937\n",
      "   Validation sindy_z Loss: 1.5100456476211548\n",
      "   Validation sindy_x Loss: 0.8606338500976562\n",
      "   Validation sindy_regularization Loss: 1.7931522130966187\n",
      "Decoder Loss Ratio: 0.000535, Decoder SINDy Loss Ratio: 0.075236\n",
      "Epoch 58\n",
      "Epoch 58\n",
      "   Training Total Loss: 0.0001627405290491879\n",
      "   Training decoder Loss: 7.864244980737567e-05\n",
      "   Training sindy_z Loss: 1.724488615989685\n",
      "   Training sindy_x Loss: 0.6606185436248779\n",
      "   Training sindy_regularization Loss: 1.803622841835022\n",
      "   Validation Total Loss: 0.0001965945994015783\n",
      "   Validation decoder Loss: 9.483435860602185e-05\n",
      "   Validation sindy_z Loss: 1.4202476739883423\n",
      "   Validation sindy_x Loss: 0.8372402191162109\n",
      "   Validation sindy_regularization Loss: 1.803622841835022\n",
      "Decoder Loss Ratio: 0.000503, Decoder SINDy Loss Ratio: 0.073191\n",
      "Epoch 59\n",
      "Epoch 59\n",
      "   Training Total Loss: 0.00024557553115300834\n",
      "   Training decoder Loss: 0.00016287941252812743\n",
      "   Training sindy_z Loss: 1.697627305984497\n",
      "   Training sindy_x Loss: 0.6455886363983154\n",
      "   Training sindy_regularization Loss: 1.8137255907058716\n",
      "   Validation Total Loss: 0.0002780815993901342\n",
      "   Validation decoder Loss: 0.00017614016542211175\n",
      "   Validation sindy_z Loss: 1.4344276189804077\n",
      "   Validation sindy_x Loss: 0.8380419611930847\n",
      "   Validation sindy_regularization Loss: 1.8137255907058716\n",
      "Decoder Loss Ratio: 0.000935, Decoder SINDy Loss Ratio: 0.073261\n",
      "Epoch 60\n",
      "Epoch 60\n",
      "   Training Total Loss: 0.00015405763406306505\n",
      "   Training decoder Loss: 7.262582948897034e-05\n",
      "   Training sindy_z Loss: 1.630768895149231\n",
      "   Training sindy_x Loss: 0.6320657134056091\n",
      "   Training sindy_regularization Loss: 1.8225229978561401\n",
      "   Validation Total Loss: 0.0001872640277724713\n",
      "   Validation decoder Loss: 8.881656685844064e-05\n",
      "   Validation sindy_z Loss: 1.3594028949737549\n",
      "   Validation sindy_x Loss: 0.8022223711013794\n",
      "   Validation sindy_regularization Loss: 1.8225229978561401\n",
      "Decoder Loss Ratio: 0.000471, Decoder SINDy Loss Ratio: 0.070130\n",
      "Epoch 61\n",
      "Epoch 61\n",
      "   Training Total Loss: 0.00015861520660109818\n",
      "   Training decoder Loss: 7.875490700826049e-05\n",
      "   Training sindy_z Loss: 1.562157392501831\n",
      "   Training sindy_x Loss: 0.6154346466064453\n",
      "   Training sindy_regularization Loss: 1.8316839933395386\n",
      "   Validation Total Loss: 0.0001918342022690922\n",
      "   Validation decoder Loss: 9.560577746015042e-05\n",
      "   Validation sindy_z Loss: 1.3001346588134766\n",
      "   Validation sindy_x Loss: 0.7791158556938171\n",
      "   Validation sindy_regularization Loss: 1.8316839933395386\n",
      "Decoder Loss Ratio: 0.000507, Decoder SINDy Loss Ratio: 0.068110\n",
      "Epoch 62\n",
      "Epoch 62\n",
      "   Training Total Loss: 0.00015191589773166925\n",
      "   Training decoder Loss: 7.357865251833573e-05\n",
      "   Training sindy_z Loss: 1.5081804990768433\n",
      "   Training sindy_x Loss: 0.5994511842727661\n",
      "   Training sindy_regularization Loss: 1.8392126560211182\n",
      "   Validation Total Loss: 0.00018584726785775274\n",
      "   Validation decoder Loss: 9.124264761339873e-05\n",
      "   Validation sindy_z Loss: 1.271346092224121\n",
      "   Validation sindy_x Loss: 0.7621250748634338\n",
      "   Validation sindy_regularization Loss: 1.8392126560211182\n",
      "Decoder Loss Ratio: 0.000484, Decoder SINDy Loss Ratio: 0.066625\n",
      "Epoch 63\n",
      "Epoch 63\n",
      "   Training Total Loss: 0.00014533694775309414\n",
      "   Training decoder Loss: 6.787625898141414e-05\n",
      "   Training sindy_z Loss: 1.5024696588516235\n",
      "   Training sindy_x Loss: 0.5898829698562622\n",
      "   Training sindy_regularization Loss: 1.847238540649414\n",
      "   Validation Total Loss: 0.0001781631726771593\n",
      "   Validation decoder Loss: 8.426931890426204e-05\n",
      "   Validation sindy_z Loss: 1.2788114547729492\n",
      "   Validation sindy_x Loss: 0.7542146444320679\n",
      "   Validation sindy_regularization Loss: 1.847238540649414\n",
      "Decoder Loss Ratio: 0.000447, Decoder SINDy Loss Ratio: 0.065933\n",
      "Epoch 64\n",
      "Epoch 64\n",
      "   Training Total Loss: 0.00014579253911506385\n",
      "   Training decoder Loss: 6.95908602210693e-05\n",
      "   Training sindy_z Loss: 1.4606249332427979\n",
      "   Training sindy_x Loss: 0.5764421820640564\n",
      "   Training sindy_regularization Loss: 1.855745553970337\n",
      "   Validation Total Loss: 0.00017878164362628013\n",
      "   Validation decoder Loss: 8.626381895737723e-05\n",
      "   Validation sindy_z Loss: 1.2510499954223633\n",
      "   Validation sindy_x Loss: 0.739603579044342\n",
      "   Validation sindy_regularization Loss: 1.855745553970337\n",
      "Decoder Loss Ratio: 0.000458, Decoder SINDy Loss Ratio: 0.064656\n",
      "Epoch 65\n",
      "Epoch 65\n",
      "   Training Total Loss: 0.00014226262283045799\n",
      "   Training decoder Loss: 6.695263436995447e-05\n",
      "   Training sindy_z Loss: 1.4244346618652344\n",
      "   Training sindy_x Loss: 0.5667318105697632\n",
      "   Training sindy_regularization Loss: 1.8636810779571533\n",
      "   Validation Total Loss: 0.00017516524530947208\n",
      "   Validation decoder Loss: 8.37468687677756e-05\n",
      "   Validation sindy_z Loss: 1.227820873260498\n",
      "   Validation sindy_x Loss: 0.7278156280517578\n",
      "   Validation sindy_regularization Loss: 1.8636810779571533\n",
      "Decoder Loss Ratio: 0.000444, Decoder SINDy Loss Ratio: 0.063625\n",
      "Epoch 66\n",
      "Epoch 66\n",
      "   Training Total Loss: 0.00016474896983709186\n",
      "   Training decoder Loss: 9.05496854102239e-05\n",
      "   Training sindy_z Loss: 1.3955786228179932\n",
      "   Training sindy_x Loss: 0.5549118518829346\n",
      "   Training sindy_regularization Loss: 1.8708099126815796\n",
      "   Validation Total Loss: 0.00019743744633160532\n",
      "   Validation decoder Loss: 0.00010657122766133398\n",
      "   Validation sindy_z Loss: 1.2179036140441895\n",
      "   Validation sindy_x Loss: 0.7215811610221863\n",
      "   Validation sindy_regularization Loss: 1.8708099126815796\n",
      "Decoder Loss Ratio: 0.000566, Decoder SINDy Loss Ratio: 0.063080\n",
      "Epoch 67\n",
      "Epoch 67\n",
      "   Training Total Loss: 0.0001939141657203436\n",
      "   Training decoder Loss: 0.00012085639900760725\n",
      "   Training sindy_z Loss: 1.3644341230392456\n",
      "   Training sindy_x Loss: 0.5428383946418762\n",
      "   Training sindy_regularization Loss: 1.8773927688598633\n",
      "   Validation Total Loss: 0.00022429137607105076\n",
      "   Validation decoder Loss: 0.0001342382893199101\n",
      "   Validation sindy_z Loss: 1.207162857055664\n",
      "   Validation sindy_x Loss: 0.7127916812896729\n",
      "   Validation sindy_regularization Loss: 1.8773927688598633\n",
      "Decoder Loss Ratio: 0.000712, Decoder SINDy Loss Ratio: 0.062312\n",
      "Epoch 68\n",
      "Epoch 68\n",
      "   Training Total Loss: 0.00013531467993743718\n",
      "   Training decoder Loss: 6.292132457019761e-05\n",
      "   Training sindy_z Loss: 1.3204878568649292\n",
      "   Training sindy_x Loss: 0.5356199145317078\n",
      "   Training sindy_regularization Loss: 1.8831360340118408\n",
      "   Validation Total Loss: 0.00016735319513827562\n",
      "   Validation decoder Loss: 7.988973811734468e-05\n",
      "   Validation sindy_z Loss: 1.1567655801773071\n",
      "   Validation sindy_x Loss: 0.6863210201263428\n",
      "   Validation sindy_regularization Loss: 1.8831360340118408\n",
      "Decoder Loss Ratio: 0.000424, Decoder SINDy Loss Ratio: 0.059998\n",
      "Epoch 69\n",
      "Epoch 69\n",
      "   Training Total Loss: 0.0001429379772162065\n",
      "   Training decoder Loss: 7.161298708524555e-05\n",
      "   Training sindy_z Loss: 1.294338583946228\n",
      "   Training sindy_x Loss: 0.5242756605148315\n",
      "   Training sindy_regularization Loss: 1.889742374420166\n",
      "   Validation Total Loss: 0.00017564378504175693\n",
      "   Validation decoder Loss: 8.885531860869378e-05\n",
      "   Validation sindy_z Loss: 1.1530359983444214\n",
      "   Validation sindy_x Loss: 0.678910493850708\n",
      "   Validation sindy_regularization Loss: 1.889742374420166\n",
      "Decoder Loss Ratio: 0.000472, Decoder SINDy Loss Ratio: 0.059350\n",
      "Epoch 70\n",
      "Epoch 70\n",
      "   Training Total Loss: 0.0001393505954183638\n",
      "   Training decoder Loss: 6.905824557179585e-05\n",
      "   Training sindy_z Loss: 1.265573501586914\n",
      "   Training sindy_x Loss: 0.5133187770843506\n",
      "   Training sindy_regularization Loss: 1.896047592163086\n",
      "   Validation Total Loss: 0.00017158349510282278\n",
      "   Validation decoder Loss: 8.572934166295454e-05\n",
      "   Validation sindy_z Loss: 1.130142331123352\n",
      "   Validation sindy_x Loss: 0.6689367294311523\n",
      "   Validation sindy_regularization Loss: 1.896047592163086\n",
      "Decoder Loss Ratio: 0.000455, Decoder SINDy Loss Ratio: 0.058478\n",
      "Epoch 71\n",
      "Epoch 71\n",
      "   Training Total Loss: 0.00013297317491378635\n",
      "   Training decoder Loss: 6.334743375191465e-05\n",
      "   Training sindy_z Loss: 1.24139404296875\n",
      "   Training sindy_x Loss: 0.5060827732086182\n",
      "   Training sindy_regularization Loss: 1.9017467498779297\n",
      "   Validation Total Loss: 0.00016467159730382264\n",
      "   Validation decoder Loss: 7.976125198183581e-05\n",
      "   Validation sindy_z Loss: 1.1165492534637451\n",
      "   Validation sindy_x Loss: 0.6589289307594299\n",
      "   Validation sindy_regularization Loss: 1.9017467498779297\n",
      "Decoder Loss Ratio: 0.000423, Decoder SINDy Loss Ratio: 0.057603\n",
      "Epoch 72\n",
      "Epoch 72\n",
      "   Training Total Loss: 0.00013904503430239856\n",
      "   Training decoder Loss: 6.956248398637399e-05\n",
      "   Training sindy_z Loss: 1.2053518295288086\n",
      "   Training sindy_x Loss: 0.5040760636329651\n",
      "   Training sindy_regularization Loss: 1.9074939489364624\n",
      "   Validation Total Loss: 0.00016992770542856306\n",
      "   Validation decoder Loss: 8.668443479109555e-05\n",
      "   Validation sindy_z Loss: 1.0869070291519165\n",
      "   Validation sindy_x Loss: 0.6416832804679871\n",
      "   Validation sindy_regularization Loss: 1.9074939489364624\n",
      "Decoder Loss Ratio: 0.000460, Decoder SINDy Loss Ratio: 0.056096\n",
      "Epoch 73\n",
      "Epoch 73\n",
      "   Training Total Loss: 0.0001382980844937265\n",
      "   Training decoder Loss: 7.02567704138346e-05\n",
      "   Training sindy_z Loss: 1.187967300415039\n",
      "   Training sindy_x Loss: 0.4891023337841034\n",
      "   Training sindy_regularization Loss: 1.913108468055725\n",
      "   Validation Total Loss: 0.0001703677698969841\n",
      "   Validation decoder Loss: 8.740781049709767e-05\n",
      "   Validation sindy_z Loss: 1.089839220046997\n",
      "   Validation sindy_x Loss: 0.638288676738739\n",
      "   Validation sindy_regularization Loss: 1.913108468055725\n",
      "Decoder Loss Ratio: 0.000464, Decoder SINDy Loss Ratio: 0.055799\n",
      "Epoch 74\n",
      "Epoch 74\n",
      "   Training Total Loss: 0.00012738039367832243\n",
      "   Training decoder Loss: 5.9735662944149226e-05\n",
      "   Training sindy_z Loss: 1.1624884605407715\n",
      "   Training sindy_x Loss: 0.48482203483581543\n",
      "   Training sindy_regularization Loss: 1.916253685951233\n",
      "   Validation Total Loss: 0.00015935854753479362\n",
      "   Validation decoder Loss: 7.582086982438341e-05\n",
      "   Validation sindy_z Loss: 1.0971229076385498\n",
      "   Validation sindy_x Loss: 0.643751323223114\n",
      "   Validation sindy_regularization Loss: 1.916253685951233\n",
      "Decoder Loss Ratio: 0.000402, Decoder SINDy Loss Ratio: 0.056277\n",
      "Epoch 75\n",
      "Epoch 75\n",
      "   Training Total Loss: 0.0001224368897965178\n",
      "   Training decoder Loss: 5.560992212849669e-05\n",
      "   Training sindy_z Loss: 1.1451783180236816\n",
      "   Training sindy_x Loss: 0.4761914312839508\n",
      "   Training sindy_regularization Loss: 1.9207836389541626\n",
      "   Validation Total Loss: 0.00015266714035533369\n",
      "   Validation decoder Loss: 7.178763917181641e-05\n",
      "   Validation sindy_z Loss: 1.0595855712890625\n",
      "   Validation sindy_x Loss: 0.6167166829109192\n",
      "   Validation sindy_regularization Loss: 1.9207836389541626\n",
      "Decoder Loss Ratio: 0.000381, Decoder SINDy Loss Ratio: 0.053913\n",
      "Epoch 76\n",
      "Epoch 76\n",
      "   Training Total Loss: 0.0001245071762241423\n",
      "   Training decoder Loss: 5.8047262427862734e-05\n",
      "   Training sindy_z Loss: 1.1234678030014038\n",
      "   Training sindy_x Loss: 0.47190436720848083\n",
      "   Training sindy_regularization Loss: 1.9269486665725708\n",
      "   Validation Total Loss: 0.00015474487736355513\n",
      "   Validation decoder Loss: 7.462305075023323e-05\n",
      "   Validation sindy_z Loss: 1.0440716743469238\n",
      "   Validation sindy_x Loss: 0.608523428440094\n",
      "   Validation sindy_regularization Loss: 1.9269486665725708\n",
      "Decoder Loss Ratio: 0.000396, Decoder SINDy Loss Ratio: 0.053197\n",
      "Epoch 77\n",
      "Epoch 77\n",
      "   Training Total Loss: 0.0001255507522728294\n",
      "   Training decoder Loss: 5.971077916910872e-05\n",
      "   Training sindy_z Loss: 1.096972107887268\n",
      "   Training sindy_x Loss: 0.4651145935058594\n",
      "   Training sindy_regularization Loss: 1.932850956916809\n",
      "   Validation Total Loss: 0.00015596282901242375\n",
      "   Validation decoder Loss: 7.662217103643343e-05\n",
      "   Validation sindy_z Loss: 1.0309600830078125\n",
      "   Validation sindy_x Loss: 0.6001215577125549\n",
      "   Validation sindy_regularization Loss: 1.932850956916809\n",
      "Decoder Loss Ratio: 0.000407, Decoder SINDy Loss Ratio: 0.052462\n",
      "Epoch 78\n",
      "Epoch 78\n",
      "   Training Total Loss: 0.0001584716810612008\n",
      "   Training decoder Loss: 9.337849041912705e-05\n",
      "   Training sindy_z Loss: 1.0991544723510742\n",
      "   Training sindy_x Loss: 0.457145631313324\n",
      "   Training sindy_regularization Loss: 1.9378629922866821\n",
      "   Validation Total Loss: 0.00018920176080428064\n",
      "   Validation decoder Loss: 0.0001096294290618971\n",
      "   Validation sindy_z Loss: 1.0448700189590454\n",
      "   Validation sindy_x Loss: 0.6019371151924133\n",
      "   Validation sindy_regularization Loss: 1.9378629922866821\n",
      "Decoder Loss Ratio: 0.000582, Decoder SINDy Loss Ratio: 0.052621\n",
      "Epoch 79\n",
      "Epoch 79\n",
      "   Training Total Loss: 0.00014168443158268929\n",
      "   Training decoder Loss: 7.721607835264876e-05\n",
      "   Training sindy_z Loss: 1.0717979669570923\n",
      "   Training sindy_x Loss: 0.4503961503505707\n",
      "   Training sindy_regularization Loss: 1.942874789237976\n",
      "   Validation Total Loss: 0.00017203485185746104\n",
      "   Validation decoder Loss: 9.33961127884686e-05\n",
      "   Validation sindy_z Loss: 1.0282033681869507\n",
      "   Validation sindy_x Loss: 0.5920999646186829\n",
      "   Validation sindy_regularization Loss: 1.942874789237976\n",
      "Decoder Loss Ratio: 0.000496, Decoder SINDy Loss Ratio: 0.051761\n",
      "Epoch 80\n",
      "Epoch 80\n",
      "   Training Total Loss: 0.00014513877977151424\n",
      "   Training decoder Loss: 8.124654414132237e-05\n",
      "   Training sindy_z Loss: 1.0574636459350586\n",
      "   Training sindy_x Loss: 0.4441162347793579\n",
      "   Training sindy_regularization Loss: 1.9480605125427246\n",
      "   Validation Total Loss: 0.0001744593755574897\n",
      "   Validation decoder Loss: 9.623642836231738e-05\n",
      "   Validation sindy_z Loss: 1.0242316722869873\n",
      "   Validation sindy_x Loss: 0.5874234437942505\n",
      "   Validation sindy_regularization Loss: 1.9480605125427246\n",
      "Decoder Loss Ratio: 0.000511, Decoder SINDy Loss Ratio: 0.051352\n",
      "Epoch 81\n",
      "Epoch 81\n",
      "   Training Total Loss: 0.0001557976211188361\n",
      "   Training decoder Loss: 9.232527372660115e-05\n",
      "   Training sindy_z Loss: 1.0431113243103027\n",
      "   Training sindy_x Loss: 0.43943682312965393\n",
      "   Training sindy_regularization Loss: 1.9528673887252808\n",
      "   Validation Total Loss: 0.00018531037494540215\n",
      "   Validation decoder Loss: 0.00010772913083201274\n",
      "   Validation sindy_z Loss: 1.0131009817123413\n",
      "   Validation sindy_x Loss: 0.580525815486908\n",
      "   Validation sindy_regularization Loss: 1.9528673887252808\n",
      "Decoder Loss Ratio: 0.000572, Decoder SINDy Loss Ratio: 0.050749\n",
      "Epoch 82\n",
      "Epoch 82\n",
      "   Training Total Loss: 0.00013722221774514765\n",
      "   Training decoder Loss: 7.420395559165627e-05\n",
      "   Training sindy_z Loss: 1.026703119277954\n",
      "   Training sindy_x Loss: 0.4344097971916199\n",
      "   Training sindy_regularization Loss: 1.957729458808899\n",
      "   Validation Total Loss: 0.00016740240971557796\n",
      "   Validation decoder Loss: 9.070163650903851e-05\n",
      "   Validation sindy_z Loss: 1.0023995637893677\n",
      "   Validation sindy_x Loss: 0.5712348818778992\n",
      "   Validation sindy_regularization Loss: 1.957729458808899\n",
      "Decoder Loss Ratio: 0.000481, Decoder SINDy Loss Ratio: 0.049937\n",
      "Epoch 83\n",
      "Epoch 83\n",
      "   Training Total Loss: 0.00012621436326298863\n",
      "   Training decoder Loss: 6.373287760652602e-05\n",
      "   Training sindy_z Loss: 1.010492205619812\n",
      "   Training sindy_x Loss: 0.4286070168018341\n",
      "   Training sindy_regularization Loss: 1.9620782136917114\n",
      "   Validation Total Loss: 0.0001549887820146978\n",
      "   Validation decoder Loss: 7.880552584538236e-05\n",
      "   Validation sindy_z Loss: 0.9950315952301025\n",
      "   Validation sindy_x Loss: 0.5656246542930603\n",
      "   Validation sindy_regularization Loss: 1.9620782136917114\n",
      "Decoder Loss Ratio: 0.000418, Decoder SINDy Loss Ratio: 0.049447\n",
      "Epoch 84\n",
      "Epoch 84\n",
      "   Training Total Loss: 0.00015910073125269264\n",
      "   Training decoder Loss: 9.69997126958333e-05\n",
      "   Training sindy_z Loss: 1.003303050994873\n",
      "   Training sindy_x Loss: 0.42432862520217896\n",
      "   Training sindy_regularization Loss: 1.966814637184143\n",
      "   Validation Total Loss: 0.00018828685279004276\n",
      "   Validation decoder Loss: 0.00011250283569097519\n",
      "   Validation sindy_z Loss: 0.9940155148506165\n",
      "   Validation sindy_x Loss: 0.5611586570739746\n",
      "   Validation sindy_regularization Loss: 1.966814637184143\n",
      "Decoder Loss Ratio: 0.000597, Decoder SINDy Loss Ratio: 0.049056\n",
      "Epoch 85\n",
      "Epoch 85\n",
      "   Training Total Loss: 0.00014219895820133388\n",
      "   Training decoder Loss: 8.064708526944742e-05\n",
      "   Training sindy_z Loss: 0.9810081124305725\n",
      "   Training sindy_x Loss: 0.4183569550514221\n",
      "   Training sindy_regularization Loss: 1.9716190099716187\n",
      "   Validation Total Loss: 0.00017069924797397107\n",
      "   Validation decoder Loss: 9.565728396410123e-05\n",
      "   Validation sindy_z Loss: 0.9824247360229492\n",
      "   Validation sindy_x Loss: 0.5532577633857727\n",
      "   Validation sindy_regularization Loss: 1.9716190099716187\n",
      "Decoder Loss Ratio: 0.000508, Decoder SINDy Loss Ratio: 0.048366\n",
      "Epoch 86\n",
      "Epoch 86\n",
      "   Training Total Loss: 0.00013795473205391318\n",
      "   Training decoder Loss: 7.678880501771346e-05\n",
      "   Training sindy_z Loss: 0.9713619947433472\n",
      "   Training sindy_x Loss: 0.4140132665634155\n",
      "   Training sindy_regularization Loss: 1.9764602184295654\n",
      "   Validation Total Loss: 0.000166762838489376\n",
      "   Validation decoder Loss: 9.223042434314266e-05\n",
      "   Validation sindy_z Loss: 0.9759440422058105\n",
      "   Validation sindy_x Loss: 0.5476781129837036\n",
      "   Validation sindy_regularization Loss: 1.9764602184295654\n",
      "Decoder Loss Ratio: 0.000489, Decoder SINDy Loss Ratio: 0.047878\n",
      "Epoch 87\n",
      "Epoch 87\n",
      "   Training Total Loss: 0.00011926756269531325\n",
      "   Training decoder Loss: 5.8542704209685326e-05\n",
      "   Training sindy_z Loss: 0.9510917663574219\n",
      "   Training sindy_x Loss: 0.4091561436653137\n",
      "   Training sindy_regularization Loss: 1.9809246063232422\n",
      "   Validation Total Loss: 0.00014743578503839672\n",
      "   Validation decoder Loss: 7.372446998488158e-05\n",
      "   Validation sindy_z Loss: 0.9652185440063477\n",
      "   Validation sindy_x Loss: 0.5390207171440125\n",
      "   Validation sindy_regularization Loss: 1.9809246063232422\n",
      "Decoder Loss Ratio: 0.000391, Decoder SINDy Loss Ratio: 0.047121\n",
      "Epoch 88\n",
      "Epoch 88\n",
      "   Training Total Loss: 0.00011458277731435373\n",
      "   Training decoder Loss: 5.418328510131687e-05\n",
      "   Training sindy_z Loss: 0.9315577149391174\n",
      "   Training sindy_x Loss: 0.405431866645813\n",
      "   Training sindy_regularization Loss: 1.9856306314468384\n",
      "   Validation Total Loss: 0.00014262904005590826\n",
      "   Validation decoder Loss: 6.961423059692606e-05\n",
      "   Validation sindy_z Loss: 0.9478565454483032\n",
      "   Validation sindy_x Loss: 0.5315850377082825\n",
      "   Validation sindy_regularization Loss: 1.9856306314468384\n",
      "Decoder Loss Ratio: 0.000369, Decoder SINDy Loss Ratio: 0.046471\n",
      "Epoch 89\n",
      "Epoch 89\n",
      "   Training Total Loss: 0.00011311924026813358\n",
      "   Training decoder Loss: 5.322302968124859e-05\n",
      "   Training sindy_z Loss: 0.9416868686676025\n",
      "   Training sindy_x Loss: 0.4000944495201111\n",
      "   Training sindy_regularization Loss: 1.98867666721344\n",
      "   Validation Total Loss: 0.0001405624207109213\n",
      "   Validation decoder Loss: 6.840867717983201e-05\n",
      "   Validation sindy_z Loss: 0.9535356163978577\n",
      "   Validation sindy_x Loss: 0.522669792175293\n",
      "   Validation sindy_regularization Loss: 1.98867666721344\n",
      "Decoder Loss Ratio: 0.000363, Decoder SINDy Loss Ratio: 0.045692\n",
      "Epoch 90\n",
      "Epoch 90\n",
      "   Training Total Loss: 0.00010823415505001321\n",
      "   Training decoder Loss: 4.8504301958018914e-05\n",
      "   Training sindy_z Loss: 0.9120704531669617\n",
      "   Training sindy_x Loss: 0.3979552090167999\n",
      "   Training sindy_regularization Loss: 1.9934332370758057\n",
      "   Validation Total Loss: 0.0001351195969618857\n",
      "   Validation decoder Loss: 6.32224982837215e-05\n",
      "   Validation sindy_z Loss: 0.9359215497970581\n",
      "   Validation sindy_x Loss: 0.5196277499198914\n",
      "   Validation sindy_regularization Loss: 1.9934332370758057\n",
      "Decoder Loss Ratio: 0.000335, Decoder SINDy Loss Ratio: 0.045426\n",
      "Epoch 91\n",
      "Epoch 91\n",
      "   Training Total Loss: 0.0001122570683946833\n",
      "   Training decoder Loss: 5.286684609018266e-05\n",
      "   Training sindy_z Loss: 0.9005155563354492\n",
      "   Training sindy_x Loss: 0.3940904438495636\n",
      "   Training sindy_regularization Loss: 1.998117446899414\n",
      "   Validation Total Loss: 0.0001399118482368067\n",
      "   Validation decoder Loss: 6.79895601933822e-05\n",
      "   Validation sindy_z Loss: 0.9384440779685974\n",
      "   Validation sindy_x Loss: 0.5194111466407776\n",
      "   Validation sindy_regularization Loss: 1.998117446899414\n",
      "Decoder Loss Ratio: 0.000361, Decoder SINDy Loss Ratio: 0.045407\n",
      "Epoch 92\n",
      "Epoch 92\n",
      "   Training Total Loss: 0.00012196104944450781\n",
      "   Training decoder Loss: 6.286783900577575e-05\n",
      "   Training sindy_z Loss: 0.9002685546875\n",
      "   Training sindy_x Loss: 0.3906847834587097\n",
      "   Training sindy_regularization Loss: 2.002473831176758\n",
      "   Validation Total Loss: 0.00014943091082386672\n",
      "   Validation decoder Loss: 7.776049460517243e-05\n",
      "   Validation sindy_z Loss: 0.9463023543357849\n",
      "   Validation sindy_x Loss: 0.5164568424224854\n",
      "   Validation sindy_regularization Loss: 2.002473831176758\n",
      "Decoder Loss Ratio: 0.000413, Decoder SINDy Loss Ratio: 0.045148\n",
      "Epoch 93\n",
      "Epoch 93\n",
      "   Training Total Loss: 0.00010437118180561811\n",
      "   Training decoder Loss: 4.590370735968463e-05\n",
      "   Training sindy_z Loss: 0.8853595852851868\n",
      "   Training sindy_x Loss: 0.384042888879776\n",
      "   Training sindy_regularization Loss: 2.006319046020508\n",
      "   Validation Total Loss: 0.00013099494390189648\n",
      "   Validation decoder Loss: 6.000798020977527e-05\n",
      "   Validation sindy_z Loss: 0.9290695190429688\n",
      "   Validation sindy_x Loss: 0.5092377066612244\n",
      "   Validation sindy_regularization Loss: 2.006319046020508\n",
      "Decoder Loss Ratio: 0.000318, Decoder SINDy Loss Ratio: 0.044517\n",
      "Epoch 94\n",
      "Epoch 94\n",
      "   Training Total Loss: 0.00010828710219357163\n",
      "   Training decoder Loss: 5.0218459364259616e-05\n",
      "   Training sindy_z Loss: 0.8646467924118042\n",
      "   Training sindy_x Loss: 0.37961697578430176\n",
      "   Training sindy_regularization Loss: 2.010694980621338\n",
      "   Validation Total Loss: 0.00013482000213116407\n",
      "   Validation decoder Loss: 6.49492212687619e-05\n",
      "   Validation sindy_z Loss: 0.9244633316993713\n",
      "   Validation sindy_x Loss: 0.4976383149623871\n",
      "   Validation sindy_regularization Loss: 2.010694980621338\n",
      "Decoder Loss Ratio: 0.000345, Decoder SINDy Loss Ratio: 0.043503\n",
      "Epoch 95\n",
      "Epoch 95\n",
      "   Training Total Loss: 0.00010644402937032282\n",
      "   Training decoder Loss: 4.8649017116986215e-05\n",
      "   Training sindy_z Loss: 0.8553653359413147\n",
      "   Training sindy_x Loss: 0.37667763233184814\n",
      "   Training sindy_regularization Loss: 2.0127251148223877\n",
      "   Validation Total Loss: 0.00013249304902274162\n",
      "   Validation decoder Loss: 6.262166425585747e-05\n",
      "   Validation sindy_z Loss: 0.9038368463516235\n",
      "   Validation sindy_x Loss: 0.4974413514137268\n",
      "   Validation sindy_regularization Loss: 2.0127251148223877\n",
      "Decoder Loss Ratio: 0.000332, Decoder SINDy Loss Ratio: 0.043486\n",
      "Epoch 96\n",
      "Epoch 96\n",
      "   Training Total Loss: 0.00010023437062045559\n",
      "   Training decoder Loss: 4.2735646275104955e-05\n",
      "   Training sindy_z Loss: 0.8423494100570679\n",
      "   Training sindy_x Loss: 0.37335628271102905\n",
      "   Training sindy_regularization Loss: 2.016309976577759\n",
      "   Validation Total Loss: 0.00012563433847390115\n",
      "   Validation decoder Loss: 5.6415021390421316e-05\n",
      "   Validation sindy_z Loss: 0.9012516140937805\n",
      "   Validation sindy_x Loss: 0.49056220054626465\n",
      "   Validation sindy_regularization Loss: 2.016309976577759\n",
      "Decoder Loss Ratio: 0.000299, Decoder SINDy Loss Ratio: 0.042885\n",
      "Epoch 97\n",
      "Epoch 97\n",
      "   Training Total Loss: 0.00010802820179378614\n",
      "   Training decoder Loss: 5.062989657744765e-05\n",
      "   Training sindy_z Loss: 0.8509128093719482\n",
      "   Training sindy_x Loss: 0.37195491790771484\n",
      "   Training sindy_regularization Loss: 2.0202810764312744\n",
      "   Validation Total Loss: 0.00013374161790125072\n",
      "   Validation decoder Loss: 6.429137283703312e-05\n",
      "   Validation sindy_z Loss: 0.9148906469345093\n",
      "   Validation sindy_x Loss: 0.49247440695762634\n",
      "   Validation sindy_regularization Loss: 2.0202810764312744\n",
      "Decoder Loss Ratio: 0.000341, Decoder SINDy Loss Ratio: 0.043052\n",
      "Epoch 98\n",
      "Epoch 98\n",
      "   Training Total Loss: 0.00010127562563866377\n",
      "   Training decoder Loss: 4.454567897482775e-05\n",
      "   Training sindy_z Loss: 0.8325780630111694\n",
      "   Training sindy_x Loss: 0.36487555503845215\n",
      "   Training sindy_regularization Loss: 2.0242390632629395\n",
      "   Validation Total Loss: 0.0001267741754418239\n",
      "   Validation decoder Loss: 5.831930320709944e-05\n",
      "   Validation sindy_z Loss: 0.9019702672958374\n",
      "   Validation sindy_x Loss: 0.48212483525276184\n",
      "   Validation sindy_regularization Loss: 2.0242390632629395\n",
      "Decoder Loss Ratio: 0.000309, Decoder SINDy Loss Ratio: 0.042147\n",
      "Epoch 99\n",
      "Epoch 99\n",
      "   Training Total Loss: 0.00011277719750069082\n",
      "   Training decoder Loss: 5.6424531067023054e-05\n",
      "   Training sindy_z Loss: 0.823017418384552\n",
      "   Training sindy_x Loss: 0.36080726981163025\n",
      "   Training sindy_regularization Loss: 2.0271942615509033\n",
      "   Validation Total Loss: 0.00013834274432156235\n",
      "   Validation decoder Loss: 7.036150782369077e-05\n",
      "   Validation sindy_z Loss: 0.8926572799682617\n",
      "   Validation sindy_x Loss: 0.4770929217338562\n",
      "   Validation sindy_regularization Loss: 2.0271942615509033\n",
      "Decoder Loss Ratio: 0.000373, Decoder SINDy Loss Ratio: 0.041707\n",
      "Epoch 100\n",
      "Epoch 100\n",
      "   Training Total Loss: 0.00010168979497393593\n",
      "   Training decoder Loss: 4.573754267767072e-05\n",
      "   Training sindy_z Loss: 0.8102593421936035\n",
      "   Training sindy_x Loss: 0.35671544075012207\n",
      "   Training sindy_regularization Loss: 2.0280704498291016\n",
      "   Validation Total Loss: 0.0001247779291588813\n",
      "   Validation decoder Loss: 5.7272038247901946e-05\n",
      "   Validation sindy_z Loss: 0.8854133486747742\n",
      "   Validation sindy_x Loss: 0.47225189208984375\n",
      "   Validation sindy_regularization Loss: 2.0280704498291016\n",
      "Decoder Loss Ratio: 0.000304, Decoder SINDy Loss Ratio: 0.041284\n",
      "Epoch 101\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m     params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoefficient_mask\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlibrary_dim\u001b[39m\u001b[38;5;124m'\u001b[39m], params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatent_dim\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m      8\u001b[0m     params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlorenz_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS_\u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m     results_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresults_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams}, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     13\u001b[0m df\u001b[38;5;241m.\u001b[39mto_pickle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexperiment_results_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ianma\\Desktop\\Study\\Python\\CS4240_DeepLearning_Group41_ReproducibilityProject\\Autoencoders-Reproduction\\examples\\../src\\training.py:40\u001b[0m, in \u001b[0;36mtrain_network\u001b[1;34m(training_data, val_data, params)\u001b[0m\n\u001b[0;32m     38\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     39\u001b[0m     loss_val, _, _ \u001b[38;5;241m=\u001b[39m autoencoder_network\u001b[38;5;241m.\u001b[39mdefine_loss(train_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m], train_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdx\u001b[39m\u001b[38;5;124m'\u001b[39m], params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m---> 40\u001b[0m     \u001b[43mloss_val\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprint_progress\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m (i \u001b[38;5;241m%\u001b[39m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprint_frequency\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\ianma\\anaconda3\\envs\\deep_learning\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ianma\\anaconda3\\envs\\deep_learning\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_experiments = 1\n",
    "df = pd.DataFrame()\n",
    "for i in range(num_experiments):\n",
    "    print('EXPERIMENT %d' % i)\n",
    "\n",
    "    params['coefficient_mask'] = torch.ones((params['library_dim'], params['latent_dim']))\n",
    "\n",
    "    params['save_name'] = 'lorenz_' + datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S_%f\")\n",
    "    \n",
    "    results_dict = train_network(training_data, validation_data, params)\n",
    "    df = df.append({**results_dict, **params}, ignore_index=True)\n",
    "\n",
    "df.to_pickle('experiment_results_' + datetime.datetime.now().strftime(\"%Y%m%d%H%M\") + '.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
